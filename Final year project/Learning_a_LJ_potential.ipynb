{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pinn import get_network\n",
    "from pinn.utils import atomic_dress, get_atomic_dress\n",
    "from ase.collections import g2\n",
    "from pinn.io import sparse_batch, load_numpy\n",
    "from docs.notebooks.network_fns import preprocess_traintest_sets, train_and_evaluate_network, _generator, predict_energy\n",
    "import matplotlib.pyplot as plt\n",
    "from ase import Atoms\n",
    "from ase.calculators.lj import LennardJones\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from pinn.optimizers import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices()\n",
    "tf.config.set_visible_devices(physical_devices[0], 'CPU')\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: get the position given PES dimension(s)\n",
    "def three_body_sample(atoms, a, r):\n",
    "    x = a * np.pi / 180\n",
    "    pos = [[0, 0, 0],\n",
    "           [0, 2, 0],\n",
    "           [0, r*np.cos(x), r*np.sin(x)]]\n",
    "    atoms.set_positions(pos)\n",
    "    return atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = Atoms('H3', calculator=LennardJones())\n",
    "\n",
    "na, nr = 50, 50\n",
    "arange = np.linspace(30,180,na)\n",
    "rrange = np.linspace(1,3,nr)\n",
    "\n",
    "# Truth\n",
    "agrid, rgrid = np.meshgrid(arange, rrange)\n",
    "egrid = np.zeros([na, nr])\n",
    "for i in range(na):\n",
    "    for j in range(nr):\n",
    "        atoms = three_body_sample(atoms, arange[i], rrange[j])\n",
    "        egrid[i,j] = atoms.get_potential_energy()\n",
    "        \n",
    "# Samples\n",
    "nsample = 100\n",
    "asample, rsample = [], []\n",
    "distsample = []\n",
    "data = {'e_data':[], 'f_data':[], 'elems':[], 'coord':[]}\n",
    "for i in range(nsample):\n",
    "    a, r = np.random.choice(arange), np.random.choice(rrange)\n",
    "    atoms = three_body_sample(atoms, a, r)\n",
    "    dist = atoms.get_all_distances()\n",
    "    dist = dist[np.nonzero(dist)]\n",
    "    data['e_data'].append(atoms.get_potential_energy())\n",
    "    data['f_data'].append(atoms.get_forces())\n",
    "    data['coord'].append(atoms.get_positions())\n",
    "    data['elems'].append(atoms.numbers)\n",
    "    asample.append(a)\n",
    "    rsample.append(r)\n",
    "    distsample.append(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset from numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_data shape:  (100,)\n",
      "f_data shape:  (100, 3, 3)\n",
      "coord shape:  (100, 3, 3)\n",
      "elems shape:  (100, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 23:14:59.095863: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-03-29 23:14:59.095978: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "data = {k:np.array(v) for k,v in data.items()}\n",
    "print(\"e_data shape: \", data['e_data'].shape)\n",
    "print(\"f_data shape: \", data['f_data'].shape)\n",
    "print(\"coord shape: \", data['coord'].shape)\n",
    "print(\"elems shape: \", data['elems'].shape)\n",
    "\n",
    "dataset = load_numpy(data, splits={'train':8, 'test':2})\n",
    "dress, error = get_atomic_dress(dataset['train'], [1])\n",
    "# train_set, test_set, batch_size = get_traintest_sets(dataset, buffer_size=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_repeats = 2000\n",
    "\n",
    "def print_shapes(batch):\n",
    "    print(\"ind_1 shape: \", batch['ind_1'].shape)\n",
    "    print(\"e_data shape: \", batch['e_data'].shape)\n",
    "    print(\"f_data shape: \", batch['f_data'].shape)\n",
    "    print(\"elems shape: \", batch['elems'].shape)\n",
    "    print(\"coord shape: \", batch['coord'].shape)\n",
    "    return batch\n",
    "\n",
    "# train_set = dataset['train'].shuffle(100).repeat(train_repeats).apply(sparse_batch(batch_size)).map(print_shapes)\n",
    "# test_set = dataset['test'].repeat(100).apply(sparse_batch(100)).map(print_shapes)\n",
    "\n",
    "train_set = dataset['train'].shuffle(100).repeat(train_repeats).apply(sparse_batch(batch_size))\n",
    "test_set = dataset['test'].repeat(100).apply(sparse_batch(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: {e_data: (None,), f_data: (None, 3), elems: (None,), coord: (None, 3), ind_1: (None, 1)}, types: {e_data: tf.float32, f_data: tf.float32, elems: tf.int32, coord: tf.float32, ind_1: tf.int32}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get network and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'optimizer': {\n",
    "        'class_name': 'Adam',\n",
    "        'config': {\n",
    "            'learning_rate': {\n",
    "                'class_name': 'ExponentialDecay',\n",
    "                'config': {\n",
    "                    'initial_learning_rate': 0.0003,\n",
    "                    'decay_steps': 10000, \n",
    "                    'decay_rate': 0.994}}, \n",
    "                    'clipnorm': 0.01}},\n",
    "    'network': {\n",
    "        'name': 'PiNet',\n",
    "        'params': {\n",
    "            'ii_nodes':[8,8],\n",
    "            'pi_nodes':[8,8],\n",
    "            'pp_nodes':[8,8],\n",
    "            'out_nodes':[8,8],\n",
    "            'depth': 4,\n",
    "            'rc': 3.0,\n",
    "            'atom_types':[1]}},\n",
    "\n",
    "    'e_dress': dress,  # element-specific energy dress\n",
    "    'e_scale': 2, # energy scale for prediction\n",
    "    'e_unit': 1.0,  # output unit of energy dur\n",
    "    'use_force': True\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_preprocessed_shapes(batch, counter=[0]):\n",
    "    counter[0] += 1\n",
    "    print(f\"=== Batch {counter[0]} ===\")\n",
    "    for key, value in batch.items():\n",
    "        print(f\"{key} shape: {value.shape}\")\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = get_network(params['network'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_traintest_sets(train_set, test_set, network=network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_network(network=None, params=None, train_set=None, test_set=None, batch_size=256, epochs=1):\n",
    "\n",
    "\n",
    "    # Instantiate an optimizer\n",
    "    from pinn.optimizers import get\n",
    "    optimizer = get(params['optimizer'])\n",
    "    # Define a loss function\n",
    "    loss_fn = tf.keras.losses.mse\n",
    "    validation_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time_epoch = time.time()\n",
    "        hund_step_times = []\n",
    "\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, batch in enumerate(train_set):\n",
    "            # print('starting step', step)\n",
    "            train_losses = []\n",
    "            train_MAEs = []\n",
    "            # Open a GradientTape to record the operations run\n",
    "            # during the forward pass, which enables auto-differentiation.\n",
    "            with tf.GradientTape() as loss_tape:\n",
    "                with tf.GradientTape() as innertape:\n",
    "                    innertape.watch(batch)\n",
    "\n",
    "                    # Run the forward pass of the layer.\n",
    "                    # The operations that the layer applies\n",
    "                    # to its inputs are going to be recorded\n",
    "                    # on the GradientTape.\n",
    "\n",
    "                    pred = network(batch, training=True)  # Logits for this minibatch\n",
    "                    # print(\"pred shape: \", pred.shape)\n",
    "\n",
    "                    ind = batch['ind_1']\n",
    "                    nbatch = tf.reduce_max(ind)+1\n",
    "                    pred = tf.math.unsorted_segment_sum(pred, ind[:, 0], nbatch)    \n",
    "                    e_data = batch['e_data']\n",
    "\n",
    "                    if params['e_dress']:\n",
    "                        e_data -= atomic_dress(batch, params['e_dress'], dtype=pred.dtype)\n",
    "                    e_data *= params['e_scale']\n",
    "                    \n",
    "                    # print(\"pred shape after unsorted_segment_sum:\", pred.shape)\n",
    "                    # print(\"e_data shape:\", e_data.shape)\n",
    "                \n",
    "                    train_losses.append(loss_fn(e_data, pred))\n",
    "                    train_MAEs.append(tf.reduce_mean(np.abs(e_data - pred)))\n",
    "\n",
    "\n",
    "                if params['use_force']:\n",
    "\n",
    "                    f_pred = innertape.gradient(pred, batch['coord'])\n",
    "                    if type(f_pred) == tf.IndexedSlices:\n",
    "                        f_pred = tf.scatter_nd(tf.expand_dims(f_pred.indices, 1), f_pred.values,\n",
    "                                            tf.cast(f_pred.dense_shape, tf.int32))\n",
    "\n",
    "                    f_pred = -f_pred\n",
    "\n",
    "                    f_data = batch['f_data']*params['e_scale']\n",
    "                    f_mask = tf.fill(tf.shape(f_pred), True)\n",
    "\n",
    "                    error = f_data - f_pred\n",
    "                    error = tf.boolean_mask(error, f_mask)\n",
    "                    # print(\"f_data shape: \", f_data.shape)\n",
    "                    # print(\"f_pred shape after scatter_nd and negate:\", f_pred.shape)\n",
    "                    train_losses.append(tf.reduce_mean(error**2))\n",
    "                    train_MAEs.append(tf.reduce_mean(error))\n",
    "\n",
    "                # Compute the loss value for this minibatch.\n",
    "                train_loss_value = tf.reduce_sum(train_losses)\n",
    "                train_MAE_value = tf.reduce_sum(train_MAEs)\n",
    "\n",
    "            # Use the gradient tape to automatically retrieve\n",
    "            # the gradients of the trainable variables with respect to the loss.\n",
    "            grads = loss_tape.gradient(train_loss_value, network.trainable_weights)\n",
    "\n",
    "            # Run one step of gradient descent by updating\n",
    "            # the value of the variables to minimize the loss.\n",
    "            optimizer.apply_gradients(zip(grads, network.trainable_weights))\n",
    "\n",
    "\n",
    "\n",
    "            # Log every n batches.\n",
    "            if step == 0:\n",
    "                print(f\"Initial loss (for one batch): {float(train_loss_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "\n",
    "\n",
    "            elif step % 20 == 0:\n",
    "                print(f\"Training loss (for one batch) at step {step}: {float(train_loss_value)}\")\n",
    "                print(f\"Training MAE (for one batch) at step {step}: {float(train_MAE_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "                hund_step_times += [(time.time() - start_time_epoch)]\n",
    "                print(f'Training time for 20 batches: {((hund_step_times[-1] - hund_step_times[-2]) if len(hund_step_times) > 1 else hund_step_times[-1])} s')\n",
    "\n",
    "\n",
    "        print(f'Training time for epoch {epoch + 1}: {(time.time() - start_time_epoch)} s')\n",
    "\n",
    "        \n",
    "\n",
    "        # Run a validation loop at the end of each epoch\n",
    "        print(f'Starting validation for epoch {(epoch + 1)}')\n",
    "\n",
    "        for step, batch in enumerate(test_set):\n",
    "\n",
    "            val_losses = []\n",
    "            val_MAEs = []\n",
    "            \n",
    "            val_pred = network(batch, training=False)\n",
    "            ind = batch['ind_1']\n",
    "            nbatch = tf.reduce_max(ind)+1\n",
    "            val_pred = tf.math.unsorted_segment_sum(val_pred, ind[:, 0], nbatch)\n",
    "            e_data = batch['e_data']\n",
    "            \n",
    "            if params['e_dress']:\n",
    "                e_data -= atomic_dress(batch, params['e_dress'], dtype=pred.dtype)\n",
    "                e_data *= params['e_scale']\n",
    "\n",
    "\n",
    "            val_losses.append(loss_fn(batch['e_data'], val_pred))\n",
    "            val_MAEs.append(np.abs(e_data - val_pred))\n",
    "\n",
    "            val_loss_value = tf.reduce_sum(val_losses)\n",
    "            val_MAE_value = tf.reduce_sum(val_MAEs)\n",
    "            validation_losses.append(val_loss_value)\n",
    "\n",
    "            # Log every n batches.\n",
    "            if step == 0:\n",
    "                print(f\"Initial validation loss (for one batch): {float(val_loss_value)}\")\n",
    "                print(f\"Initial validation MAE (for one batch): {float(val_MAE_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "\n",
    "\n",
    "            elif step % 20 == 0:\n",
    "                print(f\"Validation loss (for one batch) at step {step}: {float(val_loss_value)}\")\n",
    "                print(f\"Validation loss (for one batch) at step {step}: {float(val_MAE_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "                hund_step_times += [(time.time() - start_time_epoch)]\n",
    "                print(f'Validation time for 20 batches: {((hund_step_times[-1] - hund_step_times[-2]) if len(hund_step_times) > 1 else hund_step_times[-1])} s')\n",
    "\n",
    "            if epoch > 3 and val_loss_value >= min(validation_losses[-4:-1]):\n",
    "                        return 'Stopped.'\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"Time taken for epoch {epoch + 1}: {(time.time() - start_time_epoch)} s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "Initial loss (for one batch): 7.215100288391113\n",
      "Seen so far: 100 molecules\n",
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "Training loss (for one batch) at step 20: 6.3925251960754395\n",
      "Training MAE (for one batch) at step 20: 0.41383400559425354\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 1.0946218967437744 s\n",
      "Training loss (for one batch) at step 40: 5.961619853973389\n",
      "Training MAE (for one batch) at step 40: 0.3325004577636719\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 0.837688684463501 s\n",
      "Training loss (for one batch) at step 60: 4.676161766052246\n",
      "Training MAE (for one batch) at step 60: 0.32789310812950134\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 0.828819990158081 s\n",
      "Training loss (for one batch) at step 80: 2.875715970993042\n",
      "Training MAE (for one batch) at step 80: 0.3272826075553894\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 0.8315722942352295 s\n",
      "Training loss (for one batch) at step 100: 1.8006536960601807\n",
      "Training MAE (for one batch) at step 100: 0.23328179121017456\n",
      "Seen so far: 10100 molecules\n",
      "Training time for 20 batches: 0.8115088939666748 s\n",
      "Training loss (for one batch) at step 120: 0.8503768444061279\n",
      "Training MAE (for one batch) at step 120: 0.12051121145486832\n",
      "Seen so far: 12100 molecules\n",
      "Training time for 20 batches: 0.8071389198303223 s\n",
      "Training loss (for one batch) at step 140: 0.5608872175216675\n",
      "Training MAE (for one batch) at step 140: 0.0963912308216095\n",
      "Seen so far: 14100 molecules\n",
      "Training time for 20 batches: 0.8140931129455566 s\n",
      "Training loss (for one batch) at step 160: 0.3989240527153015\n",
      "Training MAE (for one batch) at step 160: 0.09901584684848785\n",
      "Seen so far: 16100 molecules\n",
      "Training time for 20 batches: 0.8283469676971436 s\n",
      "Training loss (for one batch) at step 180: 0.3474646806716919\n",
      "Training MAE (for one batch) at step 180: 0.09588423371315002\n",
      "Seen so far: 18100 molecules\n",
      "Training time for 20 batches: 0.8205289840698242 s\n",
      "Training loss (for one batch) at step 200: 0.28606489300727844\n",
      "Training MAE (for one batch) at step 200: 0.08703853189945221\n",
      "Seen so far: 20100 molecules\n",
      "Training time for 20 batches: 0.8157460689544678 s\n",
      "Training loss (for one batch) at step 220: 0.3840739130973816\n",
      "Training MAE (for one batch) at step 220: 0.09015998989343643\n",
      "Seen so far: 22100 molecules\n",
      "Training time for 20 batches: 0.8036308288574219 s\n",
      "Training loss (for one batch) at step 240: 0.37036001682281494\n",
      "Training MAE (for one batch) at step 240: 0.087859608232975\n",
      "Seen so far: 24100 molecules\n",
      "Training time for 20 batches: 0.8054502010345459 s\n",
      "Training loss (for one batch) at step 260: 0.2834680378437042\n",
      "Training MAE (for one batch) at step 260: 0.0726752057671547\n",
      "Seen so far: 26100 molecules\n",
      "Training time for 20 batches: 0.8088669776916504 s\n",
      "Training loss (for one batch) at step 280: 0.34667524695396423\n",
      "Training MAE (for one batch) at step 280: 0.09014490246772766\n",
      "Seen so far: 28100 molecules\n",
      "Training time for 20 batches: 0.7942681312561035 s\n",
      "Training loss (for one batch) at step 300: 0.3507853150367737\n",
      "Training MAE (for one batch) at step 300: 0.08284316956996918\n",
      "Seen so far: 30100 molecules\n",
      "Training time for 20 batches: 0.7950096130371094 s\n",
      "Training loss (for one batch) at step 320: 0.21708491444587708\n",
      "Training MAE (for one batch) at step 320: 0.06764387339353561\n",
      "Seen so far: 32100 molecules\n",
      "Training time for 20 batches: 0.7990353107452393 s\n",
      "Training loss (for one batch) at step 340: 0.19829465448856354\n",
      "Training MAE (for one batch) at step 340: 0.06733956933021545\n",
      "Seen so far: 34100 molecules\n",
      "Training time for 20 batches: 0.8243398666381836 s\n",
      "Training loss (for one batch) at step 360: 0.19499748945236206\n",
      "Training MAE (for one batch) at step 360: 0.06503193080425262\n",
      "Seen so far: 36100 molecules\n",
      "Training time for 20 batches: 0.8061151504516602 s\n",
      "Training loss (for one batch) at step 380: 0.1888153851032257\n",
      "Training MAE (for one batch) at step 380: 0.06064096838235855\n",
      "Seen so far: 38100 molecules\n",
      "Training time for 20 batches: 0.8128960132598877 s\n",
      "Training loss (for one batch) at step 400: 0.16663724184036255\n",
      "Training MAE (for one batch) at step 400: 0.06046770140528679\n",
      "Seen so far: 40100 molecules\n",
      "Training time for 20 batches: 0.7943778038024902 s\n",
      "Training loss (for one batch) at step 420: 0.18853679299354553\n",
      "Training MAE (for one batch) at step 420: 0.06398537755012512\n",
      "Seen so far: 42100 molecules\n",
      "Training time for 20 batches: 0.8043620586395264 s\n",
      "Training loss (for one batch) at step 440: 0.1666995733976364\n",
      "Training MAE (for one batch) at step 440: 0.06013139709830284\n",
      "Seen so far: 44100 molecules\n",
      "Training time for 20 batches: 0.8561179637908936 s\n",
      "Training loss (for one batch) at step 460: 0.17677873373031616\n",
      "Training MAE (for one batch) at step 460: 0.05910979583859444\n",
      "Seen so far: 46100 molecules\n",
      "Training time for 20 batches: 0.7972381114959717 s\n",
      "Training loss (for one batch) at step 480: 0.20834441483020782\n",
      "Training MAE (for one batch) at step 480: 0.05917198583483696\n",
      "Seen so far: 48100 molecules\n",
      "Training time for 20 batches: 0.7965397834777832 s\n",
      "Training loss (for one batch) at step 500: 0.1562870442867279\n",
      "Training MAE (for one batch) at step 500: 0.0521257109940052\n",
      "Seen so far: 50100 molecules\n",
      "Training time for 20 batches: 0.8047759532928467 s\n",
      "Training loss (for one batch) at step 520: 0.13987481594085693\n",
      "Training MAE (for one batch) at step 520: 0.04735836014151573\n",
      "Seen so far: 52100 molecules\n",
      "Training time for 20 batches: 0.796673059463501 s\n",
      "Training loss (for one batch) at step 540: 0.16930390894412994\n",
      "Training MAE (for one batch) at step 540: 0.058873653411865234\n",
      "Seen so far: 54100 molecules\n",
      "Training time for 20 batches: 0.8070390224456787 s\n",
      "Training loss (for one batch) at step 560: 0.17858418822288513\n",
      "Training MAE (for one batch) at step 560: 0.057424768805503845\n",
      "Seen so far: 56100 molecules\n",
      "Training time for 20 batches: 0.8138570785522461 s\n",
      "Training loss (for one batch) at step 580: 0.11010053753852844\n",
      "Training MAE (for one batch) at step 580: 0.04427866265177727\n",
      "Seen so far: 58100 molecules\n",
      "Training time for 20 batches: 0.821558952331543 s\n",
      "Training loss (for one batch) at step 600: 0.11520301550626755\n",
      "Training MAE (for one batch) at step 600: 0.046643584966659546\n",
      "Seen so far: 60100 molecules\n",
      "Training time for 20 batches: 0.8155031204223633 s\n",
      "Training loss (for one batch) at step 620: 0.0946134403347969\n",
      "Training MAE (for one batch) at step 620: 0.0414559543132782\n",
      "Seen so far: 62100 molecules\n",
      "Training time for 20 batches: 0.7950348854064941 s\n",
      "Training loss (for one batch) at step 640: 0.13814905285835266\n",
      "Training MAE (for one batch) at step 640: 0.045170001685619354\n",
      "Seen so far: 64100 molecules\n",
      "Training time for 20 batches: 0.7969100475311279 s\n",
      "Training loss (for one batch) at step 660: 0.10629807412624359\n",
      "Training MAE (for one batch) at step 660: 0.04511183872818947\n",
      "Seen so far: 66100 molecules\n",
      "Training time for 20 batches: 0.7974650859832764 s\n",
      "Training loss (for one batch) at step 680: 0.08523963391780853\n",
      "Training MAE (for one batch) at step 680: 0.038523390889167786\n",
      "Seen so far: 68100 molecules\n",
      "Training time for 20 batches: 0.8109190464019775 s\n",
      "Training loss (for one batch) at step 700: 0.10819671303033829\n",
      "Training MAE (for one batch) at step 700: 0.0469600111246109\n",
      "Seen so far: 70100 molecules\n",
      "Training time for 20 batches: 0.8007628917694092 s\n",
      "Training loss (for one batch) at step 720: 0.07936088740825653\n",
      "Training MAE (for one batch) at step 720: 0.04259442165493965\n",
      "Seen so far: 72100 molecules\n",
      "Training time for 20 batches: 0.7949841022491455 s\n",
      "Training loss (for one batch) at step 740: 0.06702230125665665\n",
      "Training MAE (for one batch) at step 740: 0.03768395632505417\n",
      "Seen so far: 74100 molecules\n",
      "Training time for 20 batches: 0.7992479801177979 s\n",
      "Training loss (for one batch) at step 760: 0.06013408303260803\n",
      "Training MAE (for one batch) at step 760: 0.03735281899571419\n",
      "Seen so far: 76100 molecules\n",
      "Training time for 20 batches: 0.7963638305664062 s\n",
      "Training loss (for one batch) at step 780: 0.08300551772117615\n",
      "Training MAE (for one batch) at step 780: 0.045137498527765274\n",
      "Seen so far: 78100 molecules\n",
      "Training time for 20 batches: 0.8080730438232422 s\n",
      "Training loss (for one batch) at step 800: 0.05566280707716942\n",
      "Training MAE (for one batch) at step 800: 0.03567413240671158\n",
      "Seen so far: 80100 molecules\n",
      "Training time for 20 batches: 0.8188910484313965 s\n",
      "Training loss (for one batch) at step 820: 0.04458281025290489\n",
      "Training MAE (for one batch) at step 820: 0.031694840639829636\n",
      "Seen so far: 82100 molecules\n",
      "Training time for 20 batches: 0.836129903793335 s\n",
      "Training loss (for one batch) at step 840: 0.05446014180779457\n",
      "Training MAE (for one batch) at step 840: 0.03976165130734444\n",
      "Seen so far: 84100 molecules\n",
      "Training time for 20 batches: 0.8163940906524658 s\n",
      "Training loss (for one batch) at step 860: 0.037778228521347046\n",
      "Training MAE (for one batch) at step 860: 0.030606653541326523\n",
      "Seen so far: 86100 molecules\n",
      "Training time for 20 batches: 0.8076980113983154 s\n",
      "Training loss (for one batch) at step 880: 0.03342784196138382\n",
      "Training MAE (for one batch) at step 880: 0.03193425387144089\n",
      "Seen so far: 88100 molecules\n",
      "Training time for 20 batches: 0.8065609931945801 s\n",
      "Training loss (for one batch) at step 900: 0.03023993968963623\n",
      "Training MAE (for one batch) at step 900: 0.029035208746790886\n",
      "Seen so far: 90100 molecules\n",
      "Training time for 20 batches: 0.8171570301055908 s\n",
      "Training loss (for one batch) at step 920: 0.02807060070335865\n",
      "Training MAE (for one batch) at step 920: 0.02739657834172249\n",
      "Seen so far: 92100 molecules\n",
      "Training time for 20 batches: 0.8859939575195312 s\n",
      "Training loss (for one batch) at step 940: 0.027149761095643044\n",
      "Training MAE (for one batch) at step 940: 0.02481367625296116\n",
      "Seen so far: 94100 molecules\n",
      "Training time for 20 batches: 0.8018770217895508 s\n",
      "Training loss (for one batch) at step 960: 0.024804357439279556\n",
      "Training MAE (for one batch) at step 960: 0.027381274849176407\n",
      "Seen so far: 96100 molecules\n",
      "Training time for 20 batches: 0.7999229431152344 s\n",
      "Training loss (for one batch) at step 980: 0.019166002050042152\n",
      "Training MAE (for one batch) at step 980: 0.027072053402662277\n",
      "Seen so far: 98100 molecules\n",
      "Training time for 20 batches: 0.7980589866638184 s\n",
      "Training loss (for one batch) at step 1000: 0.01655908301472664\n",
      "Training MAE (for one batch) at step 1000: 0.024751955643296242\n",
      "Seen so far: 100100 molecules\n",
      "Training time for 20 batches: 0.8003561496734619 s\n",
      "Training loss (for one batch) at step 1020: 0.016650795936584473\n",
      "Training MAE (for one batch) at step 1020: 0.020041437819600105\n",
      "Seen so far: 102100 molecules\n",
      "Training time for 20 batches: 0.8109369277954102 s\n",
      "Training loss (for one batch) at step 1040: 0.016569292172789574\n",
      "Training MAE (for one batch) at step 1040: 0.020938705652952194\n",
      "Seen so far: 104100 molecules\n",
      "Training time for 20 batches: 0.8173999786376953 s\n",
      "Training loss (for one batch) at step 1060: 0.01453049760311842\n",
      "Training MAE (for one batch) at step 1060: 0.020176013931632042\n",
      "Seen so far: 106100 molecules\n",
      "Training time for 20 batches: 0.8127009868621826 s\n",
      "Training loss (for one batch) at step 1080: 0.014558039605617523\n",
      "Training MAE (for one batch) at step 1080: 0.018664296716451645\n",
      "Seen so far: 108100 molecules\n",
      "Training time for 20 batches: 0.8047158718109131 s\n",
      "Training loss (for one batch) at step 1100: 0.012666787952184677\n",
      "Training MAE (for one batch) at step 1100: 0.017501091584563255\n",
      "Seen so far: 110100 molecules\n",
      "Training time for 20 batches: 0.7995400428771973 s\n",
      "Training loss (for one batch) at step 1120: 0.012694602832198143\n",
      "Training MAE (for one batch) at step 1120: 0.02091752178966999\n",
      "Seen so far: 112100 molecules\n",
      "Training time for 20 batches: 0.7972421646118164 s\n",
      "Training loss (for one batch) at step 1140: 0.0128026083111763\n",
      "Training MAE (for one batch) at step 1140: 0.01736639440059662\n",
      "Seen so far: 114100 molecules\n",
      "Training time for 20 batches: 0.7943680286407471 s\n",
      "Training loss (for one batch) at step 1160: 0.01240379735827446\n",
      "Training MAE (for one batch) at step 1160: 0.02091912180185318\n",
      "Seen so far: 116100 molecules\n",
      "Training time for 20 batches: 0.8006088733673096 s\n",
      "Training loss (for one batch) at step 1180: 0.00957498513162136\n",
      "Training MAE (for one batch) at step 1180: 0.017901722341775894\n",
      "Seen so far: 118100 molecules\n",
      "Training time for 20 batches: 0.79860520362854 s\n",
      "Training loss (for one batch) at step 1200: 0.013332865200936794\n",
      "Training MAE (for one batch) at step 1200: 0.016641374677419662\n",
      "Seen so far: 120100 molecules\n",
      "Training time for 20 batches: 0.7996947765350342 s\n",
      "Training loss (for one batch) at step 1220: 0.009707743301987648\n",
      "Training MAE (for one batch) at step 1220: 0.015766676515340805\n",
      "Seen so far: 122100 molecules\n",
      "Training time for 20 batches: 0.8094358444213867 s\n",
      "Training loss (for one batch) at step 1240: 0.011762280017137527\n",
      "Training MAE (for one batch) at step 1240: 0.024223806336522102\n",
      "Seen so far: 124100 molecules\n",
      "Training time for 20 batches: 0.8042323589324951 s\n",
      "Training loss (for one batch) at step 1260: 0.011038945987820625\n",
      "Training MAE (for one batch) at step 1260: 0.017800642177462578\n",
      "Seen so far: 126100 molecules\n",
      "Training time for 20 batches: 0.8156008720397949 s\n",
      "Training loss (for one batch) at step 1280: 0.009328855201601982\n",
      "Training MAE (for one batch) at step 1280: 0.017723403871059418\n",
      "Seen so far: 128100 molecules\n",
      "Training time for 20 batches: 0.8123819828033447 s\n",
      "Training loss (for one batch) at step 1300: 0.008895643055438995\n",
      "Training MAE (for one batch) at step 1300: 0.019505828619003296\n",
      "Seen so far: 130100 molecules\n",
      "Training time for 20 batches: 0.8126230239868164 s\n",
      "Training loss (for one batch) at step 1320: 0.013391930609941483\n",
      "Training MAE (for one batch) at step 1320: 0.018493542447686195\n",
      "Seen so far: 132100 molecules\n",
      "Training time for 20 batches: 0.7997598648071289 s\n",
      "Training loss (for one batch) at step 1340: 0.007993683218955994\n",
      "Training MAE (for one batch) at step 1340: 0.01601349003612995\n",
      "Seen so far: 134100 molecules\n",
      "Training time for 20 batches: 0.7991490364074707 s\n",
      "Training loss (for one batch) at step 1360: 0.00738053023815155\n",
      "Training MAE (for one batch) at step 1360: 0.017511846497654915\n",
      "Seen so far: 136100 molecules\n",
      "Training time for 20 batches: 0.8090040683746338 s\n",
      "Training loss (for one batch) at step 1380: 0.010357451625168324\n",
      "Training MAE (for one batch) at step 1380: 0.017009492963552475\n",
      "Seen so far: 138100 molecules\n",
      "Training time for 20 batches: 0.8013858795166016 s\n",
      "Training loss (for one batch) at step 1400: 0.00941528845578432\n",
      "Training MAE (for one batch) at step 1400: 0.018740996718406677\n",
      "Seen so far: 140100 molecules\n",
      "Training time for 20 batches: 0.8040692806243896 s\n",
      "Training loss (for one batch) at step 1420: 0.00958341360092163\n",
      "Training MAE (for one batch) at step 1420: 0.021706631407141685\n",
      "Seen so far: 142100 molecules\n",
      "Training time for 20 batches: 0.8011939525604248 s\n",
      "Training loss (for one batch) at step 1440: 0.012570262886583805\n",
      "Training MAE (for one batch) at step 1440: 0.02622993476688862\n",
      "Seen so far: 144100 molecules\n",
      "Training time for 20 batches: 0.7987630367279053 s\n",
      "Training loss (for one batch) at step 1460: 0.0075707160867750645\n",
      "Training MAE (for one batch) at step 1460: 0.017318353056907654\n",
      "Seen so far: 146100 molecules\n",
      "Training time for 20 batches: 0.7990257740020752 s\n",
      "Training loss (for one batch) at step 1480: 0.0063504064455628395\n",
      "Training MAE (for one batch) at step 1480: 0.01872490532696247\n",
      "Seen so far: 148100 molecules\n",
      "Training time for 20 batches: 0.8120079040527344 s\n",
      "Training loss (for one batch) at step 1500: 0.010724268853664398\n",
      "Training MAE (for one batch) at step 1500: 0.017242982983589172\n",
      "Seen so far: 150100 molecules\n",
      "Training time for 20 batches: 0.8637361526489258 s\n",
      "Training loss (for one batch) at step 1520: 0.007884039543569088\n",
      "Training MAE (for one batch) at step 1520: 0.022379841655492783\n",
      "Seen so far: 152100 molecules\n",
      "Training time for 20 batches: 0.7996680736541748 s\n",
      "Training loss (for one batch) at step 1540: 0.007109521888196468\n",
      "Training MAE (for one batch) at step 1540: 0.02050764299929142\n",
      "Seen so far: 154100 molecules\n",
      "Training time for 20 batches: 0.7993309497833252 s\n",
      "Training loss (for one batch) at step 1560: 0.009566069580614567\n",
      "Training MAE (for one batch) at step 1560: 0.01847093552350998\n",
      "Seen so far: 156100 molecules\n",
      "Training time for 20 batches: 0.800832986831665 s\n",
      "Training loss (for one batch) at step 1580: 0.009196903556585312\n",
      "Training MAE (for one batch) at step 1580: 0.018072448670864105\n",
      "Seen so far: 158100 molecules\n",
      "Training time for 20 batches: 0.8036150932312012 s\n",
      "Training time for epoch 1: 64.90996193885803 s\n",
      "Starting validation for epoch 1\n",
      "Initial validation loss (for one batch): 0.2741093933582306\n",
      "Initial validation MAE (for one batch): 2.42061185836792\n",
      "Seen so far: 100 molecules\n",
      "Time taken for epoch 1: 65.22037768363953 s\n",
      "\n",
      "Start of epoch 1\n",
      "Initial loss (for one batch): 0.006184331141412258\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 0.009492735378444195\n",
      "Training MAE (for one batch) at step 20: 0.016814909875392914\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 0.9375419616699219 s\n",
      "Training loss (for one batch) at step 40: 0.007706685923039913\n",
      "Training MAE (for one batch) at step 40: 0.019033145159482956\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 0.8039629459381104 s\n",
      "Training loss (for one batch) at step 60: 0.005208288785070181\n",
      "Training MAE (for one batch) at step 60: 0.01625989004969597\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 0.7993929386138916 s\n",
      "Training loss (for one batch) at step 80: 0.00853450782597065\n",
      "Training MAE (for one batch) at step 80: 0.014508215710520744\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 0.8063058853149414 s\n",
      "Training loss (for one batch) at step 100: 0.007901087403297424\n",
      "Training MAE (for one batch) at step 100: 0.018152838572859764\n",
      "Seen so far: 10100 molecules\n",
      "Training time for 20 batches: 0.8027281761169434 s\n",
      "Training loss (for one batch) at step 120: 0.004770986270159483\n",
      "Training MAE (for one batch) at step 120: 0.01512296311557293\n",
      "Seen so far: 12100 molecules\n",
      "Training time for 20 batches: 0.8123948574066162 s\n",
      "Training loss (for one batch) at step 140: 0.009675712324678898\n",
      "Training MAE (for one batch) at step 140: 0.01527312956750393\n",
      "Seen so far: 14100 molecules\n",
      "Training time for 20 batches: 0.8025813102722168 s\n",
      "Training loss (for one batch) at step 160: 0.0074268486350774765\n",
      "Training MAE (for one batch) at step 160: 0.016735762357711792\n",
      "Seen so far: 16100 molecules\n",
      "Training time for 20 batches: 0.8004088401794434 s\n",
      "Training loss (for one batch) at step 180: 0.005718197673559189\n",
      "Training MAE (for one batch) at step 180: 0.016895843669772148\n",
      "Seen so far: 18100 molecules\n",
      "Training time for 20 batches: 0.8728339672088623 s\n",
      "Training loss (for one batch) at step 200: 0.009833362884819508\n",
      "Training MAE (for one batch) at step 200: 0.01408776082098484\n",
      "Seen so far: 20100 molecules\n",
      "Training time for 20 batches: 0.8021609783172607 s\n",
      "Training loss (for one batch) at step 220: 0.00612268503755331\n",
      "Training MAE (for one batch) at step 220: 0.016699161380529404\n",
      "Seen so far: 22100 molecules\n",
      "Training time for 20 batches: 0.805030107498169 s\n",
      "Training loss (for one batch) at step 240: 0.005179321859031916\n",
      "Training MAE (for one batch) at step 240: 0.01583065278828144\n",
      "Seen so far: 24100 molecules\n",
      "Training time for 20 batches: 0.803027868270874 s\n",
      "Training loss (for one batch) at step 260: 0.008066833950579166\n",
      "Training MAE (for one batch) at step 260: 0.013428693637251854\n",
      "Seen so far: 26100 molecules\n",
      "Training time for 20 batches: 0.800325870513916 s\n",
      "Training loss (for one batch) at step 280: 0.005944551434367895\n",
      "Training MAE (for one batch) at step 280: 0.016877293586730957\n",
      "Seen so far: 28100 molecules\n",
      "Training time for 20 batches: 0.798166036605835 s\n",
      "Training loss (for one batch) at step 300: 0.0039903088472783566\n",
      "Training MAE (for one batch) at step 300: 0.01593860611319542\n",
      "Seen so far: 30100 molecules\n",
      "Training time for 20 batches: 0.8021759986877441 s\n",
      "Training loss (for one batch) at step 320: 0.009180237539112568\n",
      "Training MAE (for one batch) at step 320: 0.014173009432852268\n",
      "Seen so far: 32100 molecules\n",
      "Training time for 20 batches: 0.8015141487121582 s\n",
      "Training loss (for one batch) at step 340: 0.006083383224904537\n",
      "Training MAE (for one batch) at step 340: 0.017850549891591072\n",
      "Seen so far: 34100 molecules\n",
      "Training time for 20 batches: 0.8126060962677002 s\n",
      "Training loss (for one batch) at step 360: 0.004556410945951939\n",
      "Training MAE (for one batch) at step 360: 0.01832672767341137\n",
      "Seen so far: 36100 molecules\n",
      "Training time for 20 batches: 0.8048148155212402 s\n",
      "Training loss (for one batch) at step 380: 0.008539525792002678\n",
      "Training MAE (for one batch) at step 380: 0.0152593944221735\n",
      "Seen so far: 38100 molecules\n",
      "Training time for 20 batches: 0.7991161346435547 s\n",
      "Training loss (for one batch) at step 400: 0.005834941752254963\n",
      "Training MAE (for one batch) at step 400: 0.014757986180484295\n",
      "Seen so far: 40100 molecules\n",
      "Training time for 20 batches: 0.8001358509063721 s\n",
      "Training loss (for one batch) at step 420: 0.00468400027602911\n",
      "Training MAE (for one batch) at step 420: 0.01954319141805172\n",
      "Seen so far: 42100 molecules\n",
      "Training time for 20 batches: 0.8071720600128174 s\n",
      "Training loss (for one batch) at step 440: 0.005893780384212732\n",
      "Training MAE (for one batch) at step 440: 0.013869100250303745\n",
      "Seen so far: 44100 molecules\n",
      "Training time for 20 batches: 0.800433874130249 s\n",
      "Training loss (for one batch) at step 460: 0.005203410051763058\n",
      "Training MAE (for one batch) at step 460: 0.015940995886921883\n",
      "Seen so far: 46100 molecules\n",
      "Training time for 20 batches: 0.800990104675293 s\n",
      "Training loss (for one batch) at step 480: 0.0033858013339340687\n",
      "Training MAE (for one batch) at step 480: 0.013795768842101097\n",
      "Seen so far: 48100 molecules\n",
      "Training time for 20 batches: 0.8015480041503906 s\n",
      "Training loss (for one batch) at step 500: 0.008098090998828411\n",
      "Training MAE (for one batch) at step 500: 0.012716667726635933\n",
      "Seen so far: 50100 molecules\n",
      "Training time for 20 batches: 0.8010742664337158 s\n",
      "Training loss (for one batch) at step 520: 0.004017701372504234\n",
      "Training MAE (for one batch) at step 520: 0.015101487748324871\n",
      "Seen so far: 52100 molecules\n",
      "Training time for 20 batches: 0.8003318309783936 s\n",
      "Training loss (for one batch) at step 540: 0.004413404036313295\n",
      "Training MAE (for one batch) at step 540: 0.019578542560338974\n",
      "Seen so far: 54100 molecules\n",
      "Training time for 20 batches: 0.8013710975646973 s\n",
      "Training loss (for one batch) at step 560: 0.009022940881550312\n",
      "Training MAE (for one batch) at step 560: 0.01708000712096691\n",
      "Seen so far: 56100 molecules\n",
      "Training time for 20 batches: 0.819756031036377 s\n",
      "Training loss (for one batch) at step 580: 0.005917700938880444\n",
      "Training MAE (for one batch) at step 580: 0.01650959998369217\n",
      "Seen so far: 58100 molecules\n",
      "Training time for 20 batches: 0.8038859367370605 s\n",
      "Training loss (for one batch) at step 600: 0.00334497494623065\n",
      "Training MAE (for one batch) at step 600: 0.011608997359871864\n",
      "Seen so far: 60100 molecules\n",
      "Training time for 20 batches: 0.798314094543457 s\n",
      "Training loss (for one batch) at step 620: 0.006506489589810371\n",
      "Training MAE (for one batch) at step 620: 0.015098567120730877\n",
      "Seen so far: 62100 molecules\n",
      "Training time for 20 batches: 0.796860933303833 s\n",
      "Training loss (for one batch) at step 640: 0.00491009745746851\n",
      "Training MAE (for one batch) at step 640: 0.014237967319786549\n",
      "Seen so far: 64100 molecules\n",
      "Training time for 20 batches: 0.7998409271240234 s\n",
      "Training loss (for one batch) at step 660: 0.0032255861442536116\n",
      "Training MAE (for one batch) at step 660: 0.013404933735728264\n",
      "Seen so far: 66100 molecules\n",
      "Training time for 20 batches: 0.7993028163909912 s\n",
      "Training loss (for one batch) at step 680: 0.0069220587611198425\n",
      "Training MAE (for one batch) at step 680: 0.012993288226425648\n",
      "Seen so far: 68100 molecules\n",
      "Training time for 20 batches: 0.8035080432891846 s\n",
      "Training loss (for one batch) at step 700: 0.004044724628329277\n",
      "Training MAE (for one batch) at step 700: 0.012429076246917248\n",
      "Seen so far: 70100 molecules\n",
      "Training time for 20 batches: 0.803389310836792 s\n",
      "Training loss (for one batch) at step 720: 0.003383822273463011\n",
      "Training MAE (for one batch) at step 720: 0.012722881510853767\n",
      "Seen so far: 72100 molecules\n",
      "Training time for 20 batches: 0.8004117012023926 s\n",
      "Training loss (for one batch) at step 740: 0.008511636406183243\n",
      "Training MAE (for one batch) at step 740: 0.013065256178379059\n",
      "Seen so far: 74100 molecules\n",
      "Training time for 20 batches: 0.8008110523223877 s\n",
      "Training loss (for one batch) at step 760: 0.004199141636490822\n",
      "Training MAE (for one batch) at step 760: 0.013249990530312061\n",
      "Seen so far: 76100 molecules\n",
      "Training time for 20 batches: 0.8078210353851318 s\n",
      "Training loss (for one batch) at step 780: 0.0036135977134108543\n",
      "Training MAE (for one batch) at step 780: 0.012775719165802002\n",
      "Seen so far: 78100 molecules\n",
      "Training time for 20 batches: 0.816411018371582 s\n",
      "Training loss (for one batch) at step 800: 0.00867626816034317\n",
      "Training MAE (for one batch) at step 800: 0.011553588323295116\n",
      "Seen so far: 80100 molecules\n",
      "Training time for 20 batches: 0.8091099262237549 s\n",
      "Training loss (for one batch) at step 820: 0.004372988361865282\n",
      "Training MAE (for one batch) at step 820: 0.013878749683499336\n",
      "Seen so far: 82100 molecules\n",
      "Training time for 20 batches: 0.7997009754180908 s\n",
      "Training loss (for one batch) at step 840: 0.0030216805171221495\n",
      "Training MAE (for one batch) at step 840: 0.02223821170628071\n",
      "Seen so far: 84100 molecules\n",
      "Training time for 20 batches: 0.8017921447753906 s\n",
      "Training loss (for one batch) at step 860: 0.00638270890340209\n",
      "Training MAE (for one batch) at step 860: 0.011970622465014458\n",
      "Seen so far: 86100 molecules\n",
      "Training time for 20 batches: 0.8003628253936768 s\n",
      "Training loss (for one batch) at step 880: 0.004074563272297382\n",
      "Training MAE (for one batch) at step 880: 0.011802366003394127\n",
      "Seen so far: 88100 molecules\n",
      "Training time for 20 batches: 0.8005352020263672 s\n",
      "Training loss (for one batch) at step 900: 0.0028226070571690798\n",
      "Training MAE (for one batch) at step 900: 0.016187634319067\n",
      "Seen so far: 90100 molecules\n",
      "Training time for 20 batches: 0.7989020347595215 s\n",
      "Training loss (for one batch) at step 920: 0.007400377653539181\n",
      "Training MAE (for one batch) at step 920: 0.013946152292191982\n",
      "Seen so far: 92100 molecules\n",
      "Training time for 20 batches: 0.8002936840057373 s\n",
      "Training loss (for one batch) at step 940: 0.004066694527864456\n",
      "Training MAE (for one batch) at step 940: 0.01183998305350542\n",
      "Seen so far: 94100 molecules\n",
      "Training time for 20 batches: 0.8008613586425781 s\n",
      "Training loss (for one batch) at step 960: 0.004190938081592321\n",
      "Training MAE (for one batch) at step 960: 0.014368544332683086\n",
      "Seen so far: 96100 molecules\n",
      "Training time for 20 batches: 0.8638079166412354 s\n",
      "Training loss (for one batch) at step 980: 0.00529130594804883\n",
      "Training MAE (for one batch) at step 980: 0.01171180047094822\n",
      "Seen so far: 98100 molecules\n",
      "Training time for 20 batches: 0.8044838905334473 s\n",
      "Training loss (for one batch) at step 1000: 0.0048265908844769\n",
      "Training MAE (for one batch) at step 1000: 0.0345379002392292\n",
      "Seen so far: 100100 molecules\n",
      "Training time for 20 batches: 0.8177211284637451 s\n",
      "Training loss (for one batch) at step 1020: 0.010921456851065159\n",
      "Training MAE (for one batch) at step 1020: 0.013866539113223553\n",
      "Seen so far: 102100 molecules\n",
      "Training time for 20 batches: 0.8005969524383545 s\n",
      "Training loss (for one batch) at step 1040: 0.007181056775152683\n",
      "Training MAE (for one batch) at step 1040: 0.013611127622425556\n",
      "Seen so far: 104100 molecules\n",
      "Training time for 20 batches: 0.7998430728912354 s\n",
      "Training loss (for one batch) at step 1060: 0.02260083332657814\n",
      "Training MAE (for one batch) at step 1060: 0.018095405772328377\n",
      "Seen so far: 106100 molecules\n",
      "Training time for 20 batches: 0.7992708683013916 s\n",
      "Training loss (for one batch) at step 1080: 0.0031132586300373077\n",
      "Training MAE (for one batch) at step 1080: 0.009049284271895885\n",
      "Seen so far: 108100 molecules\n",
      "Training time for 20 batches: 0.8674628734588623 s\n",
      "Training loss (for one batch) at step 1100: 0.007481179665774107\n",
      "Training MAE (for one batch) at step 1100: 0.01583496667444706\n",
      "Seen so far: 110100 molecules\n",
      "Training time for 20 batches: 0.8119940757751465 s\n",
      "Training loss (for one batch) at step 1120: 0.003094420302659273\n",
      "Training MAE (for one batch) at step 1120: 0.023606153205037117\n",
      "Seen so far: 112100 molecules\n",
      "Training time for 20 batches: 0.8013041019439697 s\n",
      "Training loss (for one batch) at step 1140: 0.002480651717633009\n",
      "Training MAE (for one batch) at step 1140: 0.01525938231498003\n",
      "Seen so far: 114100 molecules\n",
      "Training time for 20 batches: 0.8031308650970459 s\n",
      "Training loss (for one batch) at step 1160: 0.013337664306163788\n",
      "Training MAE (for one batch) at step 1160: 0.018066534772515297\n",
      "Seen so far: 116100 molecules\n",
      "Training time for 20 batches: 0.8020381927490234 s\n",
      "Training loss (for one batch) at step 1180: 0.005360234994441271\n",
      "Training MAE (for one batch) at step 1180: 0.018080847337841988\n",
      "Seen so far: 118100 molecules\n",
      "Training time for 20 batches: 0.80519700050354 s\n",
      "Training loss (for one batch) at step 1200: 0.007259786128997803\n",
      "Training MAE (for one batch) at step 1200: 0.01938965730369091\n",
      "Seen so far: 120100 molecules\n",
      "Training time for 20 batches: 0.8019628524780273 s\n",
      "Training loss (for one batch) at step 1220: 0.0036932576913386583\n",
      "Training MAE (for one batch) at step 1220: 0.010835873894393444\n",
      "Seen so far: 122100 molecules\n",
      "Training time for 20 batches: 0.815544843673706 s\n",
      "Training loss (for one batch) at step 1240: 0.0020876016933470964\n",
      "Training MAE (for one batch) at step 1240: 0.016277380287647247\n",
      "Seen so far: 124100 molecules\n",
      "Training time for 20 batches: 0.7999811172485352 s\n",
      "Training loss (for one batch) at step 1260: 0.006700340658426285\n",
      "Training MAE (for one batch) at step 1260: 0.010678835213184357\n",
      "Seen so far: 126100 molecules\n",
      "Training time for 20 batches: 0.8000671863555908 s\n",
      "Training loss (for one batch) at step 1280: 0.004917239770293236\n",
      "Training MAE (for one batch) at step 1280: 0.012019429355859756\n",
      "Seen so far: 128100 molecules\n",
      "Training time for 20 batches: 0.8053958415985107 s\n",
      "Training loss (for one batch) at step 1300: 0.002078515477478504\n",
      "Training MAE (for one batch) at step 1300: 0.009994444437325\n",
      "Seen so far: 130100 molecules\n",
      "Training time for 20 batches: 0.8005771636962891 s\n",
      "Training loss (for one batch) at step 1320: 0.00777479400858283\n",
      "Training MAE (for one batch) at step 1320: 0.0171260517090559\n",
      "Seen so far: 132100 molecules\n",
      "Training time for 20 batches: 0.8002548217773438 s\n",
      "Training loss (for one batch) at step 1340: 0.0025738512631505728\n",
      "Training MAE (for one batch) at step 1340: 0.008539596572518349\n",
      "Seen so far: 134100 molecules\n",
      "Training time for 20 batches: 0.7986009120941162 s\n",
      "Training loss (for one batch) at step 1360: 0.002464503515511751\n",
      "Training MAE (for one batch) at step 1360: 0.01245261449366808\n",
      "Seen so far: 136100 molecules\n",
      "Training time for 20 batches: 0.799504280090332 s\n",
      "Training loss (for one batch) at step 1380: 0.005349756218492985\n",
      "Training MAE (for one batch) at step 1380: 0.015046749264001846\n",
      "Seen so far: 138100 molecules\n",
      "Training time for 20 batches: 0.8030200004577637 s\n",
      "Training loss (for one batch) at step 1400: 0.0028268839232623577\n",
      "Training MAE (for one batch) at step 1400: 0.008307809010148048\n",
      "Seen so far: 140100 molecules\n",
      "Training time for 20 batches: 0.7981109619140625 s\n",
      "Training loss (for one batch) at step 1420: 0.0023347404785454273\n",
      "Training MAE (for one batch) at step 1420: 0.010381949134171009\n",
      "Seen so far: 142100 molecules\n",
      "Training time for 20 batches: 0.8053638935089111 s\n",
      "Training loss (for one batch) at step 1440: 0.0053178961388766766\n",
      "Training MAE (for one batch) at step 1440: 0.009516114369034767\n",
      "Seen so far: 144100 molecules\n",
      "Training time for 20 batches: 0.8208820819854736 s\n",
      "Training loss (for one batch) at step 1460: 0.003211145754903555\n",
      "Training MAE (for one batch) at step 1460: 0.011065171100199223\n",
      "Seen so far: 146100 molecules\n",
      "Training time for 20 batches: 0.8020439147949219 s\n",
      "Training loss (for one batch) at step 1480: 0.0026411558501422405\n",
      "Training MAE (for one batch) at step 1480: 0.0093558793887496\n",
      "Seen so far: 148100 molecules\n",
      "Training time for 20 batches: 0.8028111457824707 s\n",
      "Training loss (for one batch) at step 1500: 0.004860179964452982\n",
      "Training MAE (for one batch) at step 1500: 0.008795472793281078\n",
      "Seen so far: 150100 molecules\n",
      "Training time for 20 batches: 0.7978768348693848 s\n",
      "Training loss (for one batch) at step 1520: 0.0035836240276694298\n",
      "Training MAE (for one batch) at step 1520: 0.008822835050523281\n",
      "Seen so far: 152100 molecules\n",
      "Training time for 20 batches: 0.7981278896331787 s\n",
      "Training loss (for one batch) at step 1540: 0.001878288690932095\n",
      "Training MAE (for one batch) at step 1540: 0.01171443797647953\n",
      "Seen so far: 154100 molecules\n",
      "Training time for 20 batches: 0.7979471683502197 s\n",
      "Training loss (for one batch) at step 1560: 0.0031413105316460133\n",
      "Training MAE (for one batch) at step 1560: 0.009234136901795864\n",
      "Seen so far: 156100 molecules\n",
      "Training time for 20 batches: 0.7990210056304932 s\n",
      "Training loss (for one batch) at step 1580: 0.003321362193673849\n",
      "Training MAE (for one batch) at step 1580: 0.009204039350152016\n",
      "Seen so far: 158100 molecules\n",
      "Training time for 20 batches: 0.8074021339416504 s\n",
      "Training time for epoch 2: 64.46794486045837 s\n",
      "Starting validation for epoch 2\n",
      "Initial validation loss (for one batch): 0.27640536427497864\n",
      "Initial validation MAE (for one batch): 0.8078740835189819\n",
      "Seen so far: 100 molecules\n",
      "Time taken for epoch 2: 64.7789659500122 s\n",
      "\n",
      "Start of epoch 2\n",
      "Initial loss (for one batch): 0.002121485536918044\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 0.0036976004485040903\n",
      "Training MAE (for one batch) at step 20: 0.008584773167967796\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 0.9345409870147705 s\n",
      "Training loss (for one batch) at step 40: 0.0029071413446217775\n",
      "Training MAE (for one batch) at step 40: 0.009854136034846306\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 0.8213279247283936 s\n",
      "Training loss (for one batch) at step 60: 0.0012968116207048297\n",
      "Training MAE (for one batch) at step 60: 0.00688081094995141\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 0.8010520935058594 s\n",
      "Training loss (for one batch) at step 80: 0.006971533875912428\n",
      "Training MAE (for one batch) at step 80: 0.017550047487020493\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 0.8002488613128662 s\n",
      "Training loss (for one batch) at step 100: 0.002738424576818943\n",
      "Training MAE (for one batch) at step 100: 0.007257175166159868\n",
      "Seen so far: 10100 molecules\n",
      "Training time for 20 batches: 0.8041131496429443 s\n",
      "Training loss (for one batch) at step 120: 0.0015296947676688433\n",
      "Training MAE (for one batch) at step 120: 0.010370864532887936\n",
      "Seen so far: 12100 molecules\n",
      "Training time for 20 batches: 0.8050270080566406 s\n",
      "Training loss (for one batch) at step 140: 0.003860543016344309\n",
      "Training MAE (for one batch) at step 140: 0.014893517829477787\n",
      "Seen so far: 14100 molecules\n",
      "Training time for 20 batches: 0.8031859397888184 s\n",
      "Training loss (for one batch) at step 160: 0.0020109377801418304\n",
      "Training MAE (for one batch) at step 160: 0.006831655744463205\n",
      "Seen so far: 16100 molecules\n",
      "Training time for 20 batches: 0.8006789684295654 s\n",
      "Training loss (for one batch) at step 180: 0.0035674460232257843\n",
      "Training MAE (for one batch) at step 180: 0.007387590128928423\n",
      "Seen so far: 18100 molecules\n",
      "Training time for 20 batches: 0.8019680976867676 s\n",
      "Training loss (for one batch) at step 200: 0.003727479139342904\n",
      "Training MAE (for one batch) at step 200: 0.01156171876937151\n",
      "Seen so far: 20100 molecules\n",
      "Training time for 20 batches: 0.799030065536499 s\n",
      "Training loss (for one batch) at step 220: 0.0026034663897007704\n",
      "Training MAE (for one batch) at step 220: 0.007361719384789467\n",
      "Seen so far: 22100 molecules\n",
      "Training time for 20 batches: 0.7987508773803711 s\n",
      "Training loss (for one batch) at step 240: 0.0014182584127411246\n",
      "Training MAE (for one batch) at step 240: 0.007160946726799011\n",
      "Seen so far: 24100 molecules\n",
      "Training time for 20 batches: 0.8025202751159668 s\n",
      "Training loss (for one batch) at step 260: 0.005757336039096117\n",
      "Training MAE (for one batch) at step 260: 0.016753824427723885\n",
      "Seen so far: 26100 molecules\n",
      "Training time for 20 batches: 0.8149077892303467 s\n",
      "Training loss (for one batch) at step 280: 0.0031880605965852737\n",
      "Training MAE (for one batch) at step 280: 0.011641775257885456\n",
      "Seen so far: 28100 molecules\n",
      "Training time for 20 batches: 0.8034088611602783 s\n",
      "Training loss (for one batch) at step 300: 0.002671171212568879\n",
      "Training MAE (for one batch) at step 300: 0.007153226062655449\n",
      "Seen so far: 30100 molecules\n",
      "Training time for 20 batches: 0.8830640316009521 s\n",
      "Training loss (for one batch) at step 320: 0.005100857466459274\n",
      "Training MAE (for one batch) at step 320: 0.010505463927984238\n",
      "Seen so far: 32100 molecules\n",
      "Training time for 20 batches: 0.8029160499572754 s\n",
      "Training loss (for one batch) at step 340: 0.0024395515210926533\n",
      "Training MAE (for one batch) at step 340: 0.0071126180700957775\n",
      "Seen so far: 34100 molecules\n",
      "Training time for 20 batches: 0.8013269901275635 s\n",
      "Training loss (for one batch) at step 360: 0.001283601624891162\n",
      "Training MAE (for one batch) at step 360: 0.006907411385327578\n",
      "Seen so far: 36100 molecules\n",
      "Training time for 20 batches: 0.8029959201812744 s\n",
      "Training loss (for one batch) at step 380: 0.00433863140642643\n",
      "Training MAE (for one batch) at step 380: 0.013164489530026913\n",
      "Seen so far: 38100 molecules\n",
      "Training time for 20 batches: 0.79888916015625 s\n",
      "Training loss (for one batch) at step 400: 0.0026468767318874598\n",
      "Training MAE (for one batch) at step 400: 0.011306988075375557\n",
      "Seen so far: 40100 molecules\n",
      "Training time for 20 batches: 0.8012099266052246 s\n",
      "Training loss (for one batch) at step 420: 0.003323675598949194\n",
      "Training MAE (for one batch) at step 420: 0.007553531788289547\n",
      "Seen so far: 42100 molecules\n",
      "Training time for 20 batches: 0.8651130199432373 s\n",
      "Training loss (for one batch) at step 440: 0.0068786898627877235\n",
      "Training MAE (for one batch) at step 440: 0.015269557945430279\n",
      "Seen so far: 44100 molecules\n",
      "Training time for 20 batches: 0.8048760890960693 s\n",
      "Training loss (for one batch) at step 460: 0.004709072411060333\n",
      "Training MAE (for one batch) at step 460: 0.008500440046191216\n",
      "Seen so far: 46100 molecules\n",
      "Training time for 20 batches: 0.8036849498748779 s\n",
      "Training loss (for one batch) at step 480: 0.0022776660043746233\n",
      "Training MAE (for one batch) at step 480: 0.0067160106264054775\n",
      "Seen so far: 48100 molecules\n",
      "Training time for 20 batches: 0.8128089904785156 s\n",
      "Training loss (for one batch) at step 500: 0.0029981646221131086\n",
      "Training MAE (for one batch) at step 500: 0.009226007387042046\n",
      "Seen so far: 50100 molecules\n",
      "Training time for 20 batches: 0.8032858371734619 s\n",
      "Training loss (for one batch) at step 520: 0.002957238582894206\n",
      "Training MAE (for one batch) at step 520: 0.009200835600495338\n",
      "Seen so far: 52100 molecules\n",
      "Training time for 20 batches: 0.8011519908905029 s\n",
      "Training loss (for one batch) at step 540: 0.002381647936999798\n",
      "Training MAE (for one batch) at step 540: 0.008224556222558022\n",
      "Seen so far: 54100 molecules\n",
      "Training time for 20 batches: 0.8012371063232422 s\n",
      "Training loss (for one batch) at step 560: 0.0071459803730249405\n",
      "Training MAE (for one batch) at step 560: 0.012719117105007172\n",
      "Seen so far: 56100 molecules\n",
      "Training time for 20 batches: 0.800037145614624 s\n",
      "Training loss (for one batch) at step 580: 0.0018826662562787533\n",
      "Training MAE (for one batch) at step 580: 0.009581086225807667\n",
      "Seen so far: 58100 molecules\n",
      "Training time for 20 batches: 0.8058969974517822 s\n",
      "Training loss (for one batch) at step 600: 0.0012642130022868514\n",
      "Training MAE (for one batch) at step 600: 0.0063351416029036045\n",
      "Seen so far: 60100 molecules\n",
      "Training time for 20 batches: 0.8012747764587402 s\n",
      "Training loss (for one batch) at step 620: 0.005541439168155193\n",
      "Training MAE (for one batch) at step 620: 0.010192632675170898\n",
      "Seen so far: 62100 molecules\n",
      "Training time for 20 batches: 0.8009340763092041 s\n",
      "Training loss (for one batch) at step 640: 0.002965276362374425\n",
      "Training MAE (for one batch) at step 640: 0.008810839615762234\n",
      "Seen so far: 64100 molecules\n",
      "Training time for 20 batches: 0.8094191551208496 s\n",
      "Training loss (for one batch) at step 660: 0.0030681011267006397\n",
      "Training MAE (for one batch) at step 660: 0.009533725678920746\n",
      "Seen so far: 66100 molecules\n",
      "Training time for 20 batches: 0.8008780479431152 s\n",
      "Training loss (for one batch) at step 680: 0.004791153594851494\n",
      "Training MAE (for one batch) at step 680: 0.013075752183794975\n",
      "Seen so far: 68100 molecules\n",
      "Training time for 20 batches: 0.8014688491821289 s\n",
      "Training loss (for one batch) at step 700: 0.0015673101879656315\n",
      "Training MAE (for one batch) at step 700: 0.010177492164075375\n",
      "Seen so far: 70100 molecules\n",
      "Training time for 20 batches: 0.8175628185272217 s\n",
      "Training loss (for one batch) at step 720: 0.0011816303012892604\n",
      "Training MAE (for one batch) at step 720: 0.005744017660617828\n",
      "Seen so far: 72100 molecules\n",
      "Training time for 20 batches: 0.8046741485595703 s\n",
      "Training loss (for one batch) at step 740: 0.00285052927210927\n",
      "Training MAE (for one batch) at step 740: 0.012592382729053497\n",
      "Seen so far: 74100 molecules\n",
      "Training time for 20 batches: 0.8020548820495605 s\n",
      "Training loss (for one batch) at step 760: 0.0019580761436372995\n",
      "Training MAE (for one batch) at step 760: 0.01934095472097397\n",
      "Seen so far: 76100 molecules\n",
      "Training time for 20 batches: 0.8011062145233154 s\n",
      "Training loss (for one batch) at step 780: 0.0010252445936203003\n",
      "Training MAE (for one batch) at step 780: 0.006726815830916166\n",
      "Seen so far: 78100 molecules\n",
      "Training time for 20 batches: 0.8020470142364502 s\n",
      "Training loss (for one batch) at step 800: 0.006162151228636503\n",
      "Training MAE (for one batch) at step 800: 0.01172199659049511\n",
      "Seen so far: 80100 molecules\n",
      "Training time for 20 batches: 0.8068089485168457 s\n",
      "Training loss (for one batch) at step 820: 0.0030607942026108503\n",
      "Training MAE (for one batch) at step 820: 0.006731322500854731\n",
      "Seen so far: 82100 molecules\n",
      "Training time for 20 batches: 0.8010621070861816 s\n",
      "Training loss (for one batch) at step 840: 0.0017735559958964586\n",
      "Training MAE (for one batch) at step 840: 0.007302667945623398\n",
      "Seen so far: 84100 molecules\n",
      "Training time for 20 batches: 0.8011319637298584 s\n",
      "Training loss (for one batch) at step 860: 0.0038215431850403547\n",
      "Training MAE (for one batch) at step 860: 0.012776379473507404\n",
      "Seen so far: 86100 molecules\n",
      "Training time for 20 batches: 0.8046479225158691 s\n",
      "Training loss (for one batch) at step 880: 0.0029044728726148605\n",
      "Training MAE (for one batch) at step 880: 0.005284261889755726\n",
      "Seen so far: 88100 molecules\n",
      "Training time for 20 batches: 0.8019797801971436 s\n",
      "Training loss (for one batch) at step 900: 0.002998118521645665\n",
      "Training MAE (for one batch) at step 900: 0.006641165353357792\n",
      "Seen so far: 90100 molecules\n",
      "Training time for 20 batches: 0.8026652336120605 s\n",
      "Training loss (for one batch) at step 920: 0.004616668913513422\n",
      "Training MAE (for one batch) at step 920: 0.011870921589434147\n",
      "Seen so far: 92100 molecules\n",
      "Training time for 20 batches: 0.8148040771484375 s\n",
      "Training loss (for one batch) at step 940: 0.002046664711087942\n",
      "Training MAE (for one batch) at step 940: 0.008687514811754227\n",
      "Seen so far: 94100 molecules\n",
      "Training time for 20 batches: 0.7994837760925293 s\n",
      "Training loss (for one batch) at step 960: 0.0013227516319602728\n",
      "Training MAE (for one batch) at step 960: 0.006949762348085642\n",
      "Seen so far: 96100 molecules\n",
      "Training time for 20 batches: 0.8027021884918213 s\n",
      "Training loss (for one batch) at step 980: 0.003413172671571374\n",
      "Training MAE (for one batch) at step 980: 0.010904839262366295\n",
      "Seen so far: 98100 molecules\n",
      "Training time for 20 batches: 0.8081929683685303 s\n",
      "Training loss (for one batch) at step 1000: 0.0023275690618902445\n",
      "Training MAE (for one batch) at step 1000: 0.006070451345294714\n",
      "Seen so far: 100100 molecules\n",
      "Training time for 20 batches: 0.7996208667755127 s\n",
      "Training loss (for one batch) at step 1020: 0.0013095145113766193\n",
      "Training MAE (for one batch) at step 1020: 0.005366405472159386\n",
      "Seen so far: 102100 molecules\n",
      "Training time for 20 batches: 0.8052310943603516 s\n",
      "Training loss (for one batch) at step 1040: 0.0037407877389341593\n",
      "Training MAE (for one batch) at step 1040: 0.00850697048008442\n",
      "Seen so far: 104100 molecules\n",
      "Training time for 20 batches: 0.7995460033416748 s\n",
      "Training loss (for one batch) at step 1060: 0.0017632354283705354\n",
      "Training MAE (for one batch) at step 1060: 0.006013181991875172\n",
      "Seen so far: 106100 molecules\n",
      "Training time for 20 batches: 0.8034169673919678 s\n",
      "Training loss (for one batch) at step 1080: 0.0013567747082561255\n",
      "Training MAE (for one batch) at step 1080: 0.012862663716077805\n",
      "Seen so far: 108100 molecules\n",
      "Training time for 20 batches: 0.8032541275024414 s\n",
      "Training loss (for one batch) at step 1100: 0.0036221803165972233\n",
      "Training MAE (for one batch) at step 1100: 0.00701617356389761\n",
      "Seen so far: 110100 molecules\n",
      "Training time for 20 batches: 0.8027539253234863 s\n",
      "Training loss (for one batch) at step 1120: 0.0024073906242847443\n",
      "Training MAE (for one batch) at step 1120: 0.007614684756845236\n",
      "Seen so far: 112100 molecules\n",
      "Training time for 20 batches: 0.8016998767852783 s\n",
      "Training loss (for one batch) at step 1140: 0.0009923288598656654\n",
      "Training MAE (for one batch) at step 1140: 0.011397539637982845\n",
      "Seen so far: 114100 molecules\n",
      "Training time for 20 batches: 0.884174108505249 s\n",
      "Training loss (for one batch) at step 1160: 0.005469804629683495\n",
      "Training MAE (for one batch) at step 1160: 0.028358489274978638\n",
      "Seen so far: 116100 molecules\n",
      "Training time for 20 batches: 0.8081090450286865 s\n",
      "Training loss (for one batch) at step 1180: 0.0011662057368084788\n",
      "Training MAE (for one batch) at step 1180: 0.01171388104557991\n",
      "Seen so far: 118100 molecules\n",
      "Training time for 20 batches: 0.8007948398590088 s\n",
      "Training loss (for one batch) at step 1200: 0.00539482431486249\n",
      "Training MAE (for one batch) at step 1200: 0.009337939321994781\n",
      "Seen so far: 120100 molecules\n",
      "Training time for 20 batches: 0.8024711608886719 s\n",
      "Training loss (for one batch) at step 1220: 0.0028904571663588285\n",
      "Training MAE (for one batch) at step 1220: 0.01095354463905096\n",
      "Seen so far: 122100 molecules\n",
      "Training time for 20 batches: 0.8027358055114746 s\n",
      "Training loss (for one batch) at step 1240: 0.002310285810381174\n",
      "Training MAE (for one batch) at step 1240: 0.009151924401521683\n",
      "Seen so far: 124100 molecules\n",
      "Training time for 20 batches: 0.8006300926208496 s\n",
      "Training loss (for one batch) at step 1260: 0.005724670365452766\n",
      "Training MAE (for one batch) at step 1260: 0.011639216914772987\n",
      "Seen so far: 126100 molecules\n",
      "Training time for 20 batches: 0.8022830486297607 s\n",
      "Training loss (for one batch) at step 1280: 0.003833068534731865\n",
      "Training MAE (for one batch) at step 1280: 0.009663918055593967\n",
      "Seen so far: 128100 molecules\n",
      "Training time for 20 batches: 0.8020100593566895 s\n",
      "Training loss (for one batch) at step 1300: 0.001889835111796856\n",
      "Training MAE (for one batch) at step 1300: 0.01809088885784149\n",
      "Seen so far: 130100 molecules\n",
      "Training time for 20 batches: 0.8063077926635742 s\n",
      "Training loss (for one batch) at step 1320: 0.0038132991176098585\n",
      "Training MAE (for one batch) at step 1320: 0.006203996017575264\n",
      "Seen so far: 132100 molecules\n",
      "Training time for 20 batches: 0.8100411891937256 s\n",
      "Training loss (for one batch) at step 1340: 0.0021610150579363108\n",
      "Training MAE (for one batch) at step 1340: 0.008180070668458939\n",
      "Seen so far: 134100 molecules\n",
      "Training time for 20 batches: 0.8024439811706543 s\n",
      "Training loss (for one batch) at step 1360: 0.0018606269732117653\n",
      "Training MAE (for one batch) at step 1360: 0.007457034196704626\n",
      "Seen so far: 136100 molecules\n",
      "Training time for 20 batches: 0.8127450942993164 s\n",
      "Training loss (for one batch) at step 1380: 0.005002187564969063\n",
      "Training MAE (for one batch) at step 1380: 0.005898221395909786\n",
      "Seen so far: 138100 molecules\n",
      "Training time for 20 batches: 0.8026609420776367 s\n",
      "Training loss (for one batch) at step 1400: 0.0016939195338636637\n",
      "Training MAE (for one batch) at step 1400: 0.012447478249669075\n",
      "Seen so far: 140100 molecules\n",
      "Training time for 20 batches: 0.8012378215789795 s\n",
      "Training loss (for one batch) at step 1420: 0.001598296919837594\n",
      "Training MAE (for one batch) at step 1420: 0.006578960455954075\n",
      "Seen so far: 142100 molecules\n",
      "Training time for 20 batches: 0.8036031723022461 s\n",
      "Training loss (for one batch) at step 1440: 0.003899630857631564\n",
      "Training MAE (for one batch) at step 1440: 0.008666756562888622\n",
      "Seen so far: 144100 molecules\n",
      "Training time for 20 batches: 0.8100848197937012 s\n",
      "Training loss (for one batch) at step 1460: 0.0023965074215084314\n",
      "Training MAE (for one batch) at step 1460: 0.0070956056006252766\n",
      "Seen so far: 146100 molecules\n",
      "Training time for 20 batches: 0.8034360408782959 s\n",
      "Training loss (for one batch) at step 1480: 0.0012548742815852165\n",
      "Training MAE (for one batch) at step 1480: 0.01399910543113947\n",
      "Seen so far: 148100 molecules\n",
      "Training time for 20 batches: 0.8612821102142334 s\n",
      "Training loss (for one batch) at step 1500: 0.0032640716526657343\n",
      "Training MAE (for one batch) at step 1500: 0.006394091993570328\n",
      "Seen so far: 150100 molecules\n",
      "Training time for 20 batches: 0.803947925567627 s\n",
      "Training loss (for one batch) at step 1520: 0.0010472055291756988\n",
      "Training MAE (for one batch) at step 1520: 0.012126314453780651\n",
      "Seen so far: 152100 molecules\n",
      "Training time for 20 batches: 0.8023471832275391 s\n",
      "Training loss (for one batch) at step 1540: 0.0009268881403841078\n",
      "Training MAE (for one batch) at step 1540: 0.0071820467710494995\n",
      "Seen so far: 154100 molecules\n",
      "Training time for 20 batches: 0.8033280372619629 s\n",
      "Training loss (for one batch) at step 1560: 0.002869843039661646\n",
      "Training MAE (for one batch) at step 1560: 0.017126308754086494\n",
      "Seen so far: 156100 molecules\n",
      "Training time for 20 batches: 0.8009300231933594 s\n",
      "Training loss (for one batch) at step 1580: 0.001014131586998701\n",
      "Training MAE (for one batch) at step 1580: 0.013601209037005901\n",
      "Seen so far: 158100 molecules\n",
      "Training time for 20 batches: 0.8148789405822754 s\n",
      "Training time for epoch 3: 64.62090587615967 s\n",
      "Starting validation for epoch 3\n",
      "Initial validation loss (for one batch): 0.2755446135997772\n",
      "Initial validation MAE (for one batch): 0.7582541704177856\n",
      "Seen so far: 100 molecules\n",
      "Time taken for epoch 3: 64.93275809288025 s\n",
      "\n",
      "Start of epoch 3\n",
      "Initial loss (for one batch): 0.0006567379459738731\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 0.0011930500622838736\n",
      "Training MAE (for one batch) at step 20: 0.012220355682075024\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 0.9234457015991211 s\n",
      "Training loss (for one batch) at step 40: 0.0016185782151296735\n",
      "Training MAE (for one batch) at step 40: 0.009526324458420277\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 0.8122251033782959 s\n",
      "Training loss (for one batch) at step 60: 0.0024993796832859516\n",
      "Training MAE (for one batch) at step 60: 0.006255487445741892\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 0.8025050163269043 s\n",
      "Training loss (for one batch) at step 80: 0.0009073069086298347\n",
      "Training MAE (for one batch) at step 80: 0.0048253461718559265\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 0.8030490875244141 s\n",
      "Training loss (for one batch) at step 100: 0.0011738992761820555\n",
      "Training MAE (for one batch) at step 100: 0.016606971621513367\n",
      "Seen so far: 10100 molecules\n",
      "Training time for 20 batches: 0.7970778942108154 s\n",
      "Training loss (for one batch) at step 120: 0.0062791709788143635\n",
      "Training MAE (for one batch) at step 120: 0.03685414418578148\n",
      "Seen so far: 12100 molecules\n",
      "Training time for 20 batches: 0.8065609931945801 s\n",
      "Training loss (for one batch) at step 140: 0.003355601103976369\n",
      "Training MAE (for one batch) at step 140: 0.017423946410417557\n",
      "Seen so far: 14100 molecules\n",
      "Training time for 20 batches: 0.8160920143127441 s\n",
      "Training loss (for one batch) at step 160: 0.0061299484223127365\n",
      "Training MAE (for one batch) at step 160: 0.009947685524821281\n",
      "Seen so far: 16100 molecules\n",
      "Training time for 20 batches: 0.8031418323516846 s\n",
      "Training loss (for one batch) at step 180: 0.0017758255125954747\n",
      "Training MAE (for one batch) at step 180: 0.007002739701420069\n",
      "Seen so far: 18100 molecules\n",
      "Training time for 20 batches: 0.8010222911834717 s\n",
      "Training loss (for one batch) at step 200: 0.0016331515507772565\n",
      "Training MAE (for one batch) at step 200: 0.00794086791574955\n",
      "Seen so far: 20100 molecules\n",
      "Training time for 20 batches: 0.805506706237793 s\n",
      "Training loss (for one batch) at step 220: 0.0047019780613482\n",
      "Training MAE (for one batch) at step 220: 0.010894563980400562\n",
      "Seen so far: 22100 molecules\n",
      "Training time for 20 batches: 0.8012511730194092 s\n",
      "Training loss (for one batch) at step 240: 0.0022330915089696646\n",
      "Training MAE (for one batch) at step 240: 0.005431579425930977\n",
      "Seen so far: 24100 molecules\n",
      "Training time for 20 batches: 0.8011069297790527 s\n",
      "Training loss (for one batch) at step 260: 0.002033090917393565\n",
      "Training MAE (for one batch) at step 260: 0.009064395911991596\n",
      "Seen so far: 26100 molecules\n",
      "Training time for 20 batches: 0.8037281036376953 s\n",
      "Training loss (for one batch) at step 280: 0.0030955676920711994\n",
      "Training MAE (for one batch) at step 280: 0.010999813675880432\n",
      "Seen so far: 28100 molecules\n",
      "Training time for 20 batches: 0.8772029876708984 s\n",
      "Training loss (for one batch) at step 300: 0.002818260109052062\n",
      "Training MAE (for one batch) at step 300: 0.01035447046160698\n",
      "Seen so far: 30100 molecules\n",
      "Training time for 20 batches: 0.8042497634887695 s\n",
      "Training loss (for one batch) at step 320: 0.0016495975432917476\n",
      "Training MAE (for one batch) at step 320: 0.00713178189471364\n",
      "Seen so far: 32100 molecules\n",
      "Training time for 20 batches: 0.7998132705688477 s\n",
      "Training loss (for one batch) at step 340: 0.0031456188298761845\n",
      "Training MAE (for one batch) at step 340: 0.007000402081757784\n",
      "Seen so far: 34100 molecules\n",
      "Training time for 20 batches: 0.7991328239440918 s\n",
      "Training loss (for one batch) at step 360: 0.004186014644801617\n",
      "Training MAE (for one batch) at step 360: 0.01377498172223568\n",
      "Seen so far: 36100 molecules\n",
      "Training time for 20 batches: 0.812204122543335 s\n",
      "Training loss (for one batch) at step 380: 0.0012228463310748339\n",
      "Training MAE (for one batch) at step 380: 0.004782889969646931\n",
      "Seen so far: 38100 molecules\n",
      "Training time for 20 batches: 0.8029220104217529 s\n",
      "Training loss (for one batch) at step 400: 0.00390420388430357\n",
      "Training MAE (for one batch) at step 400: 0.010089344345033169\n",
      "Seen so far: 40100 molecules\n",
      "Training time for 20 batches: 0.8138511180877686 s\n",
      "Training loss (for one batch) at step 420: 0.0021890858188271523\n",
      "Training MAE (for one batch) at step 420: 0.009520933963358402\n",
      "Seen so far: 42100 molecules\n",
      "Training time for 20 batches: 0.8015248775482178 s\n",
      "Training loss (for one batch) at step 440: 0.0014773332513868809\n",
      "Training MAE (for one batch) at step 440: 0.004790974780917168\n",
      "Seen so far: 44100 molecules\n",
      "Training time for 20 batches: 0.8034520149230957 s\n",
      "Training loss (for one batch) at step 460: 0.0036182007752358913\n",
      "Training MAE (for one batch) at step 460: 0.013003425672650337\n",
      "Seen so far: 46100 molecules\n",
      "Training time for 20 batches: 0.8017971515655518 s\n",
      "Training loss (for one batch) at step 480: 0.0029228022322058678\n",
      "Training MAE (for one batch) at step 480: 0.008565081283450127\n",
      "Seen so far: 48100 molecules\n",
      "Training time for 20 batches: 0.8032467365264893 s\n",
      "Training loss (for one batch) at step 500: 0.002519856672734022\n",
      "Training MAE (for one batch) at step 500: 0.00606377562507987\n",
      "Seen so far: 50100 molecules\n",
      "Training time for 20 batches: 0.8018510341644287 s\n",
      "Training loss (for one batch) at step 520: 0.004946887958794832\n",
      "Training MAE (for one batch) at step 520: 0.009591416455805302\n",
      "Seen so far: 52100 molecules\n",
      "Training time for 20 batches: 0.8009748458862305 s\n",
      "Training loss (for one batch) at step 540: 0.002476782537996769\n",
      "Training MAE (for one batch) at step 540: 0.006736484821885824\n",
      "Seen so far: 54100 molecules\n",
      "Training time for 20 batches: 0.8058731555938721 s\n",
      "Training loss (for one batch) at step 560: 0.001435514073818922\n",
      "Training MAE (for one batch) at step 560: 0.006822287570685148\n",
      "Seen so far: 56100 molecules\n",
      "Training time for 20 batches: 0.7987680435180664 s\n",
      "Training loss (for one batch) at step 580: 0.004200486466288567\n",
      "Training MAE (for one batch) at step 580: 0.0099449148401618\n",
      "Seen so far: 58100 molecules\n",
      "Training time for 20 batches: 0.8167541027069092 s\n",
      "Training loss (for one batch) at step 600: 0.0020815839525312185\n",
      "Training MAE (for one batch) at step 600: 0.008152472786605358\n",
      "Seen so far: 60100 molecules\n",
      "Training time for 20 batches: 0.7993919849395752 s\n",
      "Training loss (for one batch) at step 620: 0.0031405899208039045\n",
      "Training MAE (for one batch) at step 620: 0.007028442807495594\n",
      "Seen so far: 62100 molecules\n",
      "Training time for 20 batches: 0.8018887042999268 s\n",
      "Training loss (for one batch) at step 640: 0.004481238313019276\n",
      "Training MAE (for one batch) at step 640: 0.011830580420792103\n",
      "Seen so far: 64100 molecules\n",
      "Training time for 20 batches: 0.802659273147583 s\n",
      "Training loss (for one batch) at step 660: 0.004876718390733004\n",
      "Training MAE (for one batch) at step 660: 0.013272500596940517\n",
      "Seen so far: 66100 molecules\n",
      "Training time for 20 batches: 0.8034389019012451 s\n",
      "Training loss (for one batch) at step 680: 0.0015866506146267056\n",
      "Training MAE (for one batch) at step 680: 0.004931094124913216\n",
      "Seen so far: 68100 molecules\n",
      "Training time for 20 batches: 0.8021252155303955 s\n",
      "Training loss (for one batch) at step 700: 0.0042538996785879135\n",
      "Training MAE (for one batch) at step 700: 0.007754628546535969\n",
      "Seen so far: 70100 molecules\n",
      "Training time for 20 batches: 0.8016958236694336 s\n",
      "Training loss (for one batch) at step 720: 0.0023767682723701\n",
      "Training MAE (for one batch) at step 720: 0.008107841946184635\n",
      "Seen so far: 72100 molecules\n",
      "Training time for 20 batches: 0.8012361526489258 s\n",
      "Training loss (for one batch) at step 740: 0.0013826170470565557\n",
      "Training MAE (for one batch) at step 740: 0.006145286839455366\n",
      "Seen so far: 74100 molecules\n",
      "Training time for 20 batches: 0.8088498115539551 s\n",
      "Training loss (for one batch) at step 760: 0.004374089650809765\n",
      "Training MAE (for one batch) at step 760: 0.009130774065852165\n",
      "Seen so far: 76100 molecules\n",
      "Training time for 20 batches: 0.8031392097473145 s\n",
      "Training loss (for one batch) at step 780: 0.0016421196050941944\n",
      "Training MAE (for one batch) at step 780: 0.006310116034001112\n",
      "Seen so far: 78100 molecules\n",
      "Training time for 20 batches: 0.801846981048584 s\n",
      "Training loss (for one batch) at step 800: 0.0015857735415920615\n",
      "Training MAE (for one batch) at step 800: 0.007759127765893936\n",
      "Seen so far: 80100 molecules\n",
      "Training time for 20 batches: 0.8215906620025635 s\n",
      "Training loss (for one batch) at step 820: 0.008250207640230656\n",
      "Training MAE (for one batch) at step 820: 0.01618230529129505\n",
      "Seen so far: 82100 molecules\n",
      "Training time for 20 batches: 0.8037440776824951 s\n",
      "Training loss (for one batch) at step 840: 0.0012881653383374214\n",
      "Training MAE (for one batch) at step 840: 0.010728614404797554\n",
      "Seen so far: 84100 molecules\n",
      "Training time for 20 batches: 0.8012871742248535 s\n",
      "Training loss (for one batch) at step 860: 0.001098397420719266\n",
      "Training MAE (for one batch) at step 860: 0.010161850601434708\n",
      "Seen so far: 86100 molecules\n",
      "Training time for 20 batches: 0.8044958114624023 s\n",
      "Training loss (for one batch) at step 880: 0.0007406841614283621\n",
      "Training MAE (for one batch) at step 880: 0.007938328199088573\n",
      "Seen so far: 88100 molecules\n",
      "Training time for 20 batches: 0.8008823394775391 s\n",
      "Training loss (for one batch) at step 900: 0.0006779395625926554\n",
      "Training MAE (for one batch) at step 900: 0.006076897494494915\n",
      "Seen so far: 90100 molecules\n",
      "Training time for 20 batches: 0.8018467426300049 s\n",
      "Training loss (for one batch) at step 920: 0.0011348600964993238\n",
      "Training MAE (for one batch) at step 920: 0.008207128383219242\n",
      "Seen so far: 92100 molecules\n",
      "Training time for 20 batches: 0.8023941516876221 s\n",
      "Training loss (for one batch) at step 940: 0.0013978051720187068\n",
      "Training MAE (for one batch) at step 940: 0.009744624607264996\n",
      "Seen so far: 94100 molecules\n",
      "Training time for 20 batches: 0.8641567230224609 s\n",
      "Training loss (for one batch) at step 960: 0.0009352832566946745\n",
      "Training MAE (for one batch) at step 960: 0.008235128596425056\n",
      "Seen so far: 96100 molecules\n",
      "Training time for 20 batches: 0.8068380355834961 s\n",
      "Training loss (for one batch) at step 980: 0.0016496328171342611\n",
      "Training MAE (for one batch) at step 980: 0.02283453941345215\n",
      "Seen so far: 98100 molecules\n",
      "Training time for 20 batches: 0.8836421966552734 s\n",
      "Training loss (for one batch) at step 1000: 0.004294574726372957\n",
      "Training MAE (for one batch) at step 1000: 0.009794776327908039\n",
      "Seen so far: 100100 molecules\n",
      "Training time for 20 batches: 0.8022658824920654 s\n",
      "Training loss (for one batch) at step 1020: 0.000917595811188221\n",
      "Training MAE (for one batch) at step 1020: 0.014267348684370518\n",
      "Seen so far: 102100 molecules\n",
      "Training time for 20 batches: 0.8143150806427002 s\n",
      "Training loss (for one batch) at step 1040: 0.0032106745056807995\n",
      "Training MAE (for one batch) at step 1040: 0.005422359798103571\n",
      "Seen so far: 104100 molecules\n",
      "Training time for 20 batches: 0.7994568347930908 s\n",
      "Training loss (for one batch) at step 1060: 0.002364705316722393\n",
      "Training MAE (for one batch) at step 1060: 0.009639827534556389\n",
      "Seen so far: 106100 molecules\n",
      "Training time for 20 batches: 0.8042900562286377 s\n",
      "Training loss (for one batch) at step 1080: 0.0023351963609457016\n",
      "Training MAE (for one batch) at step 1080: 0.006685405038297176\n",
      "Seen so far: 108100 molecules\n",
      "Training time for 20 batches: 0.8115670680999756 s\n",
      "Training loss (for one batch) at step 1100: 0.004181996453553438\n",
      "Training MAE (for one batch) at step 1100: 0.008665764704346657\n",
      "Seen so far: 110100 molecules\n",
      "Training time for 20 batches: 0.8120532035827637 s\n",
      "Training loss (for one batch) at step 1120: 0.0029718345031142235\n",
      "Training MAE (for one batch) at step 1120: 0.009077213704586029\n",
      "Seen so far: 112100 molecules\n",
      "Training time for 20 batches: 0.8076438903808594 s\n",
      "Training loss (for one batch) at step 1140: 0.0016157770296558738\n",
      "Training MAE (for one batch) at step 1140: 0.008946064859628677\n",
      "Seen so far: 114100 molecules\n",
      "Training time for 20 batches: 0.807243824005127 s\n",
      "Training loss (for one batch) at step 1160: 0.0017957895761355758\n",
      "Training MAE (for one batch) at step 1160: 0.005424734205007553\n",
      "Seen so far: 116100 molecules\n",
      "Training time for 20 batches: 0.8078889846801758 s\n",
      "Training loss (for one batch) at step 1180: 0.0009696842753328383\n",
      "Training MAE (for one batch) at step 1180: 0.008214727975428104\n",
      "Seen so far: 118100 molecules\n",
      "Training time for 20 batches: 0.8122329711914062 s\n",
      "Training loss (for one batch) at step 1200: 0.0007020688499324024\n",
      "Training MAE (for one batch) at step 1200: 0.007836742326617241\n",
      "Seen so far: 120100 molecules\n",
      "Training time for 20 batches: 0.8111331462860107 s\n",
      "Training loss (for one batch) at step 1220: 0.0023753964342176914\n",
      "Training MAE (for one batch) at step 1220: 0.01107060257345438\n",
      "Seen so far: 122100 molecules\n",
      "Training time for 20 batches: 0.8090009689331055 s\n",
      "Training loss (for one batch) at step 1240: 0.002682774793356657\n",
      "Training MAE (for one batch) at step 1240: 0.006834436673671007\n",
      "Seen so far: 124100 molecules\n",
      "Training time for 20 batches: 0.8204309940338135 s\n",
      "Training loss (for one batch) at step 1260: 0.00175379344727844\n",
      "Training MAE (for one batch) at step 1260: 0.008260810747742653\n",
      "Seen so far: 126100 molecules\n",
      "Training time for 20 batches: 0.809704065322876 s\n",
      "Training loss (for one batch) at step 1280: 0.00486494367942214\n",
      "Training MAE (for one batch) at step 1280: 0.009626704268157482\n",
      "Seen so far: 128100 molecules\n",
      "Training time for 20 batches: 0.8065218925476074 s\n",
      "Training loss (for one batch) at step 1300: 0.0031692497432231903\n",
      "Training MAE (for one batch) at step 1300: 0.008714204654097557\n",
      "Seen so far: 130100 molecules\n",
      "Training time for 20 batches: 0.8061082363128662 s\n",
      "Training loss (for one batch) at step 1320: 0.0022243792191147804\n",
      "Training MAE (for one batch) at step 1320: 0.008319316431879997\n",
      "Seen so far: 132100 molecules\n",
      "Training time for 20 batches: 0.8068177700042725 s\n",
      "Training loss (for one batch) at step 1340: 0.004946961533278227\n",
      "Training MAE (for one batch) at step 1340: 0.008532527834177017\n",
      "Seen so far: 134100 molecules\n",
      "Training time for 20 batches: 0.8166630268096924 s\n",
      "Training loss (for one batch) at step 1360: 0.0018627515528351068\n",
      "Training MAE (for one batch) at step 1360: 0.01129046268761158\n",
      "Seen so far: 136100 molecules\n",
      "Training time for 20 batches: 0.8116872310638428 s\n",
      "Training loss (for one batch) at step 1380: 0.003657013177871704\n",
      "Training MAE (for one batch) at step 1380: 0.010310960933566093\n",
      "Seen so far: 138100 molecules\n",
      "Training time for 20 batches: 0.8086137771606445 s\n",
      "Training loss (for one batch) at step 1400: 0.003298790892586112\n",
      "Training MAE (for one batch) at step 1400: 0.008600601926445961\n",
      "Seen so far: 140100 molecules\n",
      "Training time for 20 batches: 0.8081519603729248 s\n",
      "Training loss (for one batch) at step 1420: 0.0017238769214600325\n",
      "Training MAE (for one batch) at step 1420: 0.01563616283237934\n",
      "Seen so far: 142100 molecules\n",
      "Training time for 20 batches: 0.8199830055236816 s\n",
      "Training loss (for one batch) at step 1440: 0.003985087852925062\n",
      "Training MAE (for one batch) at step 1440: 0.008099816739559174\n",
      "Seen so far: 144100 molecules\n",
      "Training time for 20 batches: 0.8063020706176758 s\n",
      "Training loss (for one batch) at step 1460: 0.0017864606343209743\n",
      "Training MAE (for one batch) at step 1460: 0.008955810219049454\n",
      "Seen so far: 146100 molecules\n",
      "Training time for 20 batches: 0.8321590423583984 s\n",
      "Training loss (for one batch) at step 1480: 0.0020829569548368454\n",
      "Training MAE (for one batch) at step 1480: 0.008173184469342232\n",
      "Seen so far: 148100 molecules\n",
      "Training time for 20 batches: 0.8123841285705566 s\n",
      "Training loss (for one batch) at step 1500: 0.005066927056759596\n",
      "Training MAE (for one batch) at step 1500: 0.008167828433215618\n",
      "Seen so far: 150100 molecules\n",
      "Training time for 20 batches: 0.8111937046051025 s\n",
      "Training loss (for one batch) at step 1520: 0.0030056843534111977\n",
      "Training MAE (for one batch) at step 1520: 0.014264819212257862\n",
      "Seen so far: 152100 molecules\n",
      "Training time for 20 batches: 0.8096401691436768 s\n",
      "Training loss (for one batch) at step 1540: 0.0012803850695490837\n",
      "Training MAE (for one batch) at step 1540: 0.007462444715201855\n",
      "Seen so far: 154100 molecules\n",
      "Training time for 20 batches: 0.8120768070220947 s\n",
      "Training loss (for one batch) at step 1560: 0.002923802239820361\n",
      "Training MAE (for one batch) at step 1560: 0.005541412625461817\n",
      "Seen so far: 156100 molecules\n",
      "Training time for 20 batches: 0.8129842281341553 s\n",
      "Training loss (for one batch) at step 1580: 0.0025930292904376984\n",
      "Training MAE (for one batch) at step 1580: 0.007513349410146475\n",
      "Seen so far: 158100 molecules\n",
      "Training time for 20 batches: 0.8134489059448242 s\n",
      "Training time for epoch 4: 64.77269673347473 s\n",
      "Starting validation for epoch 4\n",
      "Initial validation loss (for one batch): 0.2797921597957611\n",
      "Initial validation MAE (for one batch): 0.755131721496582\n",
      "Seen so far: 100 molecules\n",
      "Time taken for epoch 4: 65.08883666992188 s\n",
      "\n",
      "Start of epoch 4\n",
      "Initial loss (for one batch): 0.0019610736053436995\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 0.003582035657018423\n",
      "Training MAE (for one batch) at step 20: 0.005578328389674425\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 0.9354071617126465 s\n",
      "Training loss (for one batch) at step 40: 0.0021772198379039764\n",
      "Training MAE (for one batch) at step 40: 0.009502286091446877\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 0.8210439682006836 s\n",
      "Training loss (for one batch) at step 60: 0.001302211545407772\n",
      "Training MAE (for one batch) at step 60: 0.009499051608145237\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 0.8224799633026123 s\n",
      "Training loss (for one batch) at step 80: 0.003178789746016264\n",
      "Training MAE (for one batch) at step 80: 0.009235908277332783\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 0.8812339305877686 s\n",
      "Training loss (for one batch) at step 100: 0.0033406708389520645\n",
      "Training MAE (for one batch) at step 100: 0.009704026393592358\n",
      "Seen so far: 10100 molecules\n",
      "Training time for 20 batches: 0.8212969303131104 s\n",
      "Training loss (for one batch) at step 120: 0.0006441519362851977\n",
      "Training MAE (for one batch) at step 120: 0.007499723229557276\n",
      "Seen so far: 12100 molecules\n",
      "Training time for 20 batches: 0.8113341331481934 s\n",
      "Training loss (for one batch) at step 140: 0.006271554622799158\n",
      "Training MAE (for one batch) at step 140: 0.009844698011875153\n",
      "Seen so far: 14100 molecules\n",
      "Training time for 20 batches: 0.8077490329742432 s\n",
      "Training loss (for one batch) at step 160: 0.0018089383374899626\n",
      "Training MAE (for one batch) at step 160: 0.00930283684283495\n",
      "Seen so far: 16100 molecules\n",
      "Training time for 20 batches: 0.8080360889434814 s\n",
      "Training loss (for one batch) at step 180: 0.000973779009655118\n",
      "Training MAE (for one batch) at step 180: 0.011927218176424503\n",
      "Seen so far: 18100 molecules\n",
      "Training time for 20 batches: 0.8097410202026367 s\n",
      "Training loss (for one batch) at step 200: 0.0006877657142467797\n",
      "Training MAE (for one batch) at step 200: 0.006317427847534418\n",
      "Seen so far: 20100 molecules\n",
      "Training time for 20 batches: 0.810330867767334 s\n",
      "Training loss (for one batch) at step 220: 0.0014561160933226347\n",
      "Training MAE (for one batch) at step 220: 0.007205659057945013\n",
      "Seen so far: 22100 molecules\n",
      "Training time for 20 batches: 0.8119468688964844 s\n",
      "Training loss (for one batch) at step 240: 0.0016350464429706335\n",
      "Training MAE (for one batch) at step 240: 0.007103814743459225\n",
      "Seen so far: 24100 molecules\n",
      "Training time for 20 batches: 0.810211181640625 s\n",
      "Training loss (for one batch) at step 260: 0.000928218534681946\n",
      "Training MAE (for one batch) at step 260: 0.014025954529643059\n",
      "Seen so far: 26100 molecules\n",
      "Training time for 20 batches: 0.8119809627532959 s\n",
      "Training loss (for one batch) at step 280: 0.006938270293176174\n",
      "Training MAE (for one batch) at step 280: 0.020549405366182327\n",
      "Seen so far: 28100 molecules\n",
      "Training time for 20 batches: 0.8327929973602295 s\n",
      "Training loss (for one batch) at step 300: 0.009118524380028248\n",
      "Training MAE (for one batch) at step 300: 0.01390568632632494\n",
      "Seen so far: 30100 molecules\n",
      "Training time for 20 batches: 0.8106977939605713 s\n",
      "Training loss (for one batch) at step 320: 0.0028645554557442665\n",
      "Training MAE (for one batch) at step 320: 0.006691111717373133\n",
      "Seen so far: 32100 molecules\n",
      "Training time for 20 batches: 0.8096840381622314 s\n",
      "Training loss (for one batch) at step 340: 0.0017961045959964395\n",
      "Training MAE (for one batch) at step 340: 0.007452887482941151\n",
      "Seen so far: 34100 molecules\n",
      "Training time for 20 batches: 0.8092362880706787 s\n",
      "Training loss (for one batch) at step 360: 0.004472340457141399\n",
      "Training MAE (for one batch) at step 360: 0.008641377091407776\n",
      "Seen so far: 36100 molecules\n",
      "Training time for 20 batches: 0.8085358142852783 s\n",
      "Training loss (for one batch) at step 380: 0.0017818076303228736\n",
      "Training MAE (for one batch) at step 380: 0.008126402273774147\n",
      "Seen so far: 38100 molecules\n",
      "Training time for 20 batches: 0.8109660148620605 s\n",
      "Training loss (for one batch) at step 400: 0.0018206665990874171\n",
      "Training MAE (for one batch) at step 400: 0.0067879208363592625\n",
      "Seen so far: 40100 molecules\n",
      "Training time for 20 batches: 0.8716120719909668 s\n",
      "Training loss (for one batch) at step 420: 0.004010858945548534\n",
      "Training MAE (for one batch) at step 420: 0.00731243658810854\n",
      "Seen so far: 42100 molecules\n",
      "Training time for 20 batches: 0.8083679676055908 s\n",
      "Training loss (for one batch) at step 440: 0.0028421252500265837\n",
      "Training MAE (for one batch) at step 440: 0.009245038032531738\n",
      "Seen so far: 44100 molecules\n",
      "Training time for 20 batches: 0.8102891445159912 s\n",
      "Training loss (for one batch) at step 460: 0.0030783354304730892\n",
      "Training MAE (for one batch) at step 460: 0.021768206730484962\n",
      "Seen so far: 46100 molecules\n",
      "Training time for 20 batches: 0.808823823928833 s\n",
      "Training loss (for one batch) at step 480: 0.0013287862529978156\n",
      "Training MAE (for one batch) at step 480: 0.011555192992091179\n",
      "Seen so far: 48100 molecules\n",
      "Training time for 20 batches: 0.8085880279541016 s\n",
      "Training loss (for one batch) at step 500: 0.0035569092724472284\n",
      "Training MAE (for one batch) at step 500: 0.009564656764268875\n",
      "Seen so far: 50100 molecules\n",
      "Training time for 20 batches: 0.8276238441467285 s\n",
      "Training loss (for one batch) at step 520: 0.0007633649511262774\n",
      "Training MAE (for one batch) at step 520: 0.009579738602042198\n",
      "Seen so far: 52100 molecules\n",
      "Training time for 20 batches: 0.8120741844177246 s\n",
      "Training loss (for one batch) at step 540: 0.0010067970724776387\n",
      "Training MAE (for one batch) at step 540: 0.013185525313019753\n",
      "Seen so far: 54100 molecules\n",
      "Training time for 20 batches: 0.8094220161437988 s\n",
      "Training loss (for one batch) at step 560: 0.0011246335925534368\n",
      "Training MAE (for one batch) at step 560: 0.00885196402668953\n",
      "Seen so far: 56100 molecules\n",
      "Training time for 20 batches: 0.8096680641174316 s\n",
      "Training loss (for one batch) at step 580: 0.000766328361351043\n",
      "Training MAE (for one batch) at step 580: 0.006238475441932678\n",
      "Seen so far: 58100 molecules\n",
      "Training time for 20 batches: 0.8126657009124756 s\n",
      "Training loss (for one batch) at step 600: 0.00153699005022645\n",
      "Training MAE (for one batch) at step 600: 0.009908833540976048\n",
      "Seen so far: 60100 molecules\n",
      "Training time for 20 batches: 0.8106989860534668 s\n",
      "Training loss (for one batch) at step 620: 0.0006644829409196973\n",
      "Training MAE (for one batch) at step 620: 0.012081223540008068\n",
      "Seen so far: 62100 molecules\n",
      "Training time for 20 batches: 0.8219504356384277 s\n",
      "Training loss (for one batch) at step 640: 0.0020347393583506346\n",
      "Training MAE (for one batch) at step 640: 0.017877014353871346\n",
      "Seen so far: 64100 molecules\n",
      "Training time for 20 batches: 0.8094158172607422 s\n",
      "Training loss (for one batch) at step 660: 0.0020201359875500202\n",
      "Training MAE (for one batch) at step 660: 0.015242478810250759\n",
      "Seen so far: 66100 molecules\n",
      "Training time for 20 batches: 0.8083369731903076 s\n",
      "Training loss (for one batch) at step 680: 0.0016204632120206952\n",
      "Training MAE (for one batch) at step 680: 0.008532685227692127\n",
      "Seen so far: 68100 molecules\n",
      "Training time for 20 batches: 0.8850159645080566 s\n",
      "Training loss (for one batch) at step 700: 0.003994615748524666\n",
      "Training MAE (for one batch) at step 700: 0.008995388634502888\n",
      "Seen so far: 70100 molecules\n",
      "Training time for 20 batches: 0.8091871738433838 s\n",
      "Training loss (for one batch) at step 720: 0.00177419139072299\n",
      "Training MAE (for one batch) at step 720: 0.00973567459732294\n",
      "Seen so far: 72100 molecules\n",
      "Training time for 20 batches: 0.8117356300354004 s\n",
      "Training loss (for one batch) at step 740: 0.003844161983579397\n",
      "Training MAE (for one batch) at step 740: 0.0067419856786727905\n",
      "Seen so far: 74100 molecules\n",
      "Training time for 20 batches: 0.8123881816864014 s\n",
      "Training loss (for one batch) at step 760: 0.001732547301799059\n",
      "Training MAE (for one batch) at step 760: 0.012673960067331791\n",
      "Seen so far: 76100 molecules\n",
      "Training time for 20 batches: 0.8103091716766357 s\n",
      "Training loss (for one batch) at step 780: 0.0018938666908070445\n",
      "Training MAE (for one batch) at step 780: 0.008108539506793022\n",
      "Seen so far: 78100 molecules\n",
      "Training time for 20 batches: 0.8213777542114258 s\n",
      "Training loss (for one batch) at step 800: 0.003770958399400115\n",
      "Training MAE (for one batch) at step 800: 0.008507409133017063\n",
      "Seen so far: 80100 molecules\n",
      "Training time for 20 batches: 0.8049042224884033 s\n",
      "Training loss (for one batch) at step 820: 0.003268228145316243\n",
      "Training MAE (for one batch) at step 820: 0.021845048293471336\n",
      "Seen so far: 82100 molecules\n",
      "Training time for 20 batches: 0.8043467998504639 s\n",
      "Training loss (for one batch) at step 840: 0.0014754205476492643\n",
      "Training MAE (for one batch) at step 840: 0.0076963454484939575\n",
      "Seen so far: 84100 molecules\n",
      "Training time for 20 batches: 0.8118898868560791 s\n",
      "Training loss (for one batch) at step 860: 0.0050793797709047794\n",
      "Training MAE (for one batch) at step 860: 0.010789931751787663\n",
      "Seen so far: 86100 molecules\n",
      "Training time for 20 batches: 0.8111920356750488 s\n",
      "Training loss (for one batch) at step 880: 0.002711289096623659\n",
      "Training MAE (for one batch) at step 880: 0.009676862508058548\n",
      "Seen so far: 88100 molecules\n",
      "Training time for 20 batches: 0.8085551261901855 s\n",
      "Training loss (for one batch) at step 900: 0.0018244392704218626\n",
      "Training MAE (for one batch) at step 900: 0.009715927764773369\n",
      "Seen so far: 90100 molecules\n",
      "Training time for 20 batches: 0.8086631298065186 s\n",
      "Training loss (for one batch) at step 920: 0.005504796281456947\n",
      "Training MAE (for one batch) at step 920: 0.009382513351738453\n",
      "Seen so far: 92100 molecules\n",
      "Training time for 20 batches: 0.8099687099456787 s\n",
      "Training loss (for one batch) at step 940: 0.0033115711994469166\n",
      "Training MAE (for one batch) at step 940: 0.01005146186798811\n",
      "Seen so far: 94100 molecules\n",
      "Training time for 20 batches: 0.8125591278076172 s\n",
      "Training loss (for one batch) at step 960: 0.0029138720128685236\n",
      "Training MAE (for one batch) at step 960: 0.013882756233215332\n",
      "Seen so far: 96100 molecules\n",
      "Training time for 20 batches: 0.8162651062011719 s\n",
      "Training loss (for one batch) at step 980: 0.007734532468020916\n",
      "Training MAE (for one batch) at step 980: 0.009678343310952187\n",
      "Seen so far: 98100 molecules\n",
      "Training time for 20 batches: 0.8125491142272949 s\n",
      "Training loss (for one batch) at step 1000: 0.0009496432612650096\n",
      "Training MAE (for one batch) at step 1000: 0.010505296289920807\n",
      "Seen so far: 100100 molecules\n",
      "Training time for 20 batches: 0.8326377868652344 s\n",
      "Training loss (for one batch) at step 1020: 0.002083498053252697\n",
      "Training MAE (for one batch) at step 1020: 0.008173464797437191\n",
      "Seen so far: 102100 molecules\n",
      "Training time for 20 batches: 0.8092150688171387 s\n",
      "Training loss (for one batch) at step 1040: 0.0033167980145663023\n",
      "Training MAE (for one batch) at step 1040: 0.005313768982887268\n",
      "Seen so far: 104100 molecules\n",
      "Training time for 20 batches: 0.8145129680633545 s\n",
      "Training loss (for one batch) at step 1060: 0.0026337625458836555\n",
      "Training MAE (for one batch) at step 1060: 0.012367061339318752\n",
      "Seen so far: 106100 molecules\n",
      "Training time for 20 batches: 0.8097450733184814 s\n",
      "Training loss (for one batch) at step 1080: 0.0019611669704318047\n",
      "Training MAE (for one batch) at step 1080: 0.008013228885829449\n",
      "Seen so far: 108100 molecules\n",
      "Training time for 20 batches: 0.8087038993835449 s\n",
      "Training loss (for one batch) at step 1100: 0.004096476826816797\n",
      "Training MAE (for one batch) at step 1100: 0.006579785142093897\n",
      "Seen so far: 110100 molecules\n",
      "Training time for 20 batches: 0.8098280429840088 s\n",
      "Training loss (for one batch) at step 1120: 0.0014199004508554935\n",
      "Training MAE (for one batch) at step 1120: 0.0068668038584291935\n",
      "Seen so far: 112100 molecules\n",
      "Training time for 20 batches: 0.8168590068817139 s\n",
      "Training loss (for one batch) at step 1140: 0.0021042407024651766\n",
      "Training MAE (for one batch) at step 1140: 0.008731772191822529\n",
      "Seen so far: 114100 molecules\n",
      "Training time for 20 batches: 0.8063669204711914 s\n",
      "Training loss (for one batch) at step 1160: 0.006609045434743166\n",
      "Training MAE (for one batch) at step 1160: 0.006313328165560961\n",
      "Seen so far: 116100 molecules\n",
      "Training time for 20 batches: 0.8104681968688965 s\n",
      "Training loss (for one batch) at step 1180: 0.0036695310845971107\n",
      "Training MAE (for one batch) at step 1180: 0.012951278127729893\n",
      "Seen so far: 118100 molecules\n",
      "Training time for 20 batches: 0.8084497451782227 s\n",
      "Training loss (for one batch) at step 1200: 0.001874108798801899\n",
      "Training MAE (for one batch) at step 1200: 0.008568947203457355\n",
      "Seen so far: 120100 molecules\n",
      "Training time for 20 batches: 0.8068289756774902 s\n",
      "Training loss (for one batch) at step 1220: 0.004498885944485664\n",
      "Training MAE (for one batch) at step 1220: 0.007268206216394901\n",
      "Seen so far: 122100 molecules\n",
      "Training time for 20 batches: 0.8902223110198975 s\n",
      "Training loss (for one batch) at step 1240: 0.0016189771704375744\n",
      "Training MAE (for one batch) at step 1240: 0.006672955118119717\n",
      "Seen so far: 124100 molecules\n",
      "Training time for 20 batches: 0.8170700073242188 s\n",
      "Training loss (for one batch) at step 1260: 0.0017906773136928678\n",
      "Training MAE (for one batch) at step 1260: 0.008073105476796627\n",
      "Seen so far: 126100 molecules\n",
      "Training time for 20 batches: 0.8105590343475342 s\n",
      "Training loss (for one batch) at step 1280: 0.0035988464951515198\n",
      "Training MAE (for one batch) at step 1280: 0.008281324990093708\n",
      "Seen so far: 128100 molecules\n",
      "Training time for 20 batches: 0.8092169761657715 s\n",
      "Training loss (for one batch) at step 1300: 0.0027102588210254908\n",
      "Training MAE (for one batch) at step 1300: 0.008994748815894127\n",
      "Seen so far: 130100 molecules\n",
      "Training time for 20 batches: 0.8206846714019775 s\n",
      "Training loss (for one batch) at step 1320: 0.0031185310799628496\n",
      "Training MAE (for one batch) at step 1320: 0.008802833035588264\n",
      "Seen so far: 132100 molecules\n",
      "Training time for 20 batches: 0.8141942024230957 s\n",
      "Training loss (for one batch) at step 1340: 0.005200832150876522\n",
      "Training MAE (for one batch) at step 1340: 0.007568665314465761\n",
      "Seen so far: 134100 molecules\n",
      "Training time for 20 batches: 0.8156909942626953 s\n",
      "Training loss (for one batch) at step 1360: 0.0024373638443648815\n",
      "Training MAE (for one batch) at step 1360: 0.014570066705346107\n",
      "Seen so far: 136100 molecules\n",
      "Training time for 20 batches: 0.806114912033081 s\n",
      "Training loss (for one batch) at step 1380: 0.0009239453356713057\n",
      "Training MAE (for one batch) at step 1380: 0.01009922195225954\n",
      "Seen so far: 138100 molecules\n",
      "Training time for 20 batches: 0.80918288230896 s\n",
      "Training loss (for one batch) at step 1400: 0.0028279758989810944\n",
      "Training MAE (for one batch) at step 1400: 0.009846504777669907\n",
      "Seen so far: 140100 molecules\n",
      "Training time for 20 batches: 0.8089892864227295 s\n",
      "Training loss (for one batch) at step 1420: 0.002253913786262274\n",
      "Training MAE (for one batch) at step 1420: 0.007384300697594881\n",
      "Seen so far: 142100 molecules\n",
      "Training time for 20 batches: 0.8093440532684326 s\n",
      "Training loss (for one batch) at step 1440: 0.0015238189371302724\n",
      "Training MAE (for one batch) at step 1440: 0.008979910053312778\n",
      "Seen so far: 144100 molecules\n",
      "Training time for 20 batches: 0.8308589458465576 s\n",
      "Training loss (for one batch) at step 1460: 0.0025668905582278967\n",
      "Training MAE (for one batch) at step 1460: 0.006570341531187296\n",
      "Seen so far: 146100 molecules\n",
      "Training time for 20 batches: 0.8699448108673096 s\n",
      "Training loss (for one batch) at step 1480: 0.0018269126303493977\n",
      "Training MAE (for one batch) at step 1480: 0.006829191464930773\n",
      "Seen so far: 148100 molecules\n",
      "Training time for 20 batches: 0.810819149017334 s\n",
      "Training loss (for one batch) at step 1500: 0.0021361522376537323\n",
      "Training MAE (for one batch) at step 1500: 0.009513135999441147\n",
      "Seen so far: 150100 molecules\n",
      "Training time for 20 batches: 0.8068311214447021 s\n",
      "Training loss (for one batch) at step 1520: 0.005530238151550293\n",
      "Training MAE (for one batch) at step 1520: 0.007280681282281876\n",
      "Seen so far: 152100 molecules\n",
      "Training time for 20 batches: 0.8119807243347168 s\n",
      "Training loss (for one batch) at step 1540: 0.0023711943067610264\n",
      "Training MAE (for one batch) at step 1540: 0.007222216576337814\n",
      "Seen so far: 154100 molecules\n",
      "Training time for 20 batches: 0.8092961311340332 s\n",
      "Training loss (for one batch) at step 1560: 0.001118946704082191\n",
      "Training MAE (for one batch) at step 1560: 0.015054243616759777\n",
      "Seen so far: 156100 molecules\n",
      "Training time for 20 batches: 0.813424825668335 s\n",
      "Training loss (for one batch) at step 1580: 0.004235813394188881\n",
      "Training MAE (for one batch) at step 1580: 0.005884767044335604\n",
      "Seen so far: 158100 molecules\n",
      "Training time for 20 batches: 0.8111281394958496 s\n",
      "Training time for epoch 5: 65.33413195610046 s\n",
      "Starting validation for epoch 5\n",
      "Initial validation loss (for one batch): 0.28434035181999207\n",
      "Initial validation MAE (for one batch): 1.1056517362594604\n",
      "Seen so far: 100 molecules\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        train_and_evaluate_network(network=network, params=params, train_set=train_set, test_set=test_set, batch_size = batch_size, epochs=500)\n",
    "\n",
    "            \n",
    "        break\n",
    "\n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        print('Raised an error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8, 8, 0.0003 gave mse of 0.34 for LJ data \\\n",
    "8, 4, 0.0003 with patience 3 and train repeats 100 gave mse of 0.20 \\\n",
    "8, 4, 0.0003 with patience 2 gave 0.22 \\\n",
    "8, 4, 0.0003 with patience 3 and train repeats 50 gave 0.31 \\\n",
    "8, 4, 0.0003 with patience 5 and train repeats 50 gave 0.28 \\\n",
    "8, 8, 0.0003 with patience 3 and train repeats 100 gave 0.24 \\\n",
    "8, 4, 0.0003 with patience 3 and train repeats 80 gave 0.23\n",
    "8, 4, 0.0003 with patience 3 and train repeats 500 gave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise potential analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_energy(molecule, network=None, params=None):\n",
    "        '''Takes an ASE Atoms object and outputs PiNet's energy prediction'''\n",
    "        dtype=tf.float32\n",
    "        dtypes = {'coord': dtype, 'elems': tf.int32, 'ind_1': tf.int32}\n",
    "        shapes = {'coord': [None, 3], 'elems': [None], 'ind_1': [None, 1]}\n",
    "\n",
    "        pred_dataset = tf.data.Dataset.from_generator(lambda:_generator(molecule), dtypes, shapes)\n",
    "\n",
    "        for molecule in pred_dataset:\n",
    "                molecule = network.preprocess(molecule)\n",
    "                pred = network(molecule, training=False)\n",
    "                ind = molecule['ind_1']\n",
    "                nbatch = tf.reduce_max(ind)+1\n",
    "                pred = pred/params['e_scale']\n",
    "                if params['e_dress']:\n",
    "                        pred += atomic_dress(molecule, params['e_dress'], dtype=pred.dtype)\n",
    "                energy_prediction = tf.math.unsorted_segment_sum(pred, ind[:, 0], nbatch)\n",
    "                energy_prediction_numpy = energy_prediction.numpy()[0]\n",
    "        return energy_prediction_numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n",
      "Wrong shape! (2, 1)\n"
     ]
    }
   ],
   "source": [
    "atoms1 = Atoms('H2')\n",
    "atoms2 = Atoms('H2', calculator=LennardJones())\n",
    "\n",
    "nr2 = 100\n",
    "rrange2 = np.linspace(1,1.9,nr2)\n",
    "epred = np.zeros(nr2)\n",
    "etrue = np.zeros(nr2)\n",
    "\n",
    "for i in range(nr2):\n",
    "    pos = [[0, 0, 0],\n",
    "           [rrange2[i], 0, 0]]\n",
    "    atoms1.set_positions(pos)\n",
    "    atoms2.set_positions(pos)\n",
    "    epred[i] = predict_energy(atoms1, network=network, params=params)\n",
    "    etrue[i] = atoms2.get_potential_energy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions obtained from the example script using the estimator workflow\n",
    "estimator_epred = np.array([ 0.53924155,  0.53924155,  0.31333005,  0.10093462, -0.09251928,\n",
    "       -0.26339132, -0.41004729, -0.53268594, -0.63286495, -0.71293688,\n",
    "       -0.77557099, -0.82340288, -0.85883415, -0.88394475, -0.90048921,\n",
    "       -0.90992886, -0.91347712, -0.91214544, -0.9067837 , -0.89810729,\n",
    "       -0.88672489, -0.87315392, -0.85783267, -0.84113717, -0.82338399,\n",
    "       -0.80484247, -0.78574157, -0.76627159, -0.74659473, -0.72684616,\n",
    "       -0.70713782, -0.68756211, -0.66819561, -0.64909941, -0.63032281,\n",
    "       -0.61190391, -0.59387207, -0.57624871, -0.55904883, -0.54228193,\n",
    "       -0.52595168, -0.51005912, -0.49460107, -0.47957283, -0.46496773,\n",
    "       -0.4507778 , -0.43699384, -0.42360735, -0.41060773, -0.39798641,\n",
    "       -0.3857336 , -0.37384129, -0.36230105, -0.35110471, -0.34024522,\n",
    "       -0.32971513, -0.31950843, -0.3096177 , -0.300037  , -0.29075974,\n",
    "       -0.28178009, -0.27309084, -0.26468596, -0.25655863, -0.24870205,\n",
    "       -0.24110949, -0.23377317, -0.22668669, -0.21984196, -0.21323186,\n",
    "       -0.20684901, -0.20068565, -0.19473439, -0.18898779, -0.18343845,\n",
    "       -0.17807949, -0.17290318, -0.16790283, -0.16307196, -0.15840358,\n",
    "       -0.1538918 , -0.14953002, -0.1453124 , -0.1412335 , -0.13728777,\n",
    "       -0.13346991, -0.12977493, -0.12619817, -0.12273505, -0.1161325 ,\n",
    "       -0.11298513, -0.11298513, -0.10993543, -0.10993543, -0.10697967,\n",
    "       -0.10411453, -0.10133681, -0.09864366, -0.09603179, -0.09349895])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error when using estimator workflow = 0.16164043922153268\n",
      "Mean squared error when using the updated TF2 workflow = 0.24677919941119852\n"
     ]
    }
   ],
   "source": [
    "mse_tf2 = mean_squared_error(etrue, epred, squared=False)\n",
    "mse_estimator = mean_squared_error(etrue, estimator_epred, squared=False)\n",
    "print(f'Mean squared error when using estimator workflow = {mse_estimator}')\n",
    "print(f'Mean squared error when using the updated TF2 workflow = {mse_tf2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax1 = plt.subplots(figsize = (6,3))\n",
    "ax1.plot(rrange2, epred, '--')\n",
    "ax1.plot(rrange2, etrue)\n",
    "ax1.plot(rrange2, estimator_epred, '--')\n",
    "ax1.legend([f'PiNet TF2 Prediction (MSE:{mse_tf2: .2f})', 'Truth', f'PiNet Estimator Prediction (MSE:{mse_estimator: .2f})'], loc=2, bbox_to_anchor=(0.25,1))\n",
    "plt.title('Prediction of Lennard-Jones potential of H2')\n",
    "ax1.set_xlabel('Interatomic Distance (Å)')\n",
    "ax1.set_ylabel('Energy (eV)')\n",
    "\n",
    "output_directory = '/Users/miguelnavaharris/Project/Plots'\n",
    "output_filename = 'LJ_potential_100batch_1000repeats_8_4_3.png'\n",
    "output_path = os.path.join(output_directory, output_filename)\n",
    "plt.savefig(output_path, format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1676805 , -0.37880796, -0.55995417, -0.70940012, -0.82997358,\n",
       "       -0.92606837, -1.00197017, -1.06128728, -1.10686541, -1.14094698,\n",
       "       -1.16531432, -1.18143547, -1.19053948, -1.19368386, -1.19178522,\n",
       "       -1.1856463 , -1.17596436, -1.16335118, -1.14833665, -1.13138032,\n",
       "       -1.11287689, -1.09316468, -1.07253528, -1.05123377, -1.02947068,\n",
       "       -1.00742102, -0.98523283, -0.96303141, -0.94092137, -0.9189868 ,\n",
       "       -0.89729983, -0.87591875, -0.85489112, -0.83425516, -0.81403977,\n",
       "       -0.79426932, -0.77496165, -0.75613052, -0.73778361, -0.71992779,\n",
       "       -0.70256466, -0.68569446, -0.6693157 , -0.65342361, -0.63801467,\n",
       "       -0.62308121, -0.60861731, -0.59461331, -0.58106101, -0.56795126,\n",
       "       -0.55527419, -0.54301953, -0.53117621, -0.51973486, -0.50868303,\n",
       "       -0.49801177, -0.48770872, -0.47776422, -0.46816608, -0.45890489,\n",
       "       -0.44996932, -0.44134846, -0.43303287, -0.42501166, -0.41727552,\n",
       "       -0.4098137 , -0.40261737, -0.39567596, -0.38898137, -0.38252392,\n",
       "       -0.37629506, -0.37028658, -0.36448926, -0.35889548, -0.35349756,\n",
       "       -0.3482877 , -0.34325826, -0.33840242, -0.33371297, -0.3291834 ,\n",
       "       -0.32480735, -0.32057834, -0.31649053, -0.31253779, -0.30871496,\n",
       "       -0.30501652, -0.30143768, -0.29797262, -0.29461765, -0.29136741,\n",
       "       -0.28821787, -0.28516501, -0.28220457, -0.27933314, -0.27654669,\n",
       "       -0.27384189, -0.27121541, -0.26866424, -0.26618528, -0.26377586])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _get_lj_data():\n",
    "    from ase.calculators.lj import LennardJones\n",
    "\n",
    "    atoms = Atoms('H3', positions=[[0, 0, 0], [0, 1, 0], [1, 1, 0]])\n",
    "    atoms.set_calculator(LennardJones(rc=5.0))\n",
    "    coord, elems, e_data, f_data = [], [], [], []\n",
    "    for x_a in np.linspace(-5, 0, 1000):\n",
    "        atoms.positions[0, 0] = x_a\n",
    "        coord.append(atoms.positions.copy())\n",
    "        elems.append(atoms.numbers)\n",
    "        e_data.append(atoms.get_potential_energy())\n",
    "        f_data.append(atoms.get_forces())\n",
    "\n",
    "    data = {\n",
    "        'coord': np.array(coord),\n",
    "        'elems': np.array(elems),\n",
    "        'e_data': np.array(e_data),\n",
    "        'f_data': np.array(f_data)\n",
    "    }\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_train_network(hparams):\n",
    "\n",
    "    network = get_network(params['network'])\n",
    "    preprocess_traintest_sets(train_set, test_set, network=network)\n",
    "\n",
    "    #Set up tensorboard directory and writer\n",
    "    nodes = hparams[HP_NUM_NODES]\n",
    "    depth = hparams[HP_DEPTH]\n",
    "    lr = hparams[HP_LR]\n",
    "\n",
    "    run_dir = (\n",
    "    \"/Users/miguelnavaharris/Project/hparams_logs/LJ_hparams/\"\n",
    "    + str(nodes)\n",
    "    + \"nodes_\"\n",
    "    + str(depth)\n",
    "    + \"depth_\"\n",
    "    + str(lr)\n",
    "    + \"lr\"\n",
    "    )\n",
    "\n",
    "    run_dir_writer = tf.summary.create_file_writer(run_dir)\n",
    "\n",
    "    # Instantiate an optimizer    \n",
    "    optimizer = get(params['optimizer'])\n",
    "    # Define a loss function\n",
    "    loss_fn = tf.keras.losses.mse\n",
    "    \n",
    "\n",
    "    for epoch in range(1):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time_epoch = time.time()\n",
    "        hund_step_times = []\n",
    "\n",
    "        print(nodes, depth, lr)\n",
    "        print(network.depth)\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, batch in enumerate(train_set):\n",
    "            # print('starting step', step)\n",
    "            train_losses = []\n",
    "            train_MAEs = []\n",
    "            # Open a GradientTape to record the operations run\n",
    "            # during the forward pass, which enables auto-differentiation.\n",
    "            with tf.GradientTape() as loss_tape:\n",
    "                with tf.GradientTape() as innertape:\n",
    "                    innertape.watch(batch)\n",
    "\n",
    "                    # Run the forward pass of the layer.\n",
    "                    # The operations that the layer applies\n",
    "                    # to its inputs are going to be recorded\n",
    "                    # on the GradientTape.\n",
    "\n",
    "                    pred = network(batch, training=True)  # Logits for this minibatch\n",
    "                    # print(\"pred shape: \", pred.shape)\n",
    "\n",
    "                    ind = batch['ind_1']\n",
    "                    nbatch = tf.reduce_max(ind)+1\n",
    "                    pred = tf.math.unsorted_segment_sum(pred, ind[:, 0], nbatch)    \n",
    "                    e_data = batch['e_data']\n",
    "\n",
    "                    if params['e_dress']:\n",
    "                        e_data -= atomic_dress(batch, params['e_dress'], dtype=pred.dtype)\n",
    "                    e_data *= params['e_scale']\n",
    "                    \n",
    "                    # print(\"pred shape after unsorted_segment_sum:\", pred.shape)\n",
    "                    # print(\"e_data shape:\", e_data.shape)\n",
    "                \n",
    "                    train_losses.append(loss_fn(e_data, pred))\n",
    "                    train_MAEs.append(tf.reduce_mean(np.abs(e_data - pred)))\n",
    "\n",
    "\n",
    "                if params['use_force']:\n",
    "\n",
    "                    f_pred = innertape.gradient(pred, batch['coord'])\n",
    "                    if type(f_pred) == tf.IndexedSlices:\n",
    "                        f_pred = tf.scatter_nd(tf.expand_dims(f_pred.indices, 1), f_pred.values,\n",
    "                                            tf.cast(f_pred.dense_shape, tf.int32))\n",
    "\n",
    "                    f_pred = -f_pred\n",
    "\n",
    "                    f_data = batch['f_data']*params['e_scale']\n",
    "                    f_mask = tf.fill(tf.shape(f_pred), True)\n",
    "\n",
    "                    error = f_data - f_pred\n",
    "                    error = tf.boolean_mask(error, f_mask)\n",
    "                    # print(\"f_data shape: \", f_data.shape)\n",
    "                    # print(\"f_pred shape after scatter_nd and negate:\", f_pred.shape)\n",
    "                    train_losses.append(tf.reduce_mean(error**2))\n",
    "                    train_MAEs.append(tf.reduce_mean(error))\n",
    "\n",
    "\n",
    "                # Compute the loss value for this minibatch.\n",
    "                train_loss_value = tf.reduce_sum(train_losses)\n",
    "                train_MAE_value = tf.reduce_sum(train_MAEs)\n",
    "\n",
    "            # Use the gradient tape to automatically retrieve\n",
    "            # the gradients of the trainable variables with respect to the loss.\n",
    "            grads = loss_tape.gradient(train_loss_value, network.trainable_weights)\n",
    "\n",
    "            # Run one step of gradient descent by updating\n",
    "            # the value of the variables to minimize the loss.\n",
    "            optimizer.apply_gradients(zip(grads, network.trainable_weights))\n",
    "\n",
    "\n",
    "\n",
    "            # Log every n batches.\n",
    "            if step == 0:\n",
    "                print(f\"Initial loss (for one batch): {float(train_loss_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "\n",
    "\n",
    "            elif step % 20 == 0:\n",
    "                print(f\"Training loss (for one batch) at step {step}: {float(train_loss_value)}\")\n",
    "                print(f\"Training MAE (for one batch) at step {step}: {float(train_MAE_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "                hund_step_times += [(time.time() - start_time_epoch)]\n",
    "                print(f'Training time for 20 batches: {((hund_step_times[-1] - hund_step_times[-2]) if len(hund_step_times) > 1 else hund_step_times[-1])} s')\n",
    "\n",
    "                # Record TensorBoard metrics\n",
    "                with run_dir_writer.as_default():\n",
    "                    print('Writing to tb')\n",
    "                    tf.summary.scalar('Training loss', train_loss_value, step=step)\n",
    "                    tf.summary.scalar('Training MAE', train_MAE_value, step=step)\n",
    "\n",
    "\n",
    "        print(f'Training time for epoch {epoch + 1}: {(time.time() - start_time_epoch)} s')\n",
    "\n",
    "        \n",
    "\n",
    "        # Run a validation loop at the end of each epoch\n",
    "        print(f'Starting validation for epoch {(epoch + 1)}')\n",
    "\n",
    "        for step, batch in enumerate(test_set):\n",
    "\n",
    "            val_losses = []\n",
    "            val_MAEs = []\n",
    "            \n",
    "            val_pred = network(batch, training=False)\n",
    "            ind = batch['ind_1']\n",
    "            nbatch = tf.reduce_max(ind)+1\n",
    "            val_pred = tf.math.unsorted_segment_sum(val_pred, ind[:, 0], nbatch)\n",
    "            e_data = batch['e_data']\n",
    "\n",
    "            if params['e_dress']:\n",
    "                e_data -= atomic_dress(batch, params['e_dress'], dtype=pred.dtype)\n",
    "                e_data *= params['e_scale']\n",
    "\n",
    "            val_losses.append(loss_fn(e_data, val_pred))\n",
    "            val_MAEs.append(np.abs(e_data - val_pred))\n",
    "\n",
    "\n",
    "            val_loss_value = tf.reduce_sum(val_losses)\n",
    "            val_MAE_value = tf.reduce_sum(val_MAEs)\n",
    "\n",
    "            # Record TensorBoard metrics\n",
    "            with run_dir_writer.as_default():\n",
    "                print('Writing to tb')\n",
    "                tf.summary.scalar('Validation loss', val_loss_value, step=step)\n",
    "                tf.summary.scalar('Validation MAE', val_MAE_value, step=step)\n",
    "            \n",
    "\n",
    "\n",
    "            # Log every n batches.\n",
    "            if step == 0:\n",
    "                print(f\"Initial validation loss (for one batch): {float(val_loss_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "\n",
    "\n",
    "            elif step % 20 == 0:\n",
    "                print(f\"Validation loss (for one batch) at step {step}: {float(val_loss_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "                hund_step_times += [(time.time() - start_time_epoch)]\n",
    "                print(f'Validation time for 20 batches: {((hund_step_times[-1] - hund_step_times[-2]) if len(hund_step_times) > 1 else hund_step_times[-1])} s')\n",
    "\n",
    "        with run_dir_writer.as_default():\n",
    "            hp.hparams(hparams)\n",
    "            tf.summary.scalar(\"Training error\", train_MAE_value, step=1)\n",
    "            tf.summary.scalar(\"Validation error\", val_MAE_value, step=1)\n",
    "            tf.summary.scalar(\"Training loss\", train_loss_value, step=1)\n",
    "            tf.summary.scalar(\"Validation loss\", val_loss_value, step=1)\n",
    "\n",
    "\n",
    "        print(f\"Time taken for epoch {epoch + 1}: {(time.time() - start_time_epoch)} s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 14:09:36.154501: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-29 14:09:36.154598: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-03-29 14:09:36.188040: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-03-29 14:09:36.188198: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "8 4 0.0003\n",
      "4\n",
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "Initial loss (for one batch): 37.69180679321289\n",
      "Seen so far: 100 molecules\n",
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "Training loss (for one batch) at step 20: 28.057342529296875\n",
      "Training MAE (for one batch) at step 20: 0.6172356605529785\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 7.19642186164856 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 40: 26.139341354370117\n",
      "Training MAE (for one batch) at step 40: 0.6227275133132935\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 7.249698162078857 s\n",
      "Writing to tb\n",
      "Wrong shape! (299, 1)\n",
      "Raised an error\n",
      "\n",
      "Start of epoch 0\n",
      "8 4 0.0003\n",
      "4\n",
      "Initial loss (for one batch): 24.42711067199707\n",
      "Seen so far: 100 molecules\n",
      "Wrong shape! (299, 1)\n",
      "Raised an error\n",
      "\n",
      "Start of epoch 0\n",
      "8 4 0.0003\n",
      "4\n",
      "Initial loss (for one batch): 31.165027618408203\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 17.50162124633789\n",
      "Training MAE (for one batch) at step 20: 0.5736317038536072\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 7.7542033195495605 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 40: 20.985883712768555\n",
      "Training MAE (for one batch) at step 40: 0.5861892104148865\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 7.540507793426514 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 60: 14.71211051940918\n",
      "Training MAE (for one batch) at step 60: 0.346265584230423\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 8.120901107788086 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 80: 15.082794189453125\n",
      "Training MAE (for one batch) at step 80: 0.39740484952926636\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 7.739642858505249 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 100: 8.911031723022461\n",
      "Training MAE (for one batch) at step 100: 0.5619739890098572\n",
      "Seen so far: 10100 molecules\n",
      "Training time for 20 batches: 7.9326441287994385 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 120: 5.935659885406494\n",
      "Training MAE (for one batch) at step 120: 0.5422974228858948\n",
      "Seen so far: 12100 molecules\n",
      "Training time for 20 batches: 7.781261205673218 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 140: 4.355548858642578\n",
      "Training MAE (for one batch) at step 140: 0.3494456112384796\n",
      "Seen so far: 14100 molecules\n",
      "Training time for 20 batches: 7.682457685470581 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 160: 2.108837604522705\n",
      "Training MAE (for one batch) at step 160: 0.3381752371788025\n",
      "Seen so far: 16100 molecules\n",
      "Training time for 20 batches: 7.637746095657349 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 180: 1.5549097061157227\n",
      "Training MAE (for one batch) at step 180: 0.30995845794677734\n",
      "Seen so far: 18100 molecules\n",
      "Training time for 20 batches: 7.765434980392456 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 200: 0.9002636671066284\n",
      "Training MAE (for one batch) at step 200: 0.1944093108177185\n",
      "Seen so far: 20100 molecules\n",
      "Training time for 20 batches: 7.510735988616943 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 220: 0.6390456557273865\n",
      "Training MAE (for one batch) at step 220: 0.1821921318769455\n",
      "Seen so far: 22100 molecules\n",
      "Training time for 20 batches: 7.457710027694702 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 240: 0.4051351249217987\n",
      "Training MAE (for one batch) at step 240: 0.1607479602098465\n",
      "Seen so far: 24100 molecules\n",
      "Training time for 20 batches: 7.707867860794067 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 260: 0.31872326135635376\n",
      "Training MAE (for one batch) at step 260: 0.13519732654094696\n",
      "Seen so far: 26100 molecules\n",
      "Training time for 20 batches: 7.6280412673950195 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 280: 0.21137729287147522\n",
      "Training MAE (for one batch) at step 280: 0.13398538529872894\n",
      "Seen so far: 28100 molecules\n",
      "Training time for 20 batches: 7.753150939941406 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 300: 0.14043132960796356\n",
      "Training MAE (for one batch) at step 300: 0.09781656414270401\n",
      "Seen so far: 30100 molecules\n",
      "Training time for 20 batches: 7.907107830047607 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 320: 0.12005119025707245\n",
      "Training MAE (for one batch) at step 320: 0.10372398793697357\n",
      "Seen so far: 32100 molecules\n",
      "Training time for 20 batches: 7.968504190444946 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 340: 0.1513611227273941\n",
      "Training MAE (for one batch) at step 340: 0.09643472731113434\n",
      "Seen so far: 34100 molecules\n",
      "Training time for 20 batches: 7.9495768547058105 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 360: 0.1136859580874443\n",
      "Training MAE (for one batch) at step 360: 0.09016557782888412\n",
      "Seen so far: 36100 molecules\n",
      "Training time for 20 batches: 7.945744037628174 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 380: 0.1077704057097435\n",
      "Training MAE (for one batch) at step 380: 0.09588680416345596\n",
      "Seen so far: 38100 molecules\n",
      "Training time for 20 batches: 8.018211841583252 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 400: 0.0994277223944664\n",
      "Training MAE (for one batch) at step 400: 0.09445467591285706\n",
      "Seen so far: 40100 molecules\n",
      "Training time for 20 batches: 7.840092182159424 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 420: 0.1100936084985733\n",
      "Training MAE (for one batch) at step 420: 0.08223399519920349\n",
      "Seen so far: 42100 molecules\n",
      "Training time for 20 batches: 7.69975209236145 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 440: 0.09416141360998154\n",
      "Training MAE (for one batch) at step 440: 0.09031146764755249\n",
      "Seen so far: 44100 molecules\n",
      "Training time for 20 batches: 7.973939895629883 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 460: 0.08866626769304276\n",
      "Training MAE (for one batch) at step 460: 0.08211620151996613\n",
      "Seen so far: 46100 molecules\n",
      "Training time for 20 batches: 8.054908990859985 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 480: 0.1058063879609108\n",
      "Training MAE (for one batch) at step 480: 0.0750945433974266\n",
      "Seen so far: 48100 molecules\n",
      "Training time for 20 batches: 8.037438154220581 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 500: 0.08087839931249619\n",
      "Training MAE (for one batch) at step 500: 0.08590691536664963\n",
      "Seen so far: 50100 molecules\n",
      "Training time for 20 batches: 7.78076171875 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 520: 0.08906790614128113\n",
      "Training MAE (for one batch) at step 520: 0.07297035306692123\n",
      "Seen so far: 52100 molecules\n",
      "Training time for 20 batches: 7.681106090545654 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 540: 0.09788251668214798\n",
      "Training MAE (for one batch) at step 540: 0.07467585802078247\n",
      "Seen so far: 54100 molecules\n",
      "Training time for 20 batches: 7.89628791809082 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 560: 0.10133785009384155\n",
      "Training MAE (for one batch) at step 560: 0.06767386198043823\n",
      "Seen so far: 56100 molecules\n",
      "Training time for 20 batches: 7.7871479988098145 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 580: 0.11205081641674042\n",
      "Training MAE (for one batch) at step 580: 0.08264712244272232\n",
      "Seen so far: 58100 molecules\n",
      "Training time for 20 batches: 7.9506120681762695 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 600: 0.09545063227415085\n",
      "Training MAE (for one batch) at step 600: 0.07751882821321487\n",
      "Seen so far: 60100 molecules\n",
      "Training time for 20 batches: 7.865744113922119 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 620: 0.07371564209461212\n",
      "Training MAE (for one batch) at step 620: 0.06774138659238815\n",
      "Seen so far: 62100 molecules\n",
      "Training time for 20 batches: 7.877249002456665 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 640: 0.06122314929962158\n",
      "Training MAE (for one batch) at step 640: 0.05802268907427788\n",
      "Seen so far: 64100 molecules\n",
      "Training time for 20 batches: 7.593211889266968 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 660: 0.07413263618946075\n",
      "Training MAE (for one batch) at step 660: 0.06316331773996353\n",
      "Seen so far: 66100 molecules\n",
      "Training time for 20 batches: 7.8257200717926025 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 680: 0.05046699196100235\n",
      "Training MAE (for one batch) at step 680: 0.056571751832962036\n",
      "Seen so far: 68100 molecules\n",
      "Training time for 20 batches: 7.410866975784302 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 700: 0.07878988981246948\n",
      "Training MAE (for one batch) at step 700: 0.06641685962677002\n",
      "Seen so far: 70100 molecules\n",
      "Training time for 20 batches: 7.100839853286743 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 720: 0.045040518045425415\n",
      "Training MAE (for one batch) at step 720: 0.060775116086006165\n",
      "Seen so far: 72100 molecules\n",
      "Training time for 20 batches: 7.148031949996948 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 740: 0.04531759396195412\n",
      "Training MAE (for one batch) at step 740: 0.0663515105843544\n",
      "Seen so far: 74100 molecules\n",
      "Training time for 20 batches: 7.735384225845337 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 760: 0.04663515090942383\n",
      "Training MAE (for one batch) at step 760: 0.05058068782091141\n",
      "Seen so far: 76100 molecules\n",
      "Training time for 20 batches: 7.467986822128296 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 780: 0.07080968469381332\n",
      "Training MAE (for one batch) at step 780: 0.05279497057199478\n",
      "Seen so far: 78100 molecules\n",
      "Training time for 20 batches: 7.335652112960815 s\n",
      "Writing to tb\n",
      "Training time for epoch 1: 308.89415407180786 s\n",
      "Starting validation for epoch 1\n",
      "Writing to tb\n",
      "Initial validation loss (for one batch): 0.0018957530846819282\n",
      "Seen so far: 100 molecules\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Time taken for epoch 1: 310.82043719291687 s\n",
      "\n",
      "Start of epoch 0\n",
      "8 4 0.01\n",
      "4\n",
      "Initial loss (for one batch): 25.806169509887695\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 3.2930104732513428\n",
      "Training MAE (for one batch) at step 20: 0.41921544075012207\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 8.236807823181152 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 40: 42.060791015625\n",
      "Training MAE (for one batch) at step 40: 1.155685305595398\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 7.04440712928772 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 60: 4.673178195953369\n",
      "Training MAE (for one batch) at step 60: 0.5039855241775513\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 6.977246999740601 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 80: 1.0256389379501343\n",
      "Training MAE (for one batch) at step 80: 0.2610154151916504\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 7.110690116882324 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 100: 4.263436794281006\n",
      "Training MAE (for one batch) at step 100: 0.3678759038448334\n",
      "Seen so far: 10100 molecules\n",
      "Training time for 20 batches: 7.017128944396973 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 120: 17.428462982177734\n",
      "Training MAE (for one batch) at step 120: 0.863301157951355\n",
      "Seen so far: 12100 molecules\n",
      "Training time for 20 batches: 7.187922954559326 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 140: 2.0564355850219727\n",
      "Training MAE (for one batch) at step 140: 0.5743236541748047\n",
      "Seen so far: 14100 molecules\n",
      "Training time for 20 batches: 7.080343008041382 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 160: 0.6146554350852966\n",
      "Training MAE (for one batch) at step 160: 0.25318750739097595\n",
      "Seen so far: 16100 molecules\n",
      "Training time for 20 batches: 7.006402969360352 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 180: 4.26395845413208\n",
      "Training MAE (for one batch) at step 180: 0.33569419384002686\n",
      "Seen so far: 18100 molecules\n",
      "Training time for 20 batches: 7.155736207962036 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 200: 0.8341901898384094\n",
      "Training MAE (for one batch) at step 200: 0.37611135840415955\n",
      "Seen so far: 20100 molecules\n",
      "Training time for 20 batches: 7.321977853775024 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 220: 2.3536691665649414\n",
      "Training MAE (for one batch) at step 220: 0.5567803978919983\n",
      "Seen so far: 22100 molecules\n",
      "Training time for 20 batches: 7.56712794303894 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 240: 0.573414146900177\n",
      "Training MAE (for one batch) at step 240: 0.2722777724266052\n",
      "Seen so far: 24100 molecules\n",
      "Training time for 20 batches: 7.429585933685303 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 260: 1.274562954902649\n",
      "Training MAE (for one batch) at step 260: 0.3115166425704956\n",
      "Seen so far: 26100 molecules\n",
      "Training time for 20 batches: 7.324324131011963 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 280: 0.38025161623954773\n",
      "Training MAE (for one batch) at step 280: 0.13836480677127838\n",
      "Seen so far: 28100 molecules\n",
      "Training time for 20 batches: 7.520559072494507 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 300: 0.2470448911190033\n",
      "Training MAE (for one batch) at step 300: 0.1079249382019043\n",
      "Seen so far: 30100 molecules\n",
      "Training time for 20 batches: 7.3434367179870605 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 320: 0.325442910194397\n",
      "Training MAE (for one batch) at step 320: 0.1037311851978302\n",
      "Seen so far: 32100 molecules\n",
      "Training time for 20 batches: 7.310446262359619 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 340: 0.5748478770256042\n",
      "Training MAE (for one batch) at step 340: 0.12700611352920532\n",
      "Seen so far: 34100 molecules\n",
      "Training time for 20 batches: 7.463774919509888 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 360: 3.152714490890503\n",
      "Training MAE (for one batch) at step 360: 1.2321498394012451\n",
      "Seen so far: 36100 molecules\n",
      "Training time for 20 batches: 7.395638942718506 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 380: 0.8925788402557373\n",
      "Training MAE (for one batch) at step 380: 0.5721396207809448\n",
      "Seen so far: 38100 molecules\n",
      "Training time for 20 batches: 7.340092897415161 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 400: 1.5159262418746948\n",
      "Training MAE (for one batch) at step 400: 0.2693294584751129\n",
      "Seen so far: 40100 molecules\n",
      "Training time for 20 batches: 7.2768120765686035 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 420: 1.4995054006576538\n",
      "Training MAE (for one batch) at step 420: 0.2542128562927246\n",
      "Seen so far: 42100 molecules\n",
      "Training time for 20 batches: 7.201144218444824 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 440: 3.3255085945129395\n",
      "Training MAE (for one batch) at step 440: 1.2807122468948364\n",
      "Seen so far: 44100 molecules\n",
      "Training time for 20 batches: 7.038610935211182 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 460: 9.631036758422852\n",
      "Training MAE (for one batch) at step 460: 1.1143947839736938\n",
      "Seen so far: 46100 molecules\n",
      "Training time for 20 batches: 6.912992000579834 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 480: 0.5403515100479126\n",
      "Training MAE (for one batch) at step 480: 0.3591649830341339\n",
      "Seen so far: 48100 molecules\n",
      "Training time for 20 batches: 6.958276033401489 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 500: 0.6016290783882141\n",
      "Training MAE (for one batch) at step 500: 0.30415037274360657\n",
      "Seen so far: 50100 molecules\n",
      "Training time for 20 batches: 7.13616681098938 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 520: 1.3153642416000366\n",
      "Training MAE (for one batch) at step 520: 0.694628119468689\n",
      "Seen so far: 52100 molecules\n",
      "Training time for 20 batches: 6.888280153274536 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 540: 2.413079261779785\n",
      "Training MAE (for one batch) at step 540: 0.7693911790847778\n",
      "Seen so far: 54100 molecules\n",
      "Training time for 20 batches: 7.104067802429199 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 560: 6.073310852050781\n",
      "Training MAE (for one batch) at step 560: 0.36535385251045227\n",
      "Seen so far: 56100 molecules\n",
      "Training time for 20 batches: 7.074709177017212 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 580: 1.0058810710906982\n",
      "Training MAE (for one batch) at step 580: 0.5044102668762207\n",
      "Seen so far: 58100 molecules\n",
      "Training time for 20 batches: 7.127712965011597 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 600: 0.23077446222305298\n",
      "Training MAE (for one batch) at step 600: 0.1026146411895752\n",
      "Seen so far: 60100 molecules\n",
      "Training time for 20 batches: 7.0803070068359375 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 620: 1.2830877304077148\n",
      "Training MAE (for one batch) at step 620: 0.27097001671791077\n",
      "Seen so far: 62100 molecules\n",
      "Training time for 20 batches: 7.065303802490234 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 640: 0.4506024122238159\n",
      "Training MAE (for one batch) at step 640: 0.2203109860420227\n",
      "Seen so far: 64100 molecules\n",
      "Training time for 20 batches: 7.056768178939819 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 660: 1.2309660911560059\n",
      "Training MAE (for one batch) at step 660: 0.6399224400520325\n",
      "Seen so far: 66100 molecules\n",
      "Training time for 20 batches: 7.365867853164673 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 680: 1.084250807762146\n",
      "Training MAE (for one batch) at step 680: 0.24009862542152405\n",
      "Seen so far: 68100 molecules\n",
      "Training time for 20 batches: 7.120366096496582 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 700: 4.217133522033691\n",
      "Training MAE (for one batch) at step 700: 0.28931155800819397\n",
      "Seen so far: 70100 molecules\n",
      "Training time for 20 batches: 7.382597923278809 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 720: 0.32856690883636475\n",
      "Training MAE (for one batch) at step 720: 0.17566891014575958\n",
      "Seen so far: 72100 molecules\n",
      "Training time for 20 batches: 7.127732992172241 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 740: 0.10498175770044327\n",
      "Training MAE (for one batch) at step 740: 0.06481874734163284\n",
      "Seen so far: 74100 molecules\n",
      "Training time for 20 batches: 7.056503057479858 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 760: 0.30324169993400574\n",
      "Training MAE (for one batch) at step 760: 0.20281493663787842\n",
      "Seen so far: 76100 molecules\n",
      "Training time for 20 batches: 7.004634141921997 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 780: 1.9227144718170166\n",
      "Training MAE (for one batch) at step 780: 0.5785942673683167\n",
      "Seen so far: 78100 molecules\n",
      "Training time for 20 batches: 7.112830877304077 s\n",
      "Writing to tb\n",
      "Training time for epoch 1: 287.4605519771576 s\n",
      "Starting validation for epoch 1\n",
      "Writing to tb\n",
      "Initial validation loss (for one batch): 0.19880501925945282\n",
      "Seen so far: 100 molecules\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Time taken for epoch 1: 289.3315980434418 s\n",
      "\n",
      "Start of epoch 0\n",
      "8 4 0.03\n",
      "4\n",
      "Initial loss (for one batch): 21.69777488708496\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 16.25937843322754\n",
      "Training MAE (for one batch) at step 20: 1.6274032592773438\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 7.563126087188721 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 40: 8.289377212524414\n",
      "Training MAE (for one batch) at step 40: 1.2303341627120972\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 7.132782936096191 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 60: 3.5043983459472656\n",
      "Training MAE (for one batch) at step 60: 1.0207514762878418\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 7.122205018997192 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 80: 4.404271602630615\n",
      "Training MAE (for one batch) at step 80: 1.03549325466156\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 6.978172063827515 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 100: 7.3389129638671875\n",
      "Training MAE (for one batch) at step 100: 0.2282872498035431\n",
      "Seen so far: 10100 molecules\n",
      "Training time for 20 batches: 7.266924858093262 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 120: 1.089759111404419\n",
      "Training MAE (for one batch) at step 120: 0.578396201133728\n",
      "Seen so far: 12100 molecules\n",
      "Training time for 20 batches: 7.018213987350464 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 140: 4.940836429595947\n",
      "Training MAE (for one batch) at step 140: 1.0736513137817383\n",
      "Seen so far: 14100 molecules\n",
      "Training time for 20 batches: 6.998143911361694 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 160: 0.603512704372406\n",
      "Training MAE (for one batch) at step 160: 0.25510910153388977\n",
      "Seen so far: 16100 molecules\n",
      "Training time for 20 batches: 7.2770140171051025 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 180: 0.9727802276611328\n",
      "Training MAE (for one batch) at step 180: 0.37662872672080994\n",
      "Seen so far: 18100 molecules\n",
      "Training time for 20 batches: 7.147781133651733 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 200: 12.63492488861084\n",
      "Training MAE (for one batch) at step 200: 1.6019948720932007\n",
      "Seen so far: 20100 molecules\n",
      "Training time for 20 batches: 7.184535980224609 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 220: 7.424707889556885\n",
      "Training MAE (for one batch) at step 220: 1.3448766469955444\n",
      "Seen so far: 22100 molecules\n",
      "Training time for 20 batches: 7.1408469676971436 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 240: 4.737282752990723\n",
      "Training MAE (for one batch) at step 240: 1.1930737495422363\n",
      "Seen so far: 24100 molecules\n",
      "Training time for 20 batches: 7.0148632526397705 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 260: 2.409181833267212\n",
      "Training MAE (for one batch) at step 260: 0.3668416738510132\n",
      "Seen so far: 26100 molecules\n",
      "Training time for 20 batches: 6.8701136112213135 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 280: 1.887624979019165\n",
      "Training MAE (for one batch) at step 280: 0.4351716637611389\n",
      "Seen so far: 28100 molecules\n",
      "Training time for 20 batches: 6.8613622188568115 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 300: 1.8370444774627686\n",
      "Training MAE (for one batch) at step 300: 0.542357325553894\n",
      "Seen so far: 30100 molecules\n",
      "Training time for 20 batches: 6.849836111068726 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 320: 5.608004093170166\n",
      "Training MAE (for one batch) at step 320: 0.5333747863769531\n",
      "Seen so far: 32100 molecules\n",
      "Training time for 20 batches: 6.936638832092285 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 340: 0.9723289012908936\n",
      "Training MAE (for one batch) at step 340: 0.33889496326446533\n",
      "Seen so far: 34100 molecules\n",
      "Training time for 20 batches: 7.339416027069092 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 360: 3.540281295776367\n",
      "Training MAE (for one batch) at step 360: 0.7535401582717896\n",
      "Seen so far: 36100 molecules\n",
      "Training time for 20 batches: 6.958037853240967 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 380: 6.109994411468506\n",
      "Training MAE (for one batch) at step 380: 0.5260055661201477\n",
      "Seen so far: 38100 molecules\n",
      "Training time for 20 batches: 7.348402261734009 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 400: 23.058883666992188\n",
      "Training MAE (for one batch) at step 400: 0.8621981739997864\n",
      "Seen so far: 40100 molecules\n",
      "Training time for 20 batches: 7.39429783821106 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 420: 31.583118438720703\n",
      "Training MAE (for one batch) at step 420: 0.3742617964744568\n",
      "Seen so far: 42100 molecules\n",
      "Training time for 20 batches: 7.324944019317627 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 440: 8.89787483215332\n",
      "Training MAE (for one batch) at step 440: 0.4978029429912567\n",
      "Seen so far: 44100 molecules\n",
      "Training time for 20 batches: 7.214559078216553 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 460: 1.3655697107315063\n",
      "Training MAE (for one batch) at step 460: 0.6205521821975708\n",
      "Seen so far: 46100 molecules\n",
      "Training time for 20 batches: 7.18013596534729 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 480: 0.5020207762718201\n",
      "Training MAE (for one batch) at step 480: 0.29431551694869995\n",
      "Seen so far: 48100 molecules\n",
      "Training time for 20 batches: 7.255393981933594 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 500: 0.1356324404478073\n",
      "Training MAE (for one batch) at step 500: 0.11006338149309158\n",
      "Seen so far: 50100 molecules\n",
      "Training time for 20 batches: 7.221508026123047 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 520: 1.03476083278656\n",
      "Training MAE (for one batch) at step 520: 0.5575450658798218\n",
      "Seen so far: 52100 molecules\n",
      "Training time for 20 batches: 7.29389500617981 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 540: 0.2058955729007721\n",
      "Training MAE (for one batch) at step 540: 0.20179980993270874\n",
      "Seen so far: 54100 molecules\n",
      "Training time for 20 batches: 6.844274997711182 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 560: 7.807314872741699\n",
      "Training MAE (for one batch) at step 560: 1.4242485761642456\n",
      "Seen so far: 56100 molecules\n",
      "Training time for 20 batches: 6.950875997543335 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 580: 0.3748037815093994\n",
      "Training MAE (for one batch) at step 580: 0.3097180426120758\n",
      "Seen so far: 58100 molecules\n",
      "Training time for 20 batches: 7.0284059047698975 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 600: 1.3015246391296387\n",
      "Training MAE (for one batch) at step 600: 0.9104451537132263\n",
      "Seen so far: 60100 molecules\n",
      "Training time for 20 batches: 7.084622144699097 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 620: 0.8110959529876709\n",
      "Training MAE (for one batch) at step 620: 0.7312312126159668\n",
      "Seen so far: 62100 molecules\n",
      "Training time for 20 batches: 7.189295053482056 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 640: 0.7446449995040894\n",
      "Training MAE (for one batch) at step 640: 0.6242958903312683\n",
      "Seen so far: 64100 molecules\n",
      "Training time for 20 batches: 7.1505208015441895 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 660: 0.7509336471557617\n",
      "Training MAE (for one batch) at step 660: 0.605789840221405\n",
      "Seen so far: 66100 molecules\n",
      "Training time for 20 batches: 7.585835933685303 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 680: 0.37035608291625977\n",
      "Training MAE (for one batch) at step 680: 0.48082494735717773\n",
      "Seen so far: 68100 molecules\n",
      "Training time for 20 batches: 7.262087345123291 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 700: 0.3171749711036682\n",
      "Training MAE (for one batch) at step 700: 0.4359477162361145\n",
      "Seen so far: 70100 molecules\n",
      "Training time for 20 batches: 7.046904802322388 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 720: 0.4894968569278717\n",
      "Training MAE (for one batch) at step 720: 0.5327140688896179\n",
      "Seen so far: 72100 molecules\n",
      "Training time for 20 batches: 7.163769006729126 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 740: 0.37463197112083435\n",
      "Training MAE (for one batch) at step 740: 0.42928317189216614\n",
      "Seen so far: 74100 molecules\n",
      "Training time for 20 batches: 7.1431591510772705 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 760: 0.13779392838478088\n",
      "Training MAE (for one batch) at step 760: 0.23877306282520294\n",
      "Seen so far: 76100 molecules\n",
      "Training time for 20 batches: 7.139356851577759 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 780: 0.43557271361351013\n",
      "Training MAE (for one batch) at step 780: 0.5558039546012878\n",
      "Seen so far: 78100 molecules\n",
      "Training time for 20 batches: 7.296566963195801 s\n",
      "Writing to tb\n",
      "Training time for epoch 1: 285.6099510192871 s\n",
      "Starting validation for epoch 1\n",
      "Writing to tb\n",
      "Initial validation loss (for one batch): 0.011375932022929192\n",
      "Seen so far: 100 molecules\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Time taken for epoch 1: 287.43013095855713 s\n",
      "\n",
      "Start of epoch 0\n",
      "8 8 0.0003\n",
      "8\n",
      "Initial loss (for one batch): 42.46836471557617\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 23.472867965698242\n",
      "Training MAE (for one batch) at step 20: 0.6766778230667114\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 15.402724981307983 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 40: 15.114940643310547\n",
      "Training MAE (for one batch) at step 40: 0.43424877524375916\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 14.791734218597412 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 60: 11.941816329956055\n",
      "Training MAE (for one batch) at step 60: 0.29816412925720215\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 14.495967864990234 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 80: 10.714323997497559\n",
      "Training MAE (for one batch) at step 80: 0.3546236455440521\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 14.939101934432983 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 100: 5.531681060791016\n",
      "Training MAE (for one batch) at step 100: 0.38633841276168823\n",
      "Seen so far: 10100 molecules\n",
      "Training time for 20 batches: 15.047696113586426 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 120: 3.286923408508301\n",
      "Training MAE (for one batch) at step 120: 0.3510173261165619\n",
      "Seen so far: 12100 molecules\n",
      "Training time for 20 batches: 14.903969049453735 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 140: 1.8730416297912598\n",
      "Training MAE (for one batch) at step 140: 0.23079760372638702\n",
      "Seen so far: 14100 molecules\n",
      "Training time for 20 batches: 14.832685947418213 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 160: 0.9870462417602539\n",
      "Training MAE (for one batch) at step 160: 0.15442653000354767\n",
      "Seen so far: 16100 molecules\n",
      "Training time for 20 batches: 14.920337200164795 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 180: 0.7591780424118042\n",
      "Training MAE (for one batch) at step 180: 0.1616957187652588\n",
      "Seen so far: 18100 molecules\n",
      "Training time for 20 batches: 15.122689723968506 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 200: 0.4052039086818695\n",
      "Training MAE (for one batch) at step 200: 0.1360798180103302\n",
      "Seen so far: 20100 molecules\n",
      "Training time for 20 batches: 15.339371919631958 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 220: 0.31908535957336426\n",
      "Training MAE (for one batch) at step 220: 0.11286856234073639\n",
      "Seen so far: 22100 molecules\n",
      "Training time for 20 batches: 15.512174129486084 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 240: 0.19966498017311096\n",
      "Training MAE (for one batch) at step 240: 0.11323601752519608\n",
      "Seen so far: 24100 molecules\n",
      "Training time for 20 batches: 15.126767873764038 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 260: 0.15406078100204468\n",
      "Training MAE (for one batch) at step 260: 0.0740426704287529\n",
      "Seen so far: 26100 molecules\n",
      "Training time for 20 batches: 15.214028120040894 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 280: 0.16346177458763123\n",
      "Training MAE (for one batch) at step 280: 0.08268442749977112\n",
      "Seen so far: 28100 molecules\n",
      "Training time for 20 batches: 15.379929065704346 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 300: 0.16238656640052795\n",
      "Training MAE (for one batch) at step 300: 0.07548008859157562\n",
      "Seen so far: 30100 molecules\n",
      "Training time for 20 batches: 15.586425065994263 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 320: 0.06966643035411835\n",
      "Training MAE (for one batch) at step 320: 0.06966672837734222\n",
      "Seen so far: 32100 molecules\n",
      "Training time for 20 batches: 15.467110872268677 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 340: 0.08336281776428223\n",
      "Training MAE (for one batch) at step 340: 0.06277046352624893\n",
      "Seen so far: 34100 molecules\n",
      "Training time for 20 batches: 14.791764974594116 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 360: 0.052241213619709015\n",
      "Training MAE (for one batch) at step 360: 0.06371286511421204\n",
      "Seen so far: 36100 molecules\n",
      "Training time for 20 batches: 14.976626873016357 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 380: 0.15699058771133423\n",
      "Training MAE (for one batch) at step 380: 0.06962095201015472\n",
      "Seen so far: 38100 molecules\n",
      "Training time for 20 batches: 15.130094289779663 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 400: 0.04740392416715622\n",
      "Training MAE (for one batch) at step 400: 0.06684652715921402\n",
      "Seen so far: 40100 molecules\n",
      "Training time for 20 batches: 15.459703922271729 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 420: 0.035638485103845596\n",
      "Training MAE (for one batch) at step 420: 0.08087213337421417\n",
      "Seen so far: 42100 molecules\n",
      "Training time for 20 batches: 15.2430579662323 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 440: 0.03903165087103844\n",
      "Training MAE (for one batch) at step 440: 0.08187144994735718\n",
      "Seen so far: 44100 molecules\n",
      "Training time for 20 batches: 15.313395023345947 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 460: 0.025235477834939957\n",
      "Training MAE (for one batch) at step 460: 0.04613955691456795\n",
      "Seen so far: 46100 molecules\n",
      "Training time for 20 batches: 15.592983961105347 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 480: 0.020266298204660416\n",
      "Training MAE (for one batch) at step 480: 0.044583510607481\n",
      "Seen so far: 48100 molecules\n",
      "Training time for 20 batches: 15.591125011444092 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 500: 0.03882269561290741\n",
      "Training MAE (for one batch) at step 500: 0.04935875162482262\n",
      "Seen so far: 50100 molecules\n",
      "Training time for 20 batches: 15.215136051177979 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 520: 0.035432882606983185\n",
      "Training MAE (for one batch) at step 520: 0.04471121355891228\n",
      "Seen so far: 52100 molecules\n",
      "Training time for 20 batches: 14.834705829620361 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 540: 0.01753869466483593\n",
      "Training MAE (for one batch) at step 540: 0.03920603543519974\n",
      "Seen so far: 54100 molecules\n",
      "Training time for 20 batches: 15.08772611618042 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 560: 0.03136337548494339\n",
      "Training MAE (for one batch) at step 560: 0.04571928828954697\n",
      "Seen so far: 56100 molecules\n",
      "Training time for 20 batches: 15.607409000396729 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 580: 0.022397464141249657\n",
      "Training MAE (for one batch) at step 580: 0.047892555594444275\n",
      "Seen so far: 58100 molecules\n",
      "Training time for 20 batches: 15.805669069290161 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 600: 0.10031339526176453\n",
      "Training MAE (for one batch) at step 600: 0.06664693355560303\n",
      "Seen so far: 60100 molecules\n",
      "Training time for 20 batches: 15.802175998687744 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 620: 0.029726963490247726\n",
      "Training MAE (for one batch) at step 620: 0.039933811873197556\n",
      "Seen so far: 62100 molecules\n",
      "Training time for 20 batches: 15.792338848114014 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 640: 0.018148647621273994\n",
      "Training MAE (for one batch) at step 640: 0.03913058340549469\n",
      "Seen so far: 64100 molecules\n",
      "Training time for 20 batches: 15.585546970367432 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 660: 0.02505352720618248\n",
      "Training MAE (for one batch) at step 660: 0.07358992099761963\n",
      "Seen so far: 66100 molecules\n",
      "Training time for 20 batches: 15.956889152526855 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 680: 0.030882705003023148\n",
      "Training MAE (for one batch) at step 680: 0.07026202231645584\n",
      "Seen so far: 68100 molecules\n",
      "Training time for 20 batches: 15.27644395828247 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 700: 0.018635962158441544\n",
      "Training MAE (for one batch) at step 700: 0.02896692231297493\n",
      "Seen so far: 70100 molecules\n",
      "Training time for 20 batches: 15.351184129714966 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 720: 0.021743416786193848\n",
      "Training MAE (for one batch) at step 720: 0.030457383021712303\n",
      "Seen so far: 72100 molecules\n",
      "Training time for 20 batches: 15.593575954437256 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 740: 0.017606794834136963\n",
      "Training MAE (for one batch) at step 740: 0.022384162992239\n",
      "Seen so far: 74100 molecules\n",
      "Training time for 20 batches: 15.422938108444214 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 760: 0.011050759814679623\n",
      "Training MAE (for one batch) at step 760: 0.027319299057126045\n",
      "Seen so far: 76100 molecules\n",
      "Training time for 20 batches: 15.31583285331726 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 780: 0.013177352026104927\n",
      "Training MAE (for one batch) at step 780: 0.029524022713303566\n",
      "Seen so far: 78100 molecules\n",
      "Training time for 20 batches: 14.676939010620117 s\n",
      "Writing to tb\n",
      "Training time for epoch 1: 609.6772689819336 s\n",
      "Starting validation for epoch 1\n",
      "Writing to tb\n",
      "Initial validation loss (for one batch): 0.0004172067274339497\n",
      "Seen so far: 100 molecules\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Time taken for epoch 1: 613.3614230155945 s\n",
      "\n",
      "Start of epoch 0\n",
      "8 8 0.01\n",
      "8\n",
      "Initial loss (for one batch): 48.97613525390625\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 29.754150390625\n",
      "Training MAE (for one batch) at step 20: 0.6970869302749634\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 16.09919500350952 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 40: 4.108325004577637\n",
      "Training MAE (for one batch) at step 40: 0.5985085368156433\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 15.611036777496338 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 60: 2.4018073081970215\n",
      "Training MAE (for one batch) at step 60: 0.18498584628105164\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 15.663841009140015 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 80: 1.0055145025253296\n",
      "Training MAE (for one batch) at step 80: 0.5071741342544556\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 15.327282190322876 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 100: 3.094055652618408\n",
      "Training MAE (for one batch) at step 100: 0.974415123462677\n",
      "Seen so far: 10100 molecules\n",
      "Training time for 20 batches: 15.55538010597229 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 120: 1.746842384338379\n",
      "Training MAE (for one batch) at step 120: 0.6138141751289368\n",
      "Seen so far: 12100 molecules\n",
      "Training time for 20 batches: 15.848904848098755 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 140: 5.24034309387207\n",
      "Training MAE (for one batch) at step 140: 0.7631580829620361\n",
      "Seen so far: 14100 molecules\n",
      "Training time for 20 batches: 15.47510313987732 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 160: 3.3291282653808594\n",
      "Training MAE (for one batch) at step 160: 0.45513102412223816\n",
      "Seen so far: 16100 molecules\n",
      "Training time for 20 batches: 15.46251106262207 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 180: 1.7187080383300781\n",
      "Training MAE (for one batch) at step 180: 0.35086989402770996\n",
      "Seen so far: 18100 molecules\n",
      "Training time for 20 batches: 15.665751695632935 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 200: 7.327420234680176\n",
      "Training MAE (for one batch) at step 200: 0.41352707147598267\n",
      "Seen so far: 20100 molecules\n",
      "Training time for 20 batches: 15.615992069244385 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 220: 0.740997850894928\n",
      "Training MAE (for one batch) at step 220: 0.26929548382759094\n",
      "Seen so far: 22100 molecules\n",
      "Training time for 20 batches: 15.995211124420166 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 240: 0.2804281711578369\n",
      "Training MAE (for one batch) at step 240: 0.2008371353149414\n",
      "Seen so far: 24100 molecules\n",
      "Training time for 20 batches: 15.87285304069519 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 260: 0.21867114305496216\n",
      "Training MAE (for one batch) at step 260: 0.1917276829481125\n",
      "Seen so far: 26100 molecules\n",
      "Training time for 20 batches: 16.18113398551941 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 280: 1.964379906654358\n",
      "Training MAE (for one batch) at step 280: 0.6801708936691284\n",
      "Seen so far: 28100 molecules\n",
      "Training time for 20 batches: 15.667530059814453 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 300: 1.1330472230911255\n",
      "Training MAE (for one batch) at step 300: 0.2497650682926178\n",
      "Seen so far: 30100 molecules\n",
      "Training time for 20 batches: 15.630208015441895 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 320: 0.43279436230659485\n",
      "Training MAE (for one batch) at step 320: 0.2810322940349579\n",
      "Seen so far: 32100 molecules\n",
      "Training time for 20 batches: 15.423827886581421 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 340: 2.83644437789917\n",
      "Training MAE (for one batch) at step 340: 0.37196704745292664\n",
      "Seen so far: 34100 molecules\n",
      "Training time for 20 batches: 15.51598596572876 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 360: 2.00311541557312\n",
      "Training MAE (for one batch) at step 360: 0.21828973293304443\n",
      "Seen so far: 36100 molecules\n",
      "Training time for 20 batches: 15.313373804092407 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 380: 1.2995201349258423\n",
      "Training MAE (for one batch) at step 380: 0.5668411254882812\n",
      "Seen so far: 38100 molecules\n",
      "Training time for 20 batches: 15.523685216903687 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 400: 0.28784456849098206\n",
      "Training MAE (for one batch) at step 400: 0.2132849246263504\n",
      "Seen so far: 40100 molecules\n",
      "Training time for 20 batches: 15.407954931259155 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 420: 13.623741149902344\n",
      "Training MAE (for one batch) at step 420: 0.22309868037700653\n",
      "Seen so far: 42100 molecules\n",
      "Training time for 20 batches: 16.003700017929077 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 440: 1.1598503589630127\n",
      "Training MAE (for one batch) at step 440: 0.529504656791687\n",
      "Seen so far: 44100 molecules\n",
      "Training time for 20 batches: 16.14157795906067 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 460: 1.4000176191329956\n",
      "Training MAE (for one batch) at step 460: 0.1746860146522522\n",
      "Seen so far: 46100 molecules\n",
      "Training time for 20 batches: 16.106302976608276 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 480: 5.28373384475708\n",
      "Training MAE (for one batch) at step 480: 0.9098084568977356\n",
      "Seen so far: 48100 molecules\n",
      "Training time for 20 batches: 16.34027624130249 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 500: 1.9395142793655396\n",
      "Training MAE (for one batch) at step 500: 0.7601145505905151\n",
      "Seen so far: 50100 molecules\n",
      "Training time for 20 batches: 15.85456371307373 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 520: 2.1602327823638916\n",
      "Training MAE (for one batch) at step 520: 0.17643988132476807\n",
      "Seen so far: 52100 molecules\n",
      "Training time for 20 batches: 15.82788610458374 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 540: 1.5319390296936035\n",
      "Training MAE (for one batch) at step 540: 0.8736507892608643\n",
      "Seen so far: 54100 molecules\n",
      "Training time for 20 batches: 15.814533948898315 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 560: 0.7040776014328003\n",
      "Training MAE (for one batch) at step 560: 0.6501466035842896\n",
      "Seen so far: 56100 molecules\n",
      "Training time for 20 batches: 16.19551992416382 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 580: 0.7609178423881531\n",
      "Training MAE (for one batch) at step 580: 0.33738312125205994\n",
      "Seen so far: 58100 molecules\n",
      "Training time for 20 batches: 15.738682270050049 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 600: 0.437122642993927\n",
      "Training MAE (for one batch) at step 600: 0.4310300350189209\n",
      "Seen so far: 60100 molecules\n",
      "Training time for 20 batches: 16.278191804885864 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 620: 3.0823471546173096\n",
      "Training MAE (for one batch) at step 620: 0.5556646585464478\n",
      "Seen so far: 62100 molecules\n",
      "Training time for 20 batches: 16.132097959518433 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 640: 2.3270881175994873\n",
      "Training MAE (for one batch) at step 640: 0.8071605563163757\n",
      "Seen so far: 64100 molecules\n",
      "Training time for 20 batches: 15.966094255447388 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 660: 1.1698237657546997\n",
      "Training MAE (for one batch) at step 660: 0.15162064135074615\n",
      "Seen so far: 66100 molecules\n",
      "Training time for 20 batches: 15.79816484451294 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 680: 0.9409890174865723\n",
      "Training MAE (for one batch) at step 680: 0.6614036560058594\n",
      "Seen so far: 68100 molecules\n",
      "Training time for 20 batches: 15.687026023864746 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 700: 0.9010735750198364\n",
      "Training MAE (for one batch) at step 700: 0.6610233783721924\n",
      "Seen so far: 70100 molecules\n",
      "Training time for 20 batches: 15.677175045013428 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 720: 1.1495167016983032\n",
      "Training MAE (for one batch) at step 720: 0.30023354291915894\n",
      "Seen so far: 72100 molecules\n",
      "Training time for 20 batches: 15.78702688217163 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 740: 0.32383742928504944\n",
      "Training MAE (for one batch) at step 740: 0.2140798717737198\n",
      "Seen so far: 74100 molecules\n",
      "Training time for 20 batches: 16.233471870422363 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 760: 0.3034597337245941\n",
      "Training MAE (for one batch) at step 760: 0.37151092290878296\n",
      "Seen so far: 76100 molecules\n",
      "Training time for 20 batches: 16.555115222930908 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 780: 0.35791516304016113\n",
      "Training MAE (for one batch) at step 780: 0.19663366675376892\n",
      "Seen so far: 78100 molecules\n",
      "Training time for 20 batches: 15.922825813293457 s\n",
      "Writing to tb\n",
      "Training time for epoch 1: 631.6980171203613 s\n",
      "Starting validation for epoch 1\n",
      "Writing to tb\n",
      "Initial validation loss (for one batch): 0.06232675909996033\n",
      "Seen so far: 100 molecules\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Time taken for epoch 1: 635.3765578269958 s\n",
      "\n",
      "Start of epoch 0\n",
      "8 8 0.03\n",
      "8\n",
      "Initial loss (for one batch): 42.17091369628906\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 16.696407318115234\n",
      "Training MAE (for one batch) at step 20: 0.7556251883506775\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 15.894796133041382 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 40: 5.626795291900635\n",
      "Training MAE (for one batch) at step 40: 0.7331337332725525\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 15.276695966720581 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 60: 3.0919785499572754\n",
      "Training MAE (for one batch) at step 60: 0.8306999206542969\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 15.670701026916504 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 80: 7.538829326629639\n",
      "Training MAE (for one batch) at step 80: 0.5187711715698242\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 15.51505708694458 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 100: 13.005221366882324\n",
      "Training MAE (for one batch) at step 100: 3.325124502182007\n",
      "Seen so far: 10100 molecules\n",
      "Training time for 20 batches: 14.72155475616455 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 120: 1.3813889026641846\n",
      "Training MAE (for one batch) at step 120: 0.6668740510940552\n",
      "Seen so far: 12100 molecules\n",
      "Training time for 20 batches: 14.777373313903809 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 140: 1.3943254947662354\n",
      "Training MAE (for one batch) at step 140: 0.7558647990226746\n",
      "Seen so far: 14100 molecules\n",
      "Training time for 20 batches: 14.861626863479614 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 160: 2.7133443355560303\n",
      "Training MAE (for one batch) at step 160: 1.5800732374191284\n",
      "Seen so far: 16100 molecules\n",
      "Training time for 20 batches: 14.841055870056152 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 180: 3.162884473800659\n",
      "Training MAE (for one batch) at step 180: 1.091498613357544\n",
      "Seen so far: 18100 molecules\n",
      "Training time for 20 batches: 14.700130224227905 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 200: 1.8105648756027222\n",
      "Training MAE (for one batch) at step 200: 0.20389685034751892\n",
      "Seen so far: 20100 molecules\n",
      "Training time for 20 batches: 15.220248937606812 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 220: 1.0002613067626953\n",
      "Training MAE (for one batch) at step 220: 0.26856136322021484\n",
      "Seen so far: 22100 molecules\n",
      "Training time for 20 batches: 15.396779775619507 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 240: 0.7855592966079712\n",
      "Training MAE (for one batch) at step 240: 0.5830425024032593\n",
      "Seen so far: 24100 molecules\n",
      "Training time for 20 batches: 15.69628119468689 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 260: 0.42111727595329285\n",
      "Training MAE (for one batch) at step 260: 0.5507877469062805\n",
      "Seen so far: 26100 molecules\n",
      "Training time for 20 batches: 15.90694284439087 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 280: 0.5882183313369751\n",
      "Training MAE (for one batch) at step 280: 0.5814167857170105\n",
      "Seen so far: 28100 molecules\n",
      "Training time for 20 batches: 15.725773096084595 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 300: 0.20396435260772705\n",
      "Training MAE (for one batch) at step 300: 0.29198378324508667\n",
      "Seen so far: 30100 molecules\n",
      "Training time for 20 batches: 14.88746190071106 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 320: 0.5320098400115967\n",
      "Training MAE (for one batch) at step 320: 0.603257417678833\n",
      "Seen so far: 32100 molecules\n",
      "Training time for 20 batches: 14.802288055419922 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 340: 0.4834473729133606\n",
      "Training MAE (for one batch) at step 340: 0.5159620642662048\n",
      "Seen so far: 34100 molecules\n",
      "Training time for 20 batches: 15.313853025436401 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 360: 2.7254791259765625\n",
      "Training MAE (for one batch) at step 360: 0.39006906747817993\n",
      "Seen so far: 36100 molecules\n",
      "Training time for 20 batches: 15.408137083053589 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 380: 1.7248597145080566\n",
      "Training MAE (for one batch) at step 380: 1.1473205089569092\n",
      "Seen so far: 38100 molecules\n",
      "Training time for 20 batches: 15.534519910812378 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 400: 1.3435800075531006\n",
      "Training MAE (for one batch) at step 400: 0.9650898575782776\n",
      "Seen so far: 40100 molecules\n",
      "Training time for 20 batches: 15.391999959945679 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 420: 3.0939548015594482\n",
      "Training MAE (for one batch) at step 420: 1.6050912141799927\n",
      "Seen so far: 42100 molecules\n",
      "Training time for 20 batches: 14.866124153137207 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 440: 1.2743812799453735\n",
      "Training MAE (for one batch) at step 440: 0.8153136968612671\n",
      "Seen so far: 44100 molecules\n",
      "Training time for 20 batches: 14.896703004837036 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 460: 0.7417726516723633\n",
      "Training MAE (for one batch) at step 460: 0.6683802008628845\n",
      "Seen so far: 46100 molecules\n",
      "Training time for 20 batches: 14.849706888198853 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 480: 1.972144603729248\n",
      "Training MAE (for one batch) at step 480: 1.3156280517578125\n",
      "Seen so far: 48100 molecules\n",
      "Training time for 20 batches: 14.653005123138428 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 500: 1.3900352716445923\n",
      "Training MAE (for one batch) at step 500: 0.7025871872901917\n",
      "Seen so far: 50100 molecules\n",
      "Training time for 20 batches: 14.790029764175415 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 520: 0.42314574122428894\n",
      "Training MAE (for one batch) at step 520: 0.5274079442024231\n",
      "Seen so far: 52100 molecules\n",
      "Training time for 20 batches: 14.569416284561157 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 540: 1.398819923400879\n",
      "Training MAE (for one batch) at step 540: 1.0908280611038208\n",
      "Seen so far: 54100 molecules\n",
      "Training time for 20 batches: 14.926105737686157 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 560: 0.8312892317771912\n",
      "Training MAE (for one batch) at step 560: 0.6349425911903381\n",
      "Seen so far: 56100 molecules\n",
      "Training time for 20 batches: 14.929471015930176 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 580: 0.3635573387145996\n",
      "Training MAE (for one batch) at step 580: 0.49431294202804565\n",
      "Seen so far: 58100 molecules\n",
      "Training time for 20 batches: 15.13879919052124 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 600: 1.5654054880142212\n",
      "Training MAE (for one batch) at step 600: 1.1496294736862183\n",
      "Seen so far: 60100 molecules\n",
      "Training time for 20 batches: 15.18241000175476 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 620: 0.6961818337440491\n",
      "Training MAE (for one batch) at step 620: 0.5887949466705322\n",
      "Seen so far: 62100 molecules\n",
      "Training time for 20 batches: 15.168786764144897 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 640: 0.39667218923568726\n",
      "Training MAE (for one batch) at step 640: 0.45167744159698486\n",
      "Seen so far: 64100 molecules\n",
      "Training time for 20 batches: 15.078665018081665 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 660: 1.117418885231018\n",
      "Training MAE (for one batch) at step 660: 0.9181437492370605\n",
      "Seen so far: 66100 molecules\n",
      "Training time for 20 batches: 15.15837836265564 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 680: 0.7132871747016907\n",
      "Training MAE (for one batch) at step 680: 0.5613324046134949\n",
      "Seen so far: 68100 molecules\n",
      "Training time for 20 batches: 14.869770765304565 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 700: 0.40676242113113403\n",
      "Training MAE (for one batch) at step 700: 0.49364766478538513\n",
      "Seen so far: 70100 molecules\n",
      "Training time for 20 batches: 14.843784093856812 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 720: 0.8685263395309448\n",
      "Training MAE (for one batch) at step 720: 0.6833468675613403\n",
      "Seen so far: 72100 molecules\n",
      "Training time for 20 batches: 14.868420839309692 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 740: 0.5833935737609863\n",
      "Training MAE (for one batch) at step 740: 0.6468530297279358\n",
      "Seen so far: 74100 molecules\n",
      "Training time for 20 batches: 15.575232028961182 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 760: 0.3130260109901428\n",
      "Training MAE (for one batch) at step 760: 0.43408113718032837\n",
      "Seen so far: 76100 molecules\n",
      "Training time for 20 batches: 15.700515031814575 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 780: 0.7845721244812012\n",
      "Training MAE (for one batch) at step 780: 0.8269913792610168\n",
      "Seen so far: 78100 molecules\n",
      "Training time for 20 batches: 15.60727310180664 s\n",
      "Writing to tb\n",
      "Training time for epoch 1: 606.0276689529419 s\n",
      "Starting validation for epoch 1\n",
      "Writing to tb\n",
      "Initial validation loss (for one batch): 0.3814713656902313\n",
      "Seen so far: 100 molecules\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Time taken for epoch 1: 609.8031721115112 s\n",
      "\n",
      "Start of epoch 0\n",
      "8 16 0.0003\n",
      "16\n",
      "Initial loss (for one batch): 106.4858627319336\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 18.930004119873047\n",
      "Training MAE (for one batch) at step 20: 0.9041203260421753\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 29.14670968055725 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 40: 12.85072135925293\n",
      "Training MAE (for one batch) at step 40: 0.38839057087898254\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 26.935574054718018 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 60: 3.4659628868103027\n",
      "Training MAE (for one batch) at step 60: 0.3616739511489868\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 27.05709719657898 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 80: 1.5527021884918213\n",
      "Training MAE (for one batch) at step 80: 0.31841933727264404\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 27.05507802963257 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 100: 0.5931602120399475\n",
      "Training MAE (for one batch) at step 100: 0.23568473756313324\n",
      "Seen so far: 10100 molecules\n",
      "Training time for 20 batches: 26.750598907470703 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 120: 0.41598889231681824\n",
      "Training MAE (for one batch) at step 120: 0.188557431101799\n",
      "Seen so far: 12100 molecules\n",
      "Training time for 20 batches: 26.241739988327026 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 140: 0.3010924756526947\n",
      "Training MAE (for one batch) at step 140: 0.18768174946308136\n",
      "Seen so far: 14100 molecules\n",
      "Training time for 20 batches: 25.930495023727417 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 160: 0.25526437163352966\n",
      "Training MAE (for one batch) at step 160: 0.18007288873195648\n",
      "Seen so far: 16100 molecules\n",
      "Training time for 20 batches: 26.93346095085144 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 180: 0.15653851628303528\n",
      "Training MAE (for one batch) at step 180: 0.14107133448123932\n",
      "Seen so far: 18100 molecules\n",
      "Training time for 20 batches: 27.131530046463013 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 200: 0.15345150232315063\n",
      "Training MAE (for one batch) at step 200: 0.13350403308868408\n",
      "Seen so far: 20100 molecules\n",
      "Training time for 20 batches: 27.215576887130737 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 220: 0.12241534888744354\n",
      "Training MAE (for one batch) at step 220: 0.12048935145139694\n",
      "Seen so far: 22100 molecules\n",
      "Training time for 20 batches: 27.29275608062744 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 240: 0.10374744236469269\n",
      "Training MAE (for one batch) at step 240: 0.10585292428731918\n",
      "Seen so far: 24100 molecules\n",
      "Training time for 20 batches: 27.21057891845703 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 260: 0.10523709654808044\n",
      "Training MAE (for one batch) at step 260: 0.1069066971540451\n",
      "Seen so far: 26100 molecules\n",
      "Training time for 20 batches: 27.475614309310913 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 280: 0.08951736986637115\n",
      "Training MAE (for one batch) at step 280: 0.12386249750852585\n",
      "Seen so far: 28100 molecules\n",
      "Training time for 20 batches: 27.61184000968933 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 300: 0.06373894214630127\n",
      "Training MAE (for one batch) at step 300: 0.08442506939172745\n",
      "Seen so far: 30100 molecules\n",
      "Training time for 20 batches: 27.50432586669922 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 320: 0.06428080052137375\n",
      "Training MAE (for one batch) at step 320: 0.05793199688196182\n",
      "Seen so far: 32100 molecules\n",
      "Training time for 20 batches: 26.690080881118774 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 340: 0.05915980786085129\n",
      "Training MAE (for one batch) at step 340: 0.08570761233568192\n",
      "Seen so far: 34100 molecules\n",
      "Training time for 20 batches: 26.198503017425537 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 360: 0.060667213052511215\n",
      "Training MAE (for one batch) at step 360: 0.059406135231256485\n",
      "Seen so far: 36100 molecules\n",
      "Training time for 20 batches: 26.571358919143677 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 380: 0.056638434529304504\n",
      "Training MAE (for one batch) at step 380: 0.10149442404508591\n",
      "Seen so far: 38100 molecules\n",
      "Training time for 20 batches: 26.576292037963867 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 400: 0.034829676151275635\n",
      "Training MAE (for one batch) at step 400: 0.06349002569913864\n",
      "Seen so far: 40100 molecules\n",
      "Training time for 20 batches: 26.05081605911255 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 420: 0.025903329253196716\n",
      "Training MAE (for one batch) at step 420: 0.03998100385069847\n",
      "Seen so far: 42100 molecules\n",
      "Training time for 20 batches: 25.845072984695435 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 440: 0.038968805223703384\n",
      "Training MAE (for one batch) at step 440: 0.08397345244884491\n",
      "Seen so far: 44100 molecules\n",
      "Training time for 20 batches: 25.938538074493408 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 460: 0.08582504093647003\n",
      "Training MAE (for one batch) at step 460: 0.13841719925403595\n",
      "Seen so far: 46100 molecules\n",
      "Training time for 20 batches: 25.224476099014282 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 480: 0.022237906232476234\n",
      "Training MAE (for one batch) at step 480: 0.033777449280023575\n",
      "Seen so far: 48100 molecules\n",
      "Training time for 20 batches: 25.224201917648315 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 500: 0.02195289172232151\n",
      "Training MAE (for one batch) at step 500: 0.03528256341814995\n",
      "Seen so far: 50100 molecules\n",
      "Training time for 20 batches: 25.245366096496582 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 520: 0.02315939962863922\n",
      "Training MAE (for one batch) at step 520: 0.02723398618400097\n",
      "Seen so far: 52100 molecules\n",
      "Training time for 20 batches: 25.27200675010681 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 540: 0.021994642913341522\n",
      "Training MAE (for one batch) at step 540: 0.04075867682695389\n",
      "Seen so far: 54100 molecules\n",
      "Training time for 20 batches: 25.48128890991211 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 560: 0.05515999719500542\n",
      "Training MAE (for one batch) at step 560: 0.059945248067379\n",
      "Seen so far: 56100 molecules\n",
      "Training time for 20 batches: 25.758203268051147 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 580: 0.016554303467273712\n",
      "Training MAE (for one batch) at step 580: 0.034771718084812164\n",
      "Seen so far: 58100 molecules\n",
      "Training time for 20 batches: 26.438019037246704 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 600: 0.02199806459248066\n",
      "Training MAE (for one batch) at step 600: 0.03068496286869049\n",
      "Seen so far: 60100 molecules\n",
      "Training time for 20 batches: 26.291260957717896 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 620: 0.020257839933037758\n",
      "Training MAE (for one batch) at step 620: 0.026470797136425972\n",
      "Seen so far: 62100 molecules\n",
      "Training time for 20 batches: 26.730412006378174 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 640: 0.022955158725380898\n",
      "Training MAE (for one batch) at step 640: 0.03343695029616356\n",
      "Seen so far: 64100 molecules\n",
      "Training time for 20 batches: 26.723874807357788 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 660: 0.06850460171699524\n",
      "Training MAE (for one batch) at step 660: 0.06672731786966324\n",
      "Seen so far: 66100 molecules\n",
      "Training time for 20 batches: 26.357465028762817 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 680: 0.009480990469455719\n",
      "Training MAE (for one batch) at step 680: 0.02400018274784088\n",
      "Seen so far: 68100 molecules\n",
      "Training time for 20 batches: 25.143666982650757 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 700: 0.02372031658887863\n",
      "Training MAE (for one batch) at step 700: 0.030628535896539688\n",
      "Seen so far: 70100 molecules\n",
      "Training time for 20 batches: 25.98292303085327 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 720: 0.01796206273138523\n",
      "Training MAE (for one batch) at step 720: 0.03344675898551941\n",
      "Seen so far: 72100 molecules\n",
      "Training time for 20 batches: 26.647881984710693 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 740: 0.015559502877295017\n",
      "Training MAE (for one batch) at step 740: 0.020292561501264572\n",
      "Seen so far: 74100 molecules\n",
      "Training time for 20 batches: 26.083364009857178 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 760: 0.014611071906983852\n",
      "Training MAE (for one batch) at step 760: 0.05529026314616203\n",
      "Seen so far: 76100 molecules\n",
      "Training time for 20 batches: 25.80952501296997 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 780: 0.011050430126488209\n",
      "Training MAE (for one batch) at step 780: 0.035717133432626724\n",
      "Seen so far: 78100 molecules\n",
      "Training time for 20 batches: 26.35466504096985 s\n",
      "Writing to tb\n",
      "Training time for epoch 1: 1057.3771736621857 s\n",
      "Starting validation for epoch 1\n",
      "Writing to tb\n",
      "Initial validation loss (for one batch): 0.002453671768307686\n",
      "Seen so far: 100 molecules\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Time taken for epoch 1: 1063.0271139144897 s\n",
      "\n",
      "Start of epoch 0\n",
      "8 16 0.01\n",
      "16\n",
      "Initial loss (for one batch): 116.98727416992188\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 7.65585994720459\n",
      "Training MAE (for one batch) at step 20: 0.9414545297622681\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 28.123494148254395 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 40: 25.145587921142578\n",
      "Training MAE (for one batch) at step 40: 0.41143232583999634\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 26.51909303665161 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 60: 5.468395233154297\n",
      "Training MAE (for one batch) at step 60: 0.8824816942214966\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 26.365211963653564 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 80: 19.65526580810547\n",
      "Training MAE (for one batch) at step 80: 1.2701455354690552\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 26.680227994918823 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 100: 5.743249893188477\n",
      "Training MAE (for one batch) at step 100: 0.31013450026512146\n",
      "Seen so far: 10100 molecules\n",
      "Training time for 20 batches: 26.88180685043335 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 120: 11.909626960754395\n",
      "Training MAE (for one batch) at step 120: 1.0729734897613525\n",
      "Seen so far: 12100 molecules\n",
      "Training time for 20 batches: 26.646995067596436 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 140: 2.7594447135925293\n",
      "Training MAE (for one batch) at step 140: 0.477039635181427\n",
      "Seen so far: 14100 molecules\n",
      "Training time for 20 batches: 26.261110067367554 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 160: 4.0685248374938965\n",
      "Training MAE (for one batch) at step 160: 0.5547977089881897\n",
      "Seen so far: 16100 molecules\n",
      "Training time for 20 batches: 25.89167308807373 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 180: 1.202054500579834\n",
      "Training MAE (for one batch) at step 180: 0.26825380325317383\n",
      "Seen so far: 18100 molecules\n",
      "Training time for 20 batches: 25.99120593070984 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 200: 0.5328985452651978\n",
      "Training MAE (for one batch) at step 200: 0.5921738147735596\n",
      "Seen so far: 20100 molecules\n",
      "Training time for 20 batches: 26.21435308456421 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 220: 20.354307174682617\n",
      "Training MAE (for one batch) at step 220: 1.0182377099990845\n",
      "Seen so far: 22100 molecules\n",
      "Training time for 20 batches: 25.981127738952637 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 240: 2.0156428813934326\n",
      "Training MAE (for one batch) at step 240: 0.3583245873451233\n",
      "Seen so far: 24100 molecules\n",
      "Training time for 20 batches: 25.993672132492065 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 260: 9.554529190063477\n",
      "Training MAE (for one batch) at step 260: 0.5901331901550293\n",
      "Seen so far: 26100 molecules\n",
      "Training time for 20 batches: 25.961472988128662 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 280: 4.703887939453125\n",
      "Training MAE (for one batch) at step 280: 0.3809249699115753\n",
      "Seen so far: 28100 molecules\n",
      "Training time for 20 batches: 25.976113080978394 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 300: 1.7033652067184448\n",
      "Training MAE (for one batch) at step 300: 0.9121686816215515\n",
      "Seen so far: 30100 molecules\n",
      "Training time for 20 batches: 26.026755809783936 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 320: 2.6786396503448486\n",
      "Training MAE (for one batch) at step 320: 0.3288511633872986\n",
      "Seen so far: 32100 molecules\n",
      "Training time for 20 batches: 26.158440351486206 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 340: 1.8308310508728027\n",
      "Training MAE (for one batch) at step 340: 0.4340396821498871\n",
      "Seen so far: 34100 molecules\n",
      "Training time for 20 batches: 26.21731185913086 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 360: 0.7085074186325073\n",
      "Training MAE (for one batch) at step 360: 0.3272649645805359\n",
      "Seen so far: 36100 molecules\n",
      "Training time for 20 batches: 26.25411295890808 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 380: 0.4320209324359894\n",
      "Training MAE (for one batch) at step 380: 0.4346093237400055\n",
      "Seen so far: 38100 molecules\n",
      "Training time for 20 batches: 26.260395050048828 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 400: 0.6345009207725525\n",
      "Training MAE (for one batch) at step 400: 0.30939027667045593\n",
      "Seen so far: 40100 molecules\n",
      "Training time for 20 batches: 26.334510803222656 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 420: 0.6048592925071716\n",
      "Training MAE (for one batch) at step 420: 0.4518694281578064\n",
      "Seen so far: 42100 molecules\n",
      "Training time for 20 batches: 26.57759428024292 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 440: 0.46248331665992737\n",
      "Training MAE (for one batch) at step 440: 0.28831326961517334\n",
      "Seen so far: 44100 molecules\n",
      "Training time for 20 batches: 26.210200786590576 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 460: 0.19860656559467316\n",
      "Training MAE (for one batch) at step 460: 0.34414538741111755\n",
      "Seen so far: 46100 molecules\n",
      "Training time for 20 batches: 26.211304187774658 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 480: 0.24792389571666718\n",
      "Training MAE (for one batch) at step 480: 0.19078555703163147\n",
      "Seen so far: 48100 molecules\n",
      "Training time for 20 batches: 26.390116930007935 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 500: 1.188590168952942\n",
      "Training MAE (for one batch) at step 500: 0.6189993619918823\n",
      "Seen so far: 50100 molecules\n",
      "Training time for 20 batches: 26.492037057876587 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 520: 1.3071799278259277\n",
      "Training MAE (for one batch) at step 520: 0.5291863083839417\n",
      "Seen so far: 52100 molecules\n",
      "Training time for 20 batches: 26.092243909835815 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 540: 1.8056626319885254\n",
      "Training MAE (for one batch) at step 540: 1.2432395219802856\n",
      "Seen so far: 54100 molecules\n",
      "Training time for 20 batches: 26.36259412765503 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 560: 0.532551646232605\n",
      "Training MAE (for one batch) at step 560: 0.3158455193042755\n",
      "Seen so far: 56100 molecules\n",
      "Training time for 20 batches: 26.453391790390015 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 580: 0.23333008587360382\n",
      "Training MAE (for one batch) at step 580: 0.2953398823738098\n",
      "Seen so far: 58100 molecules\n",
      "Training time for 20 batches: 26.32080626487732 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 600: 0.5542402267456055\n",
      "Training MAE (for one batch) at step 600: 0.3762720823287964\n",
      "Seen so far: 60100 molecules\n",
      "Training time for 20 batches: 26.7151837348938 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 620: 1.1494109630584717\n",
      "Training MAE (for one batch) at step 620: 0.8029810786247253\n",
      "Seen so far: 62100 molecules\n",
      "Training time for 20 batches: 27.00858497619629 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 640: 3.9357619285583496\n",
      "Training MAE (for one batch) at step 640: 0.40159156918525696\n",
      "Seen so far: 64100 molecules\n",
      "Training time for 20 batches: 26.86430525779724 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 660: 2.082245349884033\n",
      "Training MAE (for one batch) at step 660: 0.22744065523147583\n",
      "Seen so far: 66100 molecules\n",
      "Training time for 20 batches: 26.373474836349487 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 680: 2.94474720954895\n",
      "Training MAE (for one batch) at step 680: 1.3764355182647705\n",
      "Seen so far: 68100 molecules\n",
      "Training time for 20 batches: 26.441559076309204 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 700: 0.5499836802482605\n",
      "Training MAE (for one batch) at step 700: 0.22711308300495148\n",
      "Seen so far: 70100 molecules\n",
      "Training time for 20 batches: 26.585432052612305 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 720: 3.510834217071533\n",
      "Training MAE (for one batch) at step 720: 0.9481062293052673\n",
      "Seen so far: 72100 molecules\n",
      "Training time for 20 batches: 26.72820496559143 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 740: 0.7124198079109192\n",
      "Training MAE (for one batch) at step 740: 0.2038143426179886\n",
      "Seen so far: 74100 molecules\n",
      "Training time for 20 batches: 27.180155992507935 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 760: 0.6972458958625793\n",
      "Training MAE (for one batch) at step 760: 0.1785135567188263\n",
      "Seen so far: 76100 molecules\n",
      "Training time for 20 batches: 27.235258102416992 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 780: 0.1661043018102646\n",
      "Training MAE (for one batch) at step 780: 0.16002212464809418\n",
      "Seen so far: 78100 molecules\n",
      "Training time for 20 batches: 27.233267784118652 s\n",
      "Writing to tb\n",
      "Training time for epoch 1: 1057.3651540279388 s\n",
      "Starting validation for epoch 1\n",
      "Writing to tb\n",
      "Initial validation loss (for one batch): 0.09222133457660675\n",
      "Seen so far: 100 molecules\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Time taken for epoch 1: 1062.96484708786 s\n",
      "\n",
      "Start of epoch 0\n",
      "8 16 0.03\n",
      "16\n",
      "Initial loss (for one batch): 104.05816650390625\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 36.24055099487305\n",
      "Training MAE (for one batch) at step 20: 3.349886417388916\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 28.74191927909851 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 40: 43.84088134765625\n",
      "Training MAE (for one batch) at step 40: 4.418356895446777\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 27.338897943496704 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 60: 14.904500961303711\n",
      "Training MAE (for one batch) at step 60: 2.1164162158966064\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 27.36550807952881 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 80: 6.466291427612305\n",
      "Training MAE (for one batch) at step 80: 1.6479324102401733\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 27.378578901290894 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 100: 1.7281008958816528\n",
      "Training MAE (for one batch) at step 100: 0.6962676644325256\n",
      "Seen so far: 10100 molecules\n",
      "Training time for 20 batches: 26.469421863555908 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 120: 6.8302459716796875\n",
      "Training MAE (for one batch) at step 120: 1.7555640935897827\n",
      "Seen so far: 12100 molecules\n",
      "Training time for 20 batches: 26.38499116897583 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 140: 1.722280502319336\n",
      "Training MAE (for one batch) at step 140: 0.796617865562439\n",
      "Seen so far: 14100 molecules\n",
      "Training time for 20 batches: 27.354592084884644 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 160: 1.2082018852233887\n",
      "Training MAE (for one batch) at step 160: 0.5798239707946777\n",
      "Seen so far: 16100 molecules\n",
      "Training time for 20 batches: 27.23869276046753 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 180: 1.5118249654769897\n",
      "Training MAE (for one batch) at step 180: 0.9754714965820312\n",
      "Seen so far: 18100 molecules\n",
      "Training time for 20 batches: 27.277642965316772 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 200: 1.9141825437545776\n",
      "Training MAE (for one batch) at step 200: 0.8103998899459839\n",
      "Seen so far: 20100 molecules\n",
      "Training time for 20 batches: 26.86064910888672 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 220: 0.8780592083930969\n",
      "Training MAE (for one batch) at step 220: 0.34174713492393494\n",
      "Seen so far: 22100 molecules\n",
      "Training time for 20 batches: 27.16940188407898 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 240: 2.6401491165161133\n",
      "Training MAE (for one batch) at step 240: 0.550243616104126\n",
      "Seen so far: 24100 molecules\n",
      "Training time for 20 batches: 27.18651294708252 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 260: 1.5651109218597412\n",
      "Training MAE (for one batch) at step 260: 1.1501288414001465\n",
      "Seen so far: 26100 molecules\n",
      "Training time for 20 batches: 26.98873496055603 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 280: 1.0838313102722168\n",
      "Training MAE (for one batch) at step 280: 0.9487988352775574\n",
      "Seen so far: 28100 molecules\n",
      "Training time for 20 batches: 26.737521171569824 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 300: 0.5103818774223328\n",
      "Training MAE (for one batch) at step 300: 0.6057546734809875\n",
      "Seen so far: 30100 molecules\n",
      "Training time for 20 batches: 26.3734929561615 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 320: 0.5597144365310669\n",
      "Training MAE (for one batch) at step 320: 0.5608028173446655\n",
      "Seen so far: 32100 molecules\n",
      "Training time for 20 batches: 25.7189199924469 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 340: 0.34034445881843567\n",
      "Training MAE (for one batch) at step 340: 0.47194209694862366\n",
      "Seen so far: 34100 molecules\n",
      "Training time for 20 batches: 25.641762018203735 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 360: 0.3554733693599701\n",
      "Training MAE (for one batch) at step 360: 0.4929646849632263\n",
      "Seen so far: 36100 molecules\n",
      "Training time for 20 batches: 26.142008066177368 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 380: 0.269126296043396\n",
      "Training MAE (for one batch) at step 380: 0.44040223956108093\n",
      "Seen so far: 38100 molecules\n",
      "Training time for 20 batches: 25.760011911392212 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 400: 0.6667079925537109\n",
      "Training MAE (for one batch) at step 400: 0.5072962641716003\n",
      "Seen so far: 40100 molecules\n",
      "Training time for 20 batches: 25.49609684944153 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 420: 1.2559819221496582\n",
      "Training MAE (for one batch) at step 420: 0.9127640128135681\n",
      "Seen so far: 42100 molecules\n",
      "Training time for 20 batches: 25.82443618774414 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 440: 1.522237777709961\n",
      "Training MAE (for one batch) at step 440: 0.7809086441993713\n",
      "Seen so far: 44100 molecules\n",
      "Training time for 20 batches: 24.997172832489014 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 460: 1.8930199146270752\n",
      "Training MAE (for one batch) at step 460: 1.1702865362167358\n",
      "Seen so far: 46100 molecules\n",
      "Training time for 20 batches: 25.661096334457397 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 480: 0.5067757964134216\n",
      "Training MAE (for one batch) at step 480: 0.5546145439147949\n",
      "Seen so far: 48100 molecules\n",
      "Training time for 20 batches: 26.230896711349487 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 500: 1.0801029205322266\n",
      "Training MAE (for one batch) at step 500: 0.8556557893753052\n",
      "Seen so far: 50100 molecules\n",
      "Training time for 20 batches: 25.752291202545166 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 520: 1.576250672340393\n",
      "Training MAE (for one batch) at step 520: 1.0072993040084839\n",
      "Seen so far: 52100 molecules\n",
      "Training time for 20 batches: 25.39404797554016 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 540: 0.3471728563308716\n",
      "Training MAE (for one batch) at step 540: 0.427106112241745\n",
      "Seen so far: 54100 molecules\n",
      "Training time for 20 batches: 25.57618999481201 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 560: 1.2705166339874268\n",
      "Training MAE (for one batch) at step 560: 0.7512806057929993\n",
      "Seen so far: 56100 molecules\n",
      "Training time for 20 batches: 26.19514489173889 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 580: 56.184295654296875\n",
      "Training MAE (for one batch) at step 580: 2.1528587341308594\n",
      "Seen so far: 58100 molecules\n",
      "Training time for 20 batches: 26.21345615386963 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 600: 3.3589282035827637\n",
      "Training MAE (for one batch) at step 600: 0.7895551919937134\n",
      "Seen so far: 60100 molecules\n",
      "Training time for 20 batches: 26.13813090324402 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 620: 46.79323959350586\n",
      "Training MAE (for one batch) at step 620: 4.801342010498047\n",
      "Seen so far: 62100 molecules\n",
      "Training time for 20 batches: 26.10353994369507 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 640: 51.17543411254883\n",
      "Training MAE (for one batch) at step 640: 5.655980110168457\n",
      "Seen so far: 64100 molecules\n",
      "Training time for 20 batches: 25.796880960464478 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 660: 37.71001434326172\n",
      "Training MAE (for one batch) at step 660: 4.419959545135498\n",
      "Seen so far: 66100 molecules\n",
      "Training time for 20 batches: 25.99448800086975 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 680: 55.4009895324707\n",
      "Training MAE (for one batch) at step 680: 5.183713912963867\n",
      "Seen so far: 68100 molecules\n",
      "Training time for 20 batches: 26.10461115837097 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 700: 9.110132217407227\n",
      "Training MAE (for one batch) at step 700: 2.5800955295562744\n",
      "Seen so far: 70100 molecules\n",
      "Training time for 20 batches: 25.91744303703308 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 720: 2.445615291595459\n",
      "Training MAE (for one batch) at step 720: 1.468741536140442\n",
      "Seen so far: 72100 molecules\n",
      "Training time for 20 batches: 25.627211809158325 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 740: 10.637811660766602\n",
      "Training MAE (for one batch) at step 740: 0.40138211846351624\n",
      "Seen so far: 74100 molecules\n",
      "Training time for 20 batches: 25.99690318107605 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 760: 2.128319025039673\n",
      "Training MAE (for one batch) at step 760: 0.9817566275596619\n",
      "Seen so far: 76100 molecules\n",
      "Training time for 20 batches: 26.100724935531616 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 780: 0.5509852170944214\n",
      "Training MAE (for one batch) at step 780: 0.5251460671424866\n",
      "Seen so far: 78100 molecules\n",
      "Training time for 20 batches: 25.87250590324402 s\n",
      "Writing to tb\n",
      "Training time for epoch 1: 1051.7284841537476 s\n",
      "Starting validation for epoch 1\n",
      "Writing to tb\n",
      "Initial validation loss (for one batch): 1.4542348384857178\n",
      "Seen so far: 100 molecules\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Writing to tb\n",
      "Time taken for epoch 1: 1057.3315019607544 s\n",
      "\n",
      "Start of epoch 0\n",
      "8 4 0.0003\n",
      "4\n",
      "Initial loss (for one batch): 20.70542335510254\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 22.484643936157227\n",
      "Training MAE (for one batch) at step 20: 0.614319384098053\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 9.582988977432251 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 40: 21.388641357421875\n",
      "Training MAE (for one batch) at step 40: 0.6754962205886841\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 8.852344989776611 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 60: 27.101093292236328\n",
      "Training MAE (for one batch) at step 60: 0.5030845999717712\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 8.807059049606323 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 80: 12.115224838256836\n",
      "Training MAE (for one batch) at step 80: 0.4406487047672272\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 10.962867975234985 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 100: 10.837185859680176\n",
      "Training MAE (for one batch) at step 100: 0.4548150897026062\n",
      "Seen so far: 10100 molecules\n",
      "Training time for 20 batches: 12.068262100219727 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 120: 5.290268421173096\n",
      "Training MAE (for one batch) at step 120: 0.504543662071228\n",
      "Seen so far: 12100 molecules\n",
      "Training time for 20 batches: 11.70176100730896 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 140: 2.8872857093811035\n",
      "Training MAE (for one batch) at step 140: 0.29483309388160706\n",
      "Seen so far: 14100 molecules\n",
      "Training time for 20 batches: 11.683072805404663 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 160: 1.5973281860351562\n",
      "Training MAE (for one batch) at step 160: 0.2627021372318268\n",
      "Seen so far: 16100 molecules\n",
      "Training time for 20 batches: 11.551890134811401 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 180: 1.1542649269104004\n",
      "Training MAE (for one batch) at step 180: 0.26961275935173035\n",
      "Seen so far: 18100 molecules\n",
      "Training time for 20 batches: 11.40553593635559 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 200: 1.215898871421814\n",
      "Training MAE (for one batch) at step 200: 0.2260810136795044\n",
      "Seen so far: 20100 molecules\n",
      "Training time for 20 batches: 11.09176516532898 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 220: 1.148141622543335\n",
      "Training MAE (for one batch) at step 220: 0.16958686709403992\n",
      "Seen so far: 22100 molecules\n",
      "Training time for 20 batches: 11.439758777618408 s\n",
      "Writing to tb\n",
      "Wrong shape! (299, 1)\n",
      "Raised an error\n",
      "\n",
      "Start of epoch 0\n",
      "8 4 0.0003\n",
      "4\n",
      "Initial loss (for one batch): 16.185123443603516\n",
      "Seen so far: 100 molecules\n",
      "Wrong shape! (299, 1)\n",
      "Raised an error\n",
      "\n",
      "Start of epoch 0\n",
      "8 4 0.0003\n",
      "4\n",
      "Initial loss (for one batch): 14.767644882202148\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 6.515000820159912\n",
      "Training MAE (for one batch) at step 20: 0.3989544212818146\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 12.002371788024902 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 40: 10.939314842224121\n",
      "Training MAE (for one batch) at step 40: 0.1939287781715393\n",
      "Seen so far: 4100 molecules\n",
      "Training time for 20 batches: 12.002867221832275 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 60: 5.38632869720459\n",
      "Training MAE (for one batch) at step 60: 0.18787558376789093\n",
      "Seen so far: 6100 molecules\n",
      "Training time for 20 batches: 11.522020816802979 s\n",
      "Writing to tb\n",
      "Training loss (for one batch) at step 80: 4.336162090301514\n",
      "Training MAE (for one batch) at step 80: 0.16621749103069305\n",
      "Seen so far: 8100 molecules\n",
      "Training time for 20 batches: 11.620249032974243 s\n",
      "Writing to tb\n",
      "Wrong shape! (299, 1)\n",
      "Raised an error\n",
      "\n",
      "Start of epoch 0\n",
      "8 4 0.0003\n",
      "4\n",
      "Initial loss (for one batch): 13.95382308959961\n",
      "Seen so far: 100 molecules\n",
      "Wrong shape! (299, 1)\n",
      "Raised an error\n",
      "\n",
      "Start of epoch 0\n",
      "8 4 0.0003\n",
      "4\n",
      "Initial loss (for one batch): 15.846155166625977\n",
      "Seen so far: 100 molecules\n",
      "Training loss (for one batch) at step 20: 19.358705520629883\n",
      "Training MAE (for one batch) at step 20: 0.5039013028144836\n",
      "Seen so far: 2100 molecules\n",
      "Training time for 20 batches: 12.592489957809448 s\n",
      "Writing to tb\n",
      "Wrong shape! (299, 1)\n",
      "Raised an error\n",
      "\n",
      "Start of epoch 0\n",
      "8 4 0.0003\n",
      "4\n",
      "Initial loss (for one batch): 18.747081756591797\n",
      "Seen so far: 100 molecules\n",
      "Wrong shape! (299, 1)\n",
      "Raised an error\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:5030\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5026'>5027</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5027'>5028</a>\u001b[0m   \u001b[39m# TODO(apassos) find a less bad way of detecting resource variables\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5028'>5029</a>\u001b[0m   \u001b[39m# without introducing a circular dependency.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5029'>5030</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m params\u001b[39m.\u001b[39;49msparse_read(indices, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5030'>5031</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:401\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=395'>396</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=396'>397</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=397'>398</a>\u001b[0m \u001b[39m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=398'>399</a>\u001b[0m \u001b[39m    import tensorflow.python.ops.numpy_ops.np_config\u001b[39m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=399'>400</a>\u001b[0m \u001b[39m    np_config.enable_numpy_behavior()\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n\u001b[0;32m--> <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=400'>401</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'sparse_read'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000034?line=63'>64</a>\u001b[0m                 hparams \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000034?line=64'>65</a>\u001b[0m                     HP_NUM_NODES: num_nodes,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000034?line=65'>66</a>\u001b[0m                     HP_DEPTH: depth,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000034?line=66'>67</a>\u001b[0m                     HP_LR: lr\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000034?line=67'>68</a>\u001b[0m                 }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000034?line=69'>70</a>\u001b[0m                 params\u001b[39m=\u001b[39m{\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000034?line=70'>71</a>\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m'\u001b[39m: {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000034?line=71'>72</a>\u001b[0m                         \u001b[39m'\u001b[39m\u001b[39mclass_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mAdam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000034?line=94'>95</a>\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39muse_force\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000034?line=95'>96</a>\u001b[0m                 } \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000034?line=97'>98</a>\u001b[0m                 get_and_train_network(hparams)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000034?line=99'>100</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000034?line=100'>101</a>\u001b[0m \u001b[39mexcept\u001b[39;00m tf\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mInvalidArgumentError:\n",
      "\u001b[1;32m/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb Cell 28'\u001b[0m in \u001b[0;36mget_and_train_network\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000033?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_and_train_network\u001b[39m(hparams):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000033?line=2'>3</a>\u001b[0m     network \u001b[39m=\u001b[39m get_network(params[\u001b[39m'\u001b[39m\u001b[39mnetwork\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000033?line=3'>4</a>\u001b[0m     preprocess_traintest_sets(train_set, test_set, network\u001b[39m=\u001b[39;49mnetwork)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000033?line=5'>6</a>\u001b[0m     \u001b[39m#Set up tensorboard directory and writer\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/Learning_a_LJ_potential.ipynb#ch0000033?line=6'>7</a>\u001b[0m     nodes \u001b[39m=\u001b[39m hparams[HP_NUM_NODES]\n",
      "File \u001b[0;32m~/PiNN/docs/notebooks/network_fns.py:17\u001b[0m, in \u001b[0;36mpreprocess_traintest_sets\u001b[0;34m(train_set, test_set, network)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/miguelnavaharris/PiNN/docs/notebooks/network_fns.py?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_traintest_sets\u001b[39m(train_set, test_set, network\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     <a href='file:///Users/miguelnavaharris/PiNN/docs/notebooks/network_fns.py?line=15'>16</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_set:\n\u001b[0;32m---> <a href='file:///Users/miguelnavaharris/PiNN/docs/notebooks/network_fns.py?line=16'>17</a>\u001b[0m         batch \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39;49mpreprocess(batch)\n\u001b[1;32m     <a href='file:///Users/miguelnavaharris/PiNN/docs/notebooks/network_fns.py?line=17'>18</a>\u001b[0m         connect_dist_grad(batch)\n\u001b[1;32m     <a href='file:///Users/miguelnavaharris/PiNN/docs/notebooks/network_fns.py?line=18'>19</a>\u001b[0m         \u001b[39m# print(\"Train set: Preprocessed ind_1 shape: \", batch['ind_1'].shape)\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/miguelnavaharris/PiNN/docs/notebooks/network_fns.py?line=19'>20</a>\u001b[0m         \u001b[39m# print(\"Train set: Preprocessed ind_2 shape: \", batch['ind_2'].shape)\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/miguelnavaharris/PiNN/docs/notebooks/network_fns.py?line=20'>21</a>\u001b[0m         \u001b[39m# print(\"Train set: Preprocessed elems shape: \", batch['elems'].shape)\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/miguelnavaharris/PiNN/docs/notebooks/network_fns.py?line=21'>22</a>\u001b[0m         \u001b[39m# print(\"Train set: Preprocessed coord shape: \", batch['coord'].shape)\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/miguelnavaharris/PiNN/docs/notebooks/network_fns.py?line=22'>23</a>\u001b[0m         \u001b[39m# print(\"Train set: Preprocessed e_data shape: \", batch['e_data'].shape)\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/miguelnavaharris/PiNN/docs/notebooks/network_fns.py?line=23'>24</a>\u001b[0m         \u001b[39m# print(\"Train set: Preprocessed f_data shape: \", batch['f_data'].shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:1030\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py?line=1025'>1026</a>\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py?line=1027'>1028</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py?line=1028'>1029</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py?line=1029'>1030</a>\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py?line=1031'>1032</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py?line=1032'>1033</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/PiNN/pinn/networks/pinet.py:187\u001b[0m, in \u001b[0;36mPreprocessLayer.call\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/networks/pinet.py?line=180'>181</a>\u001b[0m \u001b[39m# Check input tensors before processing\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/networks/pinet.py?line=181'>182</a>\u001b[0m \u001b[39m# print(\"=== Input tensors ===\")\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/networks/pinet.py?line=182'>183</a>\u001b[0m \u001b[39m# for key, value in tensors.items():\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/networks/pinet.py?line=183'>184</a>\u001b[0m \u001b[39m#     print(f\"{key}: {value.shape}\")\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/networks/pinet.py?line=185'>186</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mind_2\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m tensors:\n\u001b[0;32m--> <a href='file:///Users/miguelnavaharris/PiNN/pinn/networks/pinet.py?line=186'>187</a>\u001b[0m     tensors\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnl_layer(tensors))\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/networks/pinet.py?line=187'>188</a>\u001b[0m     tensors[\u001b[39m'\u001b[39m\u001b[39mprop\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/networks/pinet.py?line=188'>189</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed(tensors[\u001b[39m'\u001b[39m\u001b[39melems\u001b[39m\u001b[39m'\u001b[39m]), tensors[\u001b[39m'\u001b[39m\u001b[39mcoord\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/networks/pinet.py?line=190'>191</a>\u001b[0m \u001b[39m# Assert and print output tensors after processing\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/networks/pinet.py?line=191'>192</a>\u001b[0m \u001b[39m# print(\"=== Output tensors ===\")\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/networks/pinet.py?line=192'>193</a>\u001b[0m \u001b[39m# for key in ['ind_1', 'ind_2', 'prop', 'dist']:\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/networks/pinet.py?line=193'>194</a>\u001b[0m \u001b[39m#     assert key in tensors, f\"{key} not found in tensors\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/networks/pinet.py?line=194'>195</a>\u001b[0m \u001b[39m#     print(f\"{key} shape: {tensors[key].shape}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:1030\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py?line=1025'>1026</a>\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py?line=1027'>1028</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py?line=1028'>1029</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py?line=1029'>1030</a>\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py?line=1031'>1032</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py?line=1032'>1033</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/PiNN/pinn/layers.py:127\u001b[0m, in \u001b[0;36mCellListNL.call\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/layers.py?line=123'>124</a>\u001b[0m pair_ic_alst \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mgather(cell_alst, pair_ic_c)\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/layers.py?line=125'>126</a>\u001b[0m pair_ij \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(tf\u001b[39m.\u001b[39mwhere(pair_ic_alst), tf\u001b[39m.\u001b[39mint32)\n\u001b[0;32m--> <a href='file:///Users/miguelnavaharris/PiNN/pinn/layers.py?line=126'>127</a>\u001b[0m pair_ij_i \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mgather(pair_ic_i, pair_ij[:, \u001b[39m0\u001b[39;49m])\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/layers.py?line=127'>128</a>\u001b[0m pair_ij_j \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mgather_nd(pair_ic_alst, pair_ij) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/PiNN/pinn/layers.py?line=129'>130</a>\u001b[0m diff \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mgather(atom_apos, pair_ij_j) \u001b[39m-\u001b[39m tf\u001b[39m.\u001b[39mgather(atom_apos, pair_ij_i)\n",
      "File \u001b[0;32m~/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py?line=203'>204</a>\u001b[0m \u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py?line=204'>205</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py?line=205'>206</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py?line=206'>207</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py?line=207'>208</a>\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py?line=208'>209</a>\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py?line=209'>210</a>\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:5043\u001b[0m, in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, validate_indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5034'>5035</a>\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgather\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5035'>5036</a>\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5036'>5037</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgather_v2\u001b[39m(params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5040'>5041</a>\u001b[0m               batch_dims\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5041'>5042</a>\u001b[0m               name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5042'>5043</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m gather(\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5043'>5044</a>\u001b[0m       params,\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5044'>5045</a>\u001b[0m       indices,\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5045'>5046</a>\u001b[0m       validate_indices\u001b[39m=\u001b[39;49mvalidate_indices,\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5046'>5047</a>\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5047'>5048</a>\u001b[0m       axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5048'>5049</a>\u001b[0m       batch_dims\u001b[39m=\u001b[39;49mbatch_dims)\n",
      "File \u001b[0;32m~/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:535\u001b[0m, in \u001b[0;36mdeprecated_args.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py?line=526'>527</a>\u001b[0m         _PRINTED_WARNING[(func, arg_name)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py?line=527'>528</a>\u001b[0m       logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py?line=528'>529</a>\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is deprecated and will \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py?line=529'>530</a>\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mbe removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py?line=532'>533</a>\u001b[0m           \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date),\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py?line=533'>534</a>\u001b[0m           instructions)\n\u001b[0;32m--> <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py?line=534'>535</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py?line=203'>204</a>\u001b[0m \u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py?line=204'>205</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py?line=205'>206</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py?line=206'>207</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py?line=207'>208</a>\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py?line=208'>209</a>\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py?line=209'>210</a>\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:5032\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5029'>5030</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m params\u001b[39m.\u001b[39msparse_read(indices, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5030'>5031</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py?line=5031'>5032</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49mgather_v2(params, indices, axis, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py:3803\u001b[0m, in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py?line=3800'>3801</a>\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py?line=3801'>3802</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py?line=3802'>3803</a>\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py?line=3803'>3804</a>\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mGatherV2\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, params, indices, axis, \u001b[39m\"\u001b[39;49m\u001b[39mbatch_dims\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py?line=3804'>3805</a>\u001b[0m       batch_dims)\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py?line=3805'>3806</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py?line=3806'>3807</a>\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "HP_NUM_NODES = hp.HParam(\"num nodes\", hp.Discrete([8, 16, 32]))\n",
    "HP_DEPTH = hp.HParam(\"depth\", hp.Discrete([4, 8, 16]))\n",
    "HP_LR = hp.HParam(\"initial learning rate\", hp.Discrete([0.0003, 0.01, 0.03]))\n",
    "\n",
    "\n",
    "error_count = 0\n",
    "while True:\n",
    "    try:\n",
    "\n",
    "\n",
    "        def three_body_sample(atoms, a, r):\n",
    "            x = a * np.pi / 180\n",
    "            pos = [[0, 0, 0],\n",
    "                [0, 2, 0],\n",
    "                [0, r*np.cos(x), r*np.sin(x)]]\n",
    "            atoms.set_positions(pos)\n",
    "            return atoms\n",
    "\n",
    "        atoms = Atoms('H3', calculator=LennardJones())\n",
    "\n",
    "        na, nr = 50, 50\n",
    "        arange = np.linspace(30,180,na)\n",
    "        rrange = np.linspace(1,3,nr)\n",
    "\n",
    "        # Truth\n",
    "        agrid, rgrid = np.meshgrid(arange, rrange)\n",
    "        egrid = np.zeros([na, nr])\n",
    "        for i in range(na):\n",
    "            for j in range(nr):\n",
    "                atoms = three_body_sample(atoms, arange[i], rrange[j])\n",
    "                egrid[i,j] = atoms.get_potential_energy()\n",
    "        \n",
    "        # Samples\n",
    "        nsample = 100\n",
    "        asample, rsample = [], []\n",
    "        distsample = []\n",
    "        data = {'e_data':[], 'f_data':[], 'elems':[], 'coord':[]}\n",
    "        for i in range(nsample):\n",
    "            a, r = np.random.choice(arange), np.random.choice(rrange)\n",
    "            atoms = three_body_sample(atoms, a, r)\n",
    "            dist = atoms.get_all_distances()\n",
    "            dist = dist[np.nonzero(dist)]\n",
    "            data['e_data'].append(atoms.get_potential_energy())\n",
    "            data['f_data'].append(atoms.get_forces())\n",
    "            data['coord'].append(atoms.get_positions())\n",
    "            data['elems'].append(atoms.numbers)\n",
    "            asample.append(a)\n",
    "            rsample.append(r)\n",
    "            distsample.append(dist)\n",
    "\n",
    "        data = {k:np.array(v) for k,v in data.items()}\n",
    "        dataset = load_numpy(data, splits={'train':8, 'test':2})\n",
    "        dress, error = get_atomic_dress(dataset['train'], [1])\n",
    "\n",
    "        batch_size = 100\n",
    "        train_repeats = 1000\n",
    "        train_set = dataset['train'].shuffle(100).repeat(train_repeats).apply(sparse_batch(batch_size))\n",
    "        test_set = dataset['test'].repeat(100).apply(sparse_batch(100))\n",
    "\n",
    "        \n",
    "        for num_nodes in HP_NUM_NODES.domain.values:\n",
    "            for depth in HP_DEPTH.domain.values:\n",
    "                for lr in HP_LR.domain.values:\n",
    "                    hparams = {\n",
    "                        HP_NUM_NODES: num_nodes,\n",
    "                        HP_DEPTH: depth,\n",
    "                        HP_LR: lr\n",
    "                    }\n",
    "                    \n",
    "                    params={\n",
    "                        'optimizer': {\n",
    "                            'class_name': 'Adam',\n",
    "                            'config': {\n",
    "                                'learning_rate': {\n",
    "                                    'class_name': 'ExponentialDecay',\n",
    "                                    'config': {\n",
    "                                        'initial_learning_rate': lr,\n",
    "                                        'decay_steps': 10000, \n",
    "                                        'decay_rate': 0.994}}, \n",
    "                                        'clipnorm': 0.01}},\n",
    "                        'network': {\n",
    "                            'name': 'PiNet',\n",
    "                            'params': {\n",
    "                                'ii_nodes':[num_nodes,num_nodes],\n",
    "                                'pi_nodes':[num_nodes,num_nodes],\n",
    "                                'pp_nodes':[num_nodes,num_nodes],\n",
    "                                'out_nodes':[num_nodes,num_nodes],\n",
    "                                'depth': depth,\n",
    "                                'rc': 3.0,\n",
    "                                'atom_types':[1]}},\n",
    "\n",
    "                        'e_dress': dress,\n",
    "                        'e_scale': 2,\n",
    "                        'e_unit': 1.0,\n",
    "                        'use_force': True\n",
    "                    } \n",
    "            \n",
    "                    get_and_train_network(hparams)\n",
    "\n",
    "            break\n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        print('Raised an error')\n",
    "        error_count += 1\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4025a0c18342a57b4a17c482f921a5b0f0c41971fda061095662d1f6a4a25c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('pinn2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
