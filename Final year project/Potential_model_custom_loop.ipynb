{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The potential model, custom loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from pinn.io import load_qm9\n",
    "from docs.notebooks.network_fns import get_traintest_sets, preprocess_traintest_sets\n",
    "from pinn.layers import PolynomialBasis, GaussianBasis, ANNOutput\n",
    "from pinn.networks.pinet import OutLayer, GCBlock, ResUpdate, PreprocessLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 18:22:22.270766: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-26 18:22:22.270846: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "filelist = glob('/Users/miguelnavaharris/Project/QM9/*.xyz')\n",
    "dataset = load_qm9(filelist, splits={'train':8, 'test':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'optimizer': {\n",
    "        'class_name': 'Adam',\n",
    "        'config': {\n",
    "            'learning_rate': {\n",
    "                'class_name': 'ExponentialDecay',\n",
    "                'config': {\n",
    "                    'initial_learning_rate': 0.0003,\n",
    "                    'decay_steps': 10000, \n",
    "                    'decay_rate': 0.994}}, \n",
    "                    'clipnorm': 0.01}}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiNet(tf.keras.Model):\n",
    "    \"\"\"Keras model for the PiNet neural network\n",
    "\n",
    "    Args:\n",
    "        tensors: input data (nested tensor from dataset).\n",
    "        atom_types (list): elements for the one-hot embedding.\n",
    "        pp_nodes (list): number of nodes for pp layer.\n",
    "        pi_nodes (list): number of nodes for pi layer.\n",
    "        ii_nodes (list): number of nodes for ii layer.\n",
    "        en_nodes (list): number of nodes for en layer.\n",
    "        depth (int): number of interaction blocks.\n",
    "        rc (float): cutoff radius.\n",
    "        basis_type (string): type of basis function to use,\n",
    "            can be \"polynomial\" or \"gaussian\".\n",
    "        n_basis (int): number of basis functions to use.\n",
    "        gamma (float or array): width of gaussian function for gaussian basis.\n",
    "        center (float or array): center of gaussian function for gaussian basis.\n",
    "        cutoff_type (string): cutoff function to use with the basis.\n",
    "        act (string): activation function to use.\n",
    "        preprocess (bool): whether to return the preprocessed tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, atom_types=[1, 6, 7, 8, 9],  rc=4.0, cutoff_type='f1',\n",
    "                 basis_type='polynomial', n_basis=4, gamma=3.0, center=None,\n",
    "                 pp_nodes=[16, 16], pi_nodes=[16, 16], ii_nodes=[16, 16],\n",
    "                 out_nodes=[16, 16], out_units=1, out_pool=False,\n",
    "                 act='tanh', depth=4):\n",
    "\n",
    "        super(PiNet, self).__init__()\n",
    "\n",
    "        self.depth = depth\n",
    "        self.preprocess = PreprocessLayer(atom_types, rc)\n",
    "        self.activation = act\n",
    "\n",
    "        if basis_type == 'polynomial':\n",
    "            self.basis_fn = PolynomialBasis(cutoff_type, rc, n_basis)\n",
    "        elif basis_type == 'gaussian':\n",
    "            self.basis_fn = GaussianBasis(cutoff_type, rc, n_basis, gamma, center)\n",
    "\n",
    "        self.res_update = [ResUpdate() for i in range(depth)]\n",
    "        self.gc_blocks = [GCBlock([], pi_nodes, ii_nodes, activation=act)]\n",
    "        self.gc_blocks += [GCBlock(pp_nodes, pi_nodes, ii_nodes, activation=act)\n",
    "                           for i in range(depth-1)]\n",
    "        self.out_layers = [OutLayer(out_nodes, out_units) for i in range(depth)]\n",
    "        self.ann_output =  ANNOutput(out_pool)\n",
    "    \n",
    "    def call(self, tensors):\n",
    "        tensors = self.preprocess(tensors)\n",
    "        basis = self.basis_fn(tensors['dist'])[:, None, :]\n",
    "\n",
    "        output = 0.0\n",
    "        for i in range(self.depth):\n",
    "            prop = self.gc_blocks[i]([tensors['ind_2'], tensors['prop'], basis])\n",
    "            output = self.out_layers[i]([tensors['ind_1'], prop, output])\n",
    "            tensors['prop'] = self.res_update[i]([tensors['prop'], prop])\n",
    "\n",
    "        output = self.ann_output([tensors['ind_1'], output])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 18:22:28.666551: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-03-26 18:22:28.666776: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "network = PiNet()\n",
    "train_set, test_set, batch_size = get_traintest_sets(dataset=dataset, buffer_size=20000, batch_size=256)\n",
    "preprocess_traintest_sets(train_set, test_set, network=network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_err_metric = tf.keras.metrics.MeanAbsoluteError()\n",
    "val_err_metric = tf.keras.metrics.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 200119.703125\n",
      "Seen so far: 256 molecules\n",
      "Training loss (for one batch) at step 100: 1675.4761962890625\n",
      "Seen so far: 25856 molecules\n",
      "Training loss (for one batch) at step 200: 959.65966796875\n",
      "Seen so far: 51456 molecules\n",
      "Training loss (for one batch) at step 300: 694.817626953125\n",
      "Seen so far: 77056 molecules\n",
      "Training loss (for one batch) at step 400: 599.907470703125\n",
      "Seen so far: 102656 molecules\n",
      "Training err over epoch: 23.2858943939209\n",
      "Validation err: 23.117355346679688\n",
      "Time taken: 513.5562260150909\n"
     ]
    }
   ],
   "source": [
    "#Instantiate an optimizer\n",
    "import time\n",
    "from pinn.optimizers import get\n",
    "optimizer = get(params['optimizer'])\n",
    "loss_fn = tf.keras.losses.mse\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nStart of epoch {epoch}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, batch in enumerate(train_set):\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            pred = network(batch, training=True)  # Logits for this minibatch\n",
    "\n",
    "            ind = batch['ind_1']\n",
    "            nbatch = tf.reduce_max(ind)+1\n",
    "            pred = tf.math.unsorted_segment_sum(pred, ind[:, 0], nbatch)\n",
    "\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(batch['e_data'], pred)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, network.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, network.trainable_weights))\n",
    "\n",
    "        # #Update the training metric\n",
    "        # train_err_metric.update_state(batch['e_data'], pred)\n",
    "\n",
    "        # Log every 100 batches.\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Training loss (for one batch) at step {step}: {float(loss_value)}\"\n",
    "            )\n",
    "            print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "    \n",
    "    #Update the training metric now that the network has been trained\n",
    "    for batch in train_set:\n",
    "        pred = network(batch, training=False)  # Logits for this minibatch\n",
    "\n",
    "        ind = batch['ind_1']\n",
    "        nbatch = tf.reduce_max(ind)+1\n",
    "        pred = tf.math.unsorted_segment_sum(pred, ind[:, 0], nbatch)\n",
    "        train_err_metric.update_state(batch['e_data'], pred)\n",
    "    \n",
    "    #Display metrics at the end of each epoch\n",
    "    train_err = train_err_metric.result()\n",
    "    print(f\"Training err over epoch: {float(train_err)}\")\n",
    "\n",
    "    #Reset training metrics at the end of each epoch\n",
    "    train_err_metric.reset_states()\n",
    "\n",
    "    #Run a validation loop at the end of each epoch\n",
    "    for batch in test_set:\n",
    "        val_pred = network(batch, training=False)\n",
    "        ind = batch['ind_1']\n",
    "        nbatch = tf.reduce_max(ind)+1\n",
    "        val_pred = tf.math.unsorted_segment_sum(val_pred, ind[:, 0], nbatch)\n",
    "\n",
    "        #Update val metrics\n",
    "        val_err_metric.update_state(batch['e_data'], val_pred)\n",
    "        \n",
    "    val_err = val_err_metric.result()\n",
    "    val_err_metric.reset_states()\n",
    "    print(f\"Validation err: {float(val_err)}\")\n",
    "    print(f\"Time taken: {(time.time() - start_time)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4025a0c18342a57b4a17c482f921a5b0f0c41971fda061095662d1f6a4a25c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('pinn2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
