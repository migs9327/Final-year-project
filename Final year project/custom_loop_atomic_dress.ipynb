{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from pinn import get_network\n",
    "from pinn.utils import atomic_dress, get_atomic_dress, connect_dist_grad\n",
    "from ase.collections import g2\n",
    "from pinn.io import sparse_batch, load_qm9\n",
    "from docs.notebooks.network_fns import preprocess_traintest_sets, train_and_evaluate_network, _generator, predict_energy\n",
    "import matplotlib.pyplot as plt\n",
    "from ase import Atoms\n",
    "from pinn.optimizers import get\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices()\n",
    "tf.config.set_visible_devices(physical_devices[0], 'CPU')\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 23:22:33.789493: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-07-20 23:22:33.789598: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "filelist = glob('/Users/miguelnavaharris/Project/QM9/*.xyz')\n",
    "dataset = load_qm9(filelist, splits={'train':8, 'test':2})\n",
    "dress, error = get_atomic_dress(dataset['train'], [1,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traintest_sets(batch_size):\n",
    "    train_set = dataset['train'].shuffle(1000).apply(sparse_batch(batch_size))\n",
    "    test_set = dataset['test'].apply(sparse_batch(batch_size))\n",
    "    return (train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'optimizer': {'class_name': 'Adam', 'config': {'learning_rate': {'class_name': 'ExponentialDecay', 'config': {'initial_learning_rate': 0.0003, 'decay_steps': 10000, 'decay_rate': 0.994}}, 'clipnorm': 0.01}}, 'network': {'name': 'PiNet', 'params': {'depth': 4, 'rc': 4.0, 'atom_types': [1, 6, 7, 8, 9]}}, 'model': {'name': 'potential_model', 'params': {'learning_rate': 0.001, 'e_scale': 1, 'e_dress': dress}},'use_force':{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = get_network(params['network'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_traintest_sets(train_set, test_set):\n",
    "    for batch in train_set:\n",
    "        batch = network.preprocess(batch)\n",
    "        connect_dist_grad(batch)\n",
    "    for batch in test_set:\n",
    "        batch = network.preprocess(batch)\n",
    "        connect_dist_grad(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_set, test_set = get_traintest_sets(batch_size)\n",
    "preprocess_traintest_sets(train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_network(network=network, params=params, train_set=train_set, test_set=test_set, batch_size=256, epochs=1):\n",
    "\n",
    "\n",
    "    # Instantiate an optimizer\n",
    "    optimizer = get(params['optimizer'])\n",
    "    # Define a loss function\n",
    "    loss_fn = tf.keras.losses.mse\n",
    "    # Define metrics\n",
    "    train_loss_metric = tf.keras.metrics.MeanSquaredError()\n",
    "    val_loss_metric = tf.keras.metrics.MeanSquaredError()\n",
    "    train_err_metric = tf.keras.metrics.RootMeanSquaredError()\n",
    "    val_err_metric = tf.keras.metrics.RootMeanSquaredError()\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time_epoch = time.time()\n",
    "        hund_step_times = []\n",
    "        val_errors = []\n",
    "\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, batch in enumerate(train_set):\n",
    "\n",
    "            # Open a GradientTape to record the operations run\n",
    "            # during the forward pass, which enables auto-differentiation.\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                # Run the forward pass of the layer.\n",
    "                # The operations that the layer applies\n",
    "                # to its inputs are going to be recorded\n",
    "                # on the GradientTape.\n",
    "\n",
    "                pred = network(batch, training=True)  # Logits for this minibatch\n",
    "\n",
    "                ind = batch['ind_1']\n",
    "                nbatch = tf.reduce_max(ind)+1\n",
    "                pred = tf.math.unsorted_segment_sum(pred, ind[:, 0], nbatch)\n",
    "                e_data = batch['e_data']\n",
    "\n",
    "                if params['model']['params']['e_dress']:\n",
    "                    e_data -= atomic_dress(batch, params['model']['params']['e_dress'], dtype=pred.dtype)\n",
    "                    e_data *= params['model']['params']['e_scale']\n",
    "\n",
    "\n",
    "                # Compute the loss value for this minibatch.\n",
    "                loss_value = loss_fn(e_data, pred)\n",
    "\n",
    "            # Use the gradient tape to automatically retrieve\n",
    "            # the gradients of the trainable variables with respect to the loss.\n",
    "            grads = tape.gradient(loss_value, network.trainable_weights)\n",
    "\n",
    "            # Run one step of gradient descent by updating\n",
    "            # the value of the variables to minimize the loss.\n",
    "            optimizer.apply_gradients(zip(grads, network.trainable_weights))\n",
    "\n",
    "            # Update the loss and error metrics\n",
    "            train_loss_metric.update_state(e_data, pred)\n",
    "            train_err_metric.update_state(e_data, pred)\n",
    "\n",
    "\n",
    "\n",
    "            # Log every 100 batches.\n",
    "            if step == 0:\n",
    "                print(f\"Initial loss (for one batch): {float(loss_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "\n",
    "\n",
    "\n",
    "            elif step % 100 == 0:\n",
    "                print(f\"Training loss (for one batch) at step {step}: {float(loss_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "                hund_step_times += [(time.time() - start_time_epoch)]\n",
    "                print(f'Training time for 100 batches: {((hund_step_times[-1] - hund_step_times[-2]) if len(hund_step_times) > 1 else hund_step_times[-1])} s')\n",
    "\n",
    "\n",
    "\n",
    "        print(f'Training time for epoch {epoch + 1}: {(time.time() - start_time_epoch)} s')\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # Run a validation loop at the end of each epoch\n",
    "        print(f'Starting validation for epoch {(epoch + 1)}')\n",
    "\n",
    "        for step, batch in enumerate(test_set):\n",
    "            \n",
    "            val_pred = network(batch, training=False)\n",
    "            ind = batch['ind_1']\n",
    "            nbatch = tf.reduce_max(ind)+1\n",
    "            val_pred = tf.math.unsorted_segment_sum(val_pred, ind[:, 0], nbatch)\n",
    "            e_data = batch['e_data']\n",
    "\n",
    "            if params['model']['params']['e_dress']:\n",
    "                e_data -= atomic_dress(batch, params['model']['params']['e_dress'], dtype=pred.dtype)\n",
    "                e_data *= params['model']['params']['e_scale']\n",
    "\n",
    "\n",
    "            # Update val metrics\n",
    "            val_loss_metric.update_state(e_data, val_pred)\n",
    "            val_err_metric.update_state(e_data, val_pred)\n",
    "\n",
    "\n",
    "        print(f\"Time taken for epoch {epoch + 1}: {(time.time() - start_time_epoch)} s\")\n",
    "\n",
    "        # Display metrics at the end of each epoch\n",
    "        train_err = train_err_metric.result()\n",
    "        print(f\"Training err over epoch {(epoch + 1)}: {float(train_err)}\")\n",
    "        val_err = val_err_metric.result()\n",
    "        val_errors.append(float(val_err))\n",
    "        print(f\"Validation err for epoch {(epoch + 1)}: {float(val_err)}\")\n",
    "        # Reset training metrics at the end of each epoch        \n",
    "        train_err_metric.reset_states()\n",
    "        val_err_metric.reset_states()\n",
    "\n",
    "        if epoch > 3 and val_errors[-4] <= min(val_errors[-3:]):\n",
    "                    return 'Stopped.'\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Initial loss (for one batch): 9741.57421875\n",
      "Seen so far: 128 molecules\n",
      "Training loss (for one batch) at step 100: 1.584776759147644\n",
      "Seen so far: 12928 molecules\n",
      "Training time for 100 batches: 11.008339166641235 s\n",
      "Training loss (for one batch) at step 200: 0.5289019346237183\n",
      "Seen so far: 25728 molecules\n",
      "Training time for 100 batches: 10.860275983810425 s\n",
      "Training loss (for one batch) at step 300: 0.31758439540863037\n",
      "Seen so far: 38528 molecules\n",
      "Training time for 100 batches: 11.415031909942627 s\n",
      "Training loss (for one batch) at step 400: 1.383690595626831\n",
      "Seen so far: 51328 molecules\n",
      "Training time for 100 batches: 11.730087041854858 s\n",
      "Training loss (for one batch) at step 500: 0.25488150119781494\n",
      "Seen so far: 64128 molecules\n",
      "Training time for 100 batches: 11.979120969772339 s\n",
      "Training loss (for one batch) at step 600: 0.3045615255832672\n",
      "Seen so far: 76928 molecules\n",
      "Training time for 100 batches: 11.373357057571411 s\n",
      "Training loss (for one batch) at step 700: 0.21129250526428223\n",
      "Seen so far: 89728 molecules\n",
      "Training time for 100 batches: 11.710609912872314 s\n",
      "Training loss (for one batch) at step 800: 0.22992916405200958\n",
      "Seen so far: 102528 molecules\n",
      "Training time for 100 batches: 11.076436042785645 s\n",
      "Training time for epoch 1: 95.00657415390015 s\n",
      "Starting validation for epoch 1\n",
      "Time taken for epoch 1: 107.33924198150635 s\n",
      "Training err over epoch 1: 5.518240451812744\n",
      "Validation err for epoch 1: 0.42071062326431274\n",
      "\n",
      "Start of epoch 1\n",
      "Initial loss (for one batch): 0.18760550022125244\n",
      "Seen so far: 128 molecules\n",
      "Training loss (for one batch) at step 100: 0.1752261221408844\n",
      "Seen so far: 12928 molecules\n",
      "Training time for 100 batches: 11.200571298599243 s\n",
      "Training loss (for one batch) at step 200: 0.14067327976226807\n",
      "Seen so far: 25728 molecules\n",
      "Training time for 100 batches: 11.063788890838623 s\n",
      "Training loss (for one batch) at step 300: 0.2051984965801239\n",
      "Seen so far: 38528 molecules\n",
      "Training time for 100 batches: 11.198917865753174 s\n",
      "Training loss (for one batch) at step 400: 0.15809579193592072\n",
      "Seen so far: 51328 molecules\n",
      "Training time for 100 batches: 11.284780025482178 s\n",
      "Training loss (for one batch) at step 500: 0.1723126769065857\n",
      "Seen so far: 64128 molecules\n",
      "Training time for 100 batches: 11.65038013458252 s\n",
      "Training loss (for one batch) at step 600: 0.15114670991897583\n",
      "Seen so far: 76928 molecules\n",
      "Training time for 100 batches: 11.513000011444092 s\n",
      "Training loss (for one batch) at step 700: 0.13108637928962708\n",
      "Seen so far: 89728 molecules\n",
      "Training time for 100 batches: 11.80394697189331 s\n",
      "Training loss (for one batch) at step 800: 0.13193129003047943\n",
      "Seen so far: 102528 molecules\n",
      "Training time for 100 batches: 11.5257248878479 s\n",
      "Training time for epoch 2: 95.47157621383667 s\n",
      "Starting validation for epoch 2\n",
      "Time taken for epoch 2: 107.63066506385803 s\n",
      "Training err over epoch 2: 0.3741546869277954\n",
      "Validation err for epoch 2: 0.3748406767845154\n",
      "\n",
      "Start of epoch 2\n",
      "Initial loss (for one batch): 0.1599549949169159\n",
      "Seen so far: 128 molecules\n",
      "Training loss (for one batch) at step 100: 0.11940636485815048\n",
      "Seen so far: 12928 molecules\n",
      "Training time for 100 batches: 10.892243146896362 s\n",
      "Training loss (for one batch) at step 200: 0.10203763842582703\n",
      "Seen so far: 25728 molecules\n",
      "Training time for 100 batches: 10.711910009384155 s\n",
      "Training loss (for one batch) at step 300: 0.13345076143741608\n",
      "Seen so far: 38528 molecules\n",
      "Training time for 100 batches: 11.24455189704895 s\n",
      "Training loss (for one batch) at step 400: 0.10118229687213898\n",
      "Seen so far: 51328 molecules\n",
      "Training time for 100 batches: 11.269775152206421 s\n",
      "Training loss (for one batch) at step 500: 0.09702914953231812\n",
      "Seen so far: 64128 molecules\n",
      "Training time for 100 batches: 11.536812782287598 s\n",
      "Training loss (for one batch) at step 600: 0.09402848780155182\n",
      "Seen so far: 76928 molecules\n",
      "Training time for 100 batches: 11.325470209121704 s\n",
      "Training loss (for one batch) at step 700: 0.09874394536018372\n",
      "Seen so far: 89728 molecules\n",
      "Training time for 100 batches: 11.217292785644531 s\n",
      "Training loss (for one batch) at step 800: 0.07712528109550476\n",
      "Seen so far: 102528 molecules\n",
      "Training time for 100 batches: 10.838972330093384 s\n",
      "Training time for epoch 3: 93.0167441368103 s\n",
      "Starting validation for epoch 3\n",
      "Time taken for epoch 3: 105.34429025650024 s\n",
      "Training err over epoch 3: 0.30332088470458984\n",
      "Validation err for epoch 3: 0.24066030979156494\n",
      "\n",
      "Start of epoch 3\n",
      "Initial loss (for one batch): 0.06267625093460083\n",
      "Seen so far: 128 molecules\n",
      "Training loss (for one batch) at step 100: 0.048258692026138306\n",
      "Seen so far: 12928 molecules\n",
      "Training time for 100 batches: 10.807352066040039 s\n",
      "Training loss (for one batch) at step 200: 0.04989827796816826\n",
      "Seen so far: 25728 molecules\n",
      "Training time for 100 batches: 10.84601616859436 s\n",
      "Training loss (for one batch) at step 300: 0.05318884551525116\n",
      "Seen so far: 38528 molecules\n",
      "Training time for 100 batches: 11.247463941574097 s\n",
      "Training loss (for one batch) at step 400: 0.057420723140239716\n",
      "Seen so far: 51328 molecules\n",
      "Training time for 100 batches: 10.999733924865723 s\n",
      "Training loss (for one batch) at step 500: 0.05550340563058853\n",
      "Seen so far: 64128 molecules\n",
      "Training time for 100 batches: 10.958400011062622 s\n",
      "Training loss (for one batch) at step 600: 0.04170272499322891\n",
      "Seen so far: 76928 molecules\n",
      "Training time for 100 batches: 11.486663103103638 s\n",
      "Training loss (for one batch) at step 700: 1.1540701389312744\n",
      "Seen so far: 89728 molecules\n",
      "Training time for 100 batches: 11.91369104385376 s\n",
      "Training loss (for one batch) at step 800: 0.4376097023487091\n",
      "Seen so far: 102528 molecules\n",
      "Training time for 100 batches: 10.63945484161377 s\n",
      "Training time for epoch 4: 92.73022127151489 s\n",
      "Starting validation for epoch 4\n",
      "Time taken for epoch 4: 105.03690505027771 s\n",
      "Training err over epoch 4: 0.4037773609161377\n",
      "Validation err for epoch 4: 0.8219907283782959\n",
      "\n",
      "Start of epoch 4\n",
      "Initial loss (for one batch): 0.6486061811447144\n",
      "Seen so far: 128 molecules\n",
      "Training loss (for one batch) at step 100: 0.1234966367483139\n",
      "Seen so far: 12928 molecules\n",
      "Training time for 100 batches: 11.506479263305664 s\n",
      "Training loss (for one batch) at step 200: 0.04640965908765793\n",
      "Seen so far: 25728 molecules\n",
      "Training time for 100 batches: 11.045108795166016 s\n",
      "Training loss (for one batch) at step 300: 0.04399504140019417\n",
      "Seen so far: 38528 molecules\n",
      "Training time for 100 batches: 12.362826108932495 s\n",
      "Training loss (for one batch) at step 400: 0.037170782685279846\n",
      "Seen so far: 51328 molecules\n",
      "Training time for 100 batches: 11.337315082550049 s\n",
      "Training loss (for one batch) at step 500: 0.05069281905889511\n",
      "Seen so far: 64128 molecules\n",
      "Training time for 100 batches: 11.692704677581787 s\n",
      "Training loss (for one batch) at step 600: 0.02477584034204483\n",
      "Seen so far: 76928 molecules\n",
      "Training time for 100 batches: 11.925540208816528 s\n",
      "Training loss (for one batch) at step 700: 0.02442052587866783\n",
      "Seen so far: 89728 molecules\n",
      "Training time for 100 batches: 11.355329036712646 s\n",
      "Training loss (for one batch) at step 800: 0.041680287569761276\n",
      "Seen so far: 102528 molecules\n",
      "Training time for 100 batches: 11.99158501625061 s\n",
      "Training time for epoch 5: 97.08621120452881 s\n",
      "Starting validation for epoch 5\n",
      "Time taken for epoch 5: 109.90611815452576 s\n",
      "Training err over epoch 5: 0.2745468020439148\n",
      "Validation err for epoch 5: 0.13365767896175385\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/miguelnavaharris/PiNN/docs/notebooks/custom_loop_atomic_dress.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/custom_loop_atomic_dress.ipynb#ch0000010?line=0'>1</a>\u001b[0m train_and_evaluate_network(batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m)\n",
      "\u001b[1;32m/Users/miguelnavaharris/PiNN/docs/notebooks/custom_loop_atomic_dress.ipynb Cell 9'\u001b[0m in \u001b[0;36mtrain_and_evaluate_network\u001b[0;34m(network, params, train_set, test_set, batch_size, epochs)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/custom_loop_atomic_dress.ipynb#ch0000009?line=113'>114</a>\u001b[0m train_err_metric\u001b[39m.\u001b[39mreset_states()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/custom_loop_atomic_dress.ipynb#ch0000009?line=114'>115</a>\u001b[0m val_err_metric\u001b[39m.\u001b[39mreset_states()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/custom_loop_atomic_dress.ipynb#ch0000009?line=116'>117</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m>\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m val_errors[\u001b[39m-\u001b[39;49m\u001b[39m4\u001b[39;49m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(val_errors[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:]):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/custom_loop_atomic_dress.ipynb#ch0000009?line=117'>118</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mStopped.\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_network(batch_size=batch_size, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generator(molecule):\n",
    "        data = {'coord': molecule.positions,\n",
    "                'ind_1': np.zeros([len(molecule), 1]),\n",
    "                'elems': molecule.numbers}\n",
    "        yield data\n",
    "\n",
    "def predict_energy(molecule):\n",
    "        '''Takes an ASE Atoms object and outputs PiNet's energy prediction'''\n",
    "        dtype=tf.float32\n",
    "        dtypes = {'coord': dtype, 'elems': tf.int32, 'ind_1': tf.int32}\n",
    "        shapes = {'coord': [None, 3], 'elems': [None], 'ind_1': [None, 1]}\n",
    "\n",
    "        pred_dataset = tf.data.Dataset.from_generator(lambda:_generator(molecule), dtypes, shapes)\n",
    "\n",
    "        for molecule in pred_dataset:\n",
    "                molecule = network.preprocess(molecule)\n",
    "                pred = network(molecule, training=False)\n",
    "                ind = molecule['ind_1']\n",
    "                nbatch = tf.reduce_max(ind)+1\n",
    "                energy_prediction = tf.math.unsorted_segment_sum(pred, ind[:, 0], nbatch)\n",
    "                energy_prediction_numpy = energy_prediction.numpy()[0]\n",
    "        return energy_prediction_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coord': array([[ 0.      ,  0.      ,  0.      ],\n",
       "        [ 0.629118,  0.629118,  0.629118],\n",
       "        [-0.629118, -0.629118,  0.629118],\n",
       "        [ 0.629118, -0.629118, -0.629118],\n",
       "        [-0.629118,  0.629118, -0.629118]]),\n",
       " 'ind_1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'elems': array([6, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(_generator(g2['CH4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10940915"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_energy(g2['CH4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'elems': <tf.Tensor: shape=(2310,), dtype=int32, numpy=array([6, 8, 6, ..., 1, 1, 1], dtype=int32)>, 'coord': <tf.Tensor: shape=(2310, 3), dtype=float32, numpy=\n",
      "array([[-0.04232784,  1.399071  ,  0.02622217],\n",
      "       [-0.0028356 , -0.02999014,  0.00863342],\n",
      "       [-1.190231  , -0.64151204, -0.00798004],\n",
      "       ...,\n",
      "       [-0.3063433 , -2.4607291 ,  1.1791134 ],\n",
      "       [ 0.01973908, -2.460475  , -1.2095277 ],\n",
      "       [ 1.7551451 , -2.7061844 , -0.97355413]], dtype=float32)>, 'e_data': <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
      "array([-415.84818, -388.26443, -460.1791 , -356.4105 , -383.25824,\n",
      "       -385.81726, -440.29184, -437.95004, -363.82178, -349.87012,\n",
      "       -401.88684, -496.12506, -365.94238, -419.2043 , -398.15903,\n",
      "       -383.1639 , -387.0078 , -436.714  , -458.93304, -387.07278,\n",
      "       -348.97882, -419.14963, -399.84415, -385.89682, -437.952  ,\n",
      "       -401.03882, -396.01077, -351.17123, -421.79355, -362.56934,\n",
      "       -419.23734, -422.97028, -544.9652 , -383.71817, -389.52267,\n",
      "       -365.9758 , -379.7031 , -458.89664, -421.82675, -401.9406 ,\n",
      "       -400.6696 , -367.16446, -436.64743, -367.18372, -438.65427,\n",
      "       -401.8739 , -385.8675 , -424.17764, -349.01147, -418.06097,\n",
      "       -387.01794, -416.72238, -422.93433, -421.78848, -416.83902,\n",
      "       -399.3142 , -457.6976 , -471.76523, -419.24902, -472.65723,\n",
      "       -420.90167, -421.8071 , -437.7447 , -383.32285, -389.49014,\n",
      "       -419.27942, -423.02197, -439.0822 , -414.70868, -424.22815,\n",
      "       -494.89313, -348.96768, -365.96396, -417.96436, -395.9951 ,\n",
      "       -387.07645, -486.13986, -314.273  , -419.18356, -349.9266 ,\n",
      "       -439.1034 , -426.6227 , -382.5876 , -458.88474, -496.1288 ,\n",
      "       -416.80164, -422.97424, -455.1509 , -475.0384 , -434.06738,\n",
      "       -453.91574, -403.19064, -502.0582 , -549.501  , -400.6224 ,\n",
      "       -432.8374 , -424.218  , -390.69525, -365.00626, -387.04996,\n",
      "       -303.46808, -424.183  , -439.1031 , -404.38138, -398.27084,\n",
      "       -387.08124, -396.88498, -439.12933, -425.43262, -385.8513 ,\n",
      "       -384.96286, -437.80237, -405.5931 , -388.2607 , -336.80267,\n",
      "       -460.1547 , -327.87613, -421.86795, -418.0266 , -365.93777,\n",
      "       -385.86057, -403.1512 , -403.15845, -421.72107, -404.29816,\n",
      "       -437.8374 , -434.10992, -388.2815 ], dtype=float32)>, 'ind_1': <tf.Tensor: shape=(2310, 1), dtype=int32, numpy=\n",
      "array([[  0],\n",
      "       [  0],\n",
      "       [  0],\n",
      "       ...,\n",
      "       [127],\n",
      "       [127],\n",
      "       [127]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for i in train_set:\n",
    "    print(i)\n",
    "    a = i\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = network.preprocess(a)\n",
    "connect_dist_grad(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_pred = network(b)\n",
    "ind = b['ind_1']\n",
    "nbatch = tf.reduce_max(ind)+1\n",
    "b_pred = tf.math.unsorted_segment_sum(b_pred, ind[:, 0], nbatch)\n",
    "e_data = b['e_data']\n",
    "e_data -= atomic_dress(b, params['model']['params']['e_dress'], dtype=b_pred.dtype)\n",
    "e_data *= params['model']['params']['e_scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([ 0.02334356,  0.13129595,  0.00388181,  0.09036437,  0.06984192,\n",
       "        0.12849873,  0.03096673,  0.08801979,  0.05233441,  0.18315434,\n",
       "        0.26323727,  0.14985794,  0.28263676,  0.13722047,  0.05402932,\n",
       "        0.12795132,  0.12926355,  0.08792357,  0.15599254,  0.22791398,\n",
       "        0.01809204,  0.05822349, -0.05284816,  0.07549483,  0.05241656,\n",
       "       -0.09321058, -0.09120913,  0.18246794,  0.26677558,  0.08405244,\n",
       "       -0.00292611,  0.08461102,  0.10344365, -0.051561  ,  0.13123792,\n",
       "        0.34564012,  0.03771929,  0.12423572,  0.14190102,  0.11671156,\n",
       "        0.16461325, -0.03116125,  0.12778872,  0.17329329,  0.10661355,\n",
       "        0.292666  ,  0.20575708,  0.13029581,  0.13664913,  0.09482099,\n",
       "        0.13021207,  0.19191644,  0.16570061,  0.01283294,  0.08395585,\n",
       "       -0.08807528,  0.16191617, -0.0467796 ,  0.08784631,  0.02075377,\n",
       "       -0.00814182,  0.10030571,  0.2241647 ,  0.00799146,  0.2084836 ,\n",
       "        0.09057319,  0.08078763,  0.21616393,  0.02136239,  0.11923584,\n",
       "        0.20583102, -0.02791201,  0.16303718,  0.13737708, -0.16531694,\n",
       "        0.20710105,  0.02667157,  0.21098667,  0.04874986,  0.21351486,\n",
       "        0.09040156,  0.10408321,  0.17610954,  0.1731205 , -0.05153573,\n",
       "        0.10757688,  0.14746341, -0.01918244,  0.14231902,  0.11010721,\n",
       "        0.0307973 ,  0.03823054,  0.08654103,  0.19332945,  0.2658111 ,\n",
       "        0.17662647,  0.1802175 ,  0.24477792,  0.10319811,  0.17464355,\n",
       "        0.07322697,  0.18411827,  0.03023753,  0.14134747,  0.13052079,\n",
       "        0.17362005,  0.07282856,  0.02732435,  0.06215715,  0.1474052 ,\n",
       "        0.01185757,  0.16196534, -0.05144385,  0.09361142, -0.05576199,\n",
       "        0.19413209,  0.1164524 ,  0.06617612,  0.18130857,  0.26294422,\n",
       "        0.05976221,  0.01990116,  0.09307295,  0.26353368,  0.16225255,\n",
       "        0.21023607,  0.05190027,  0.15092778], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02023315,  0.00613403, -0.0206604 ,  0.01885986,  0.00390625,\n",
       "        0.03814697, -0.00576782, -0.0791626 ,  0.03118896,  0.04138184,\n",
       "        0.04013062, -0.02267456,  0.04064941,  0.0017395 , -0.03292847,\n",
       "        0.09823608,  0.05517578, -0.0506897 ,  0.01779175, -0.00976562,\n",
       "        0.01019287,  0.05648804, -0.04730225, -0.04141235, -0.08105469,\n",
       "       -0.03439331, -0.01473999, -0.05215454,  0.00582886,  0.0760498 ,\n",
       "       -0.03125   ,  0.03662109,  0.0402832 ,  0.00717163, -0.04452515,\n",
       "        0.00723267, -0.06369019,  0.05422974, -0.02740479, -0.01364136,\n",
       "        0.04977417,  0.02615356,  0.01586914,  0.00689697,  0.04431152,\n",
       "        0.05307007, -0.01208496,  0.03683472, -0.02246094, -0.0625    ,\n",
       "        0.04504395,  0.06854248,  0.07260132,  0.01083374, -0.04806519,\n",
       "        0.01947021,  0.04568481, -0.00109863, -0.04293823, -0.05001831,\n",
       "       -0.02484131, -0.00772095,  0.12619019, -0.06069946, -0.01196289,\n",
       "       -0.07333374, -0.01507568, -0.00375366, -0.04779053, -0.01361084,\n",
       "        0.00164795,  0.02133179,  0.01907349,  0.03414917,  0.00097656,\n",
       "       -0.01342773, -0.05435181, -0.02035522,  0.02252197, -0.0151062 ,\n",
       "       -0.02493286,  0.00695801, -0.0697937 ,  0.06613159, -0.02645874,\n",
       "       -0.01071167,  0.03265381, -0.00088501, -0.01596069,  0.00265503,\n",
       "        0.02670288, -0.05609131, -0.04031372,  0.04998779,  0.09698486,\n",
       "        0.02505493, -0.003479  , -0.00949097,  0.05429077,  0.01303101,\n",
       "        0.02334595,  0.03152466, -0.0246582 , -0.03927612,  0.03338623,\n",
       "       -0.01821899,  0.03356934, -0.0508728 , -0.01055908,  0.00411987,\n",
       "       -0.02993774,  0.06851196, -0.043396  ,  0.0098877 ,  0.03936768,\n",
       "        0.00375366,  0.03289795, -0.068573  , -0.02810669,  0.04525757,\n",
       "       -0.00512695, -0.01669312, -0.02389526,  0.07830811,  0.04394531,\n",
       "        0.03347778, -0.03991699, -0.01092529], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13507892"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(e_data, b_pred, squared=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d2dca5ff04745c24a12b1eaafe2ed2b92b4632c1d2e7bd797725dd5127e9eda"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('pinn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
