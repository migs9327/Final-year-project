{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dipole model, updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from pinn.optimizers import get\n",
    "from glob import glob\n",
    "from pinn.io import load_qm9\n",
    "from docs.notebooks.network_fns import get_traintest_sets, preprocess_traintest_sets\n",
    "import time\n",
    "from pinn.layers import PolynomialBasis, GaussianBasis, ANNOutput\n",
    "from pinn.networks.pinet import OutLayer, GCBlock, ResUpdate, PreprocessLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to train on CPU\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "tf.config.set_visible_devices(physical_devices[0], 'CPU')\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 02:05:59.482985: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-08-01 02:05:59.483089: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "filelist = glob('/Users/miguelnavaharris/Project/QM9/*.xyz')\n",
    "dataset = load_qm9(filelist, label_map={'d_data': 'mu'}, splits={'train':8, 'test':2}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'optimizer': {\n",
    "        'class_name': 'Adam',\n",
    "        'config': {\n",
    "            'learning_rate': {\n",
    "                'class_name': 'ExponentialDecay',\n",
    "                'config': {\n",
    "                    'initial_learning_rate': 0.0003,\n",
    "                    'decay_steps': 10000, \n",
    "                    'decay_rate': 0.994}}, \n",
    "                    'clipnorm': 0.01}}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiNet(tf.keras.Model):\n",
    "    \"\"\"Keras model for the PiNet neural network\n",
    "\n",
    "    Args:\n",
    "        tensors: input data (nested tensor from dataset).\n",
    "        atom_types (list): elements for the one-hot embedding.\n",
    "        pp_nodes (list): number of nodes for pp layer.\n",
    "        pi_nodes (list): number of nodes for pi layer.\n",
    "        ii_nodes (list): number of nodes for ii layer.\n",
    "        en_nodes (list): number of nodes for en layer.\n",
    "        depth (int): number of interaction blocks.\n",
    "        rc (float): cutoff radius.\n",
    "        basis_type (string): type of basis function to use,\n",
    "            can be \"polynomial\" or \"gaussian\".\n",
    "        n_basis (int): number of basis functions to use.\n",
    "        gamma (float or array): width of gaussian function for gaussian basis.\n",
    "        center (float or array): center of gaussian function for gaussian basis.\n",
    "        cutoff_type (string): cutoff function to use with the basis.\n",
    "        act (string): activation function to use.\n",
    "        preprocess (bool): whether to return the preprocessed tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, atom_types=[1, 6, 7, 8, 9],  rc=4.0, cutoff_type='f1',\n",
    "                 basis_type='polynomial', n_basis=4, gamma=3.0, center=None,\n",
    "                 pp_nodes=[16, 16], pi_nodes=[16, 16], ii_nodes=[16, 16],\n",
    "                 out_nodes=[16, 16], out_units=1, out_pool=False,\n",
    "                 act='tanh', depth=4):\n",
    "\n",
    "        super(PiNet, self).__init__()\n",
    "\n",
    "        self.depth = depth\n",
    "        self.preprocess = PreprocessLayer(atom_types, rc)\n",
    "        self.activation = act\n",
    "\n",
    "        if basis_type == 'polynomial':\n",
    "            self.basis_fn = PolynomialBasis(cutoff_type, rc, n_basis)\n",
    "        elif basis_type == 'gaussian':\n",
    "            self.basis_fn = GaussianBasis(cutoff_type, rc, n_basis, gamma, center)\n",
    "\n",
    "        self.res_update = [ResUpdate() for i in range(depth)]\n",
    "        self.gc_blocks = [GCBlock([], pi_nodes, ii_nodes, activation=act)]\n",
    "        self.gc_blocks += [GCBlock(pp_nodes, pi_nodes, ii_nodes, activation=act)\n",
    "                           for i in range(depth-1)]\n",
    "        self.out_layers = [OutLayer(out_nodes, out_units) for i in range(depth)]\n",
    "        self.ann_output =  ANNOutput(out_pool)\n",
    "    \n",
    "    def call(self, tensors):\n",
    "        tensors = self.preprocess(tensors)\n",
    "        basis = self.basis_fn(tensors['dist'])[:, None, :]\n",
    "\n",
    "        output = 0.0\n",
    "        for i in range(self.depth):\n",
    "            prop = self.gc_blocks[i]([tensors['ind_2'], tensors['prop'], basis])\n",
    "            output = self.out_layers[i]([tensors['ind_1'], prop, output])\n",
    "            tensors['prop'] = self.res_update[i]([tensors['prop'], prop])\n",
    "\n",
    "        output = self.ann_output([tensors['ind_1'], output])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_network(network=None, params=None, train_set=None, test_set=None, batch_size=256, epochs=1):\n",
    "\n",
    "\n",
    "    # Instantiate an optimizer\n",
    "    optimizer = get(params['optimizer'])\n",
    "    # Define metrics\n",
    "    D_MAE = tf.keras.metrics.MeanAbsoluteError()\n",
    "    D_RMSE = tf.keras.metrics.RootMeanSquaredError()\n",
    "    Q_MAE = tf.keras.metrics.MeanAbsoluteError()\n",
    "    Q_RMSE = tf.keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time_epoch = time.time()\n",
    "        hund_step_times = []\n",
    "\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, batch in enumerate(train_set):\n",
    "\n",
    "            train_losses = []\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                # start_time = time.time()\n",
    "\n",
    "                pred = network(batch, training=True)  # Logits for this minibatch\n",
    "                pred = tf.expand_dims(pred, axis=1)\n",
    "                ind = batch['ind_1']\n",
    "                nbatch = tf.reduce_max(ind)+1\n",
    "                charge = tf.math.unsorted_segment_sum(pred, ind[:, 0], nbatch)\n",
    "                dipole = pred * batch['coord']\n",
    "                dipole = tf.math.unsorted_segment_sum(dipole, ind[:, 0], nbatch)\n",
    "                dipole = tf.sqrt(tf.reduce_sum(dipole**2, axis=1)+1e-6)\n",
    "\n",
    "                d_data = batch['d_data']\n",
    "                q_data = tf.zeros_like(charge)\n",
    "\n",
    "                d_error = d_data - dipole\n",
    "                q_error = q_data - charge\n",
    "                d_loss = tf.reduce_mean(d_error**2)\n",
    "                train_losses.append(d_loss)\n",
    "                q_loss = tf.reduce_mean(q_error**2)\n",
    "                train_losses.append(q_loss)\n",
    "\n",
    "\n",
    "                # Compute the loss value for this minibatch.\n",
    "                train_loss_value = tf.reduce_sum(train_losses)\n",
    "\n",
    "\n",
    "            grads = tape.gradient(train_loss_value, network.trainable_weights)\n",
    "\n",
    "\n",
    "            optimizer.apply_gradients(zip(grads, network.trainable_weights))\n",
    "\n",
    "            # end_time = time.time() - start_time\n",
    "            # print('End time', end_time)\n",
    "\n",
    "            # Log every 20 batches.\n",
    "            if step == 0:\n",
    "                print(f\"Initial loss (for one batch): {float(train_loss_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "\n",
    "\n",
    "\n",
    "            elif step % 100 == 0:\n",
    "                print(f\"Training loss (for one batch) at step {step}: {float(train_loss_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "                hund_step_times += [(time.time() - start_time_epoch)]\n",
    "                print(f'Training time for 100 batches: {((hund_step_times[-1] - hund_step_times[-2]) if len(hund_step_times) > 1 else hund_step_times[-1])} s')\n",
    "\n",
    "      \n",
    "        print(f'Training time for epoch {epoch + 1}: {(time.time() - start_time_epoch)} s')        \n",
    "\n",
    "        \n",
    "\n",
    "        # Run a validation loop at the end of each epoch\n",
    "        print(f'Starting validation for epoch {(epoch + 1)}')\n",
    "\n",
    "        for step, batch in enumerate(test_set):\n",
    "\n",
    "            val_losses = []\n",
    "\n",
    "            val_pred = network(batch, training=False) # Logits for this minibatch\n",
    "            val_pred = tf.expand_dims(val_pred, axis=1)\n",
    "            ind = batch['ind_1']\n",
    "            nbatch = tf.reduce_max(ind)+1\n",
    "            charge = tf.math.unsorted_segment_sum(val_pred, ind[:, 0], nbatch)\n",
    "            dipole = val_pred * batch['coord']\n",
    "            dipole = tf.math.unsorted_segment_sum(dipole, ind[:, 0], nbatch)\n",
    "            dipole = tf.sqrt(tf.reduce_sum(dipole**2, axis=1)+1e-6)\n",
    "\n",
    "            d_data = batch['d_data']\n",
    "            q_data = tf.zeros_like(charge)\n",
    "            \n",
    "            d_error = d_data - dipole\n",
    "            q_error = q_data - charge\n",
    "            d_loss = tf.reduce_mean(d_error**2)\n",
    "            val_losses.append(d_loss)\n",
    "            q_loss = tf.reduce_mean(q_error**2)\n",
    "            val_losses.append(q_loss)\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            val_loss_value = tf.reduce_sum(val_losses)\n",
    "\n",
    "            # Update metrics\n",
    "            D_MAE.update_state(d_data, dipole)\n",
    "            D_RMSE.update_state(d_data, dipole)\n",
    "            Q_MAE.update_state(q_data, charge)\n",
    "            Q_RMSE.update_state(q_data, charge)\n",
    "\n",
    "\n",
    "            # Log every 20 batches.\n",
    "            if step == 0:\n",
    "                print(f\"Initial loss (for one batch): {float(val_loss_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "\n",
    "\n",
    "            elif step % 100 == 0:\n",
    "                print(f\"Validation loss (for one batch) at step {step}: {float(val_loss_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "                hund_step_times += [(time.time() - start_time_epoch)]\n",
    "                print(f'Validation time for 100 batches: {((hund_step_times[-1] - hund_step_times[-2]) if len(hund_step_times) > 1 else hund_step_times[-1])} s')\n",
    "            \n",
    "        # Display and reset validation metric results\n",
    "        print(f\"D_MAE: {float(D_MAE.result())}\")\n",
    "        print(f\"D_RMSE: {float(D_RMSE.result())}\")\n",
    "        print(f\"Q_MAE: {float(Q_MAE.result())}\")\n",
    "        print(f\"Q_RMSE: {float(Q_RMSE.result())}\")\n",
    "        D_MAE.reset_states()\n",
    "        D_RMSE.reset_states()\n",
    "        Q_MAE.reset_states()\n",
    "        Q_RMSE.reset_states()\n",
    "\n",
    "\n",
    "        print(f\"Time taken for epoch {epoch + 1}: {(time.time() - start_time_epoch)} s\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 02:06:03.657411: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-08-01 02:06:03.657634: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set, batch_size = get_traintest_sets(dataset=dataset, buffer_size=1000, batch_size=256)\n",
    "network = PiNet()\n",
    "preprocess_traintest_sets(train_set, test_set, network=network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Initial loss (for one batch): 1779.81689453125\n",
      "Seen so far: 256 molecules\n",
      "Training loss (for one batch) at step 100: 32.78116226196289\n",
      "Seen so far: 25856 molecules\n",
      "Training time for 100 batches: 55.77427625656128 s\n",
      "Training loss (for one batch) at step 200: 6.726493835449219\n",
      "Seen so far: 51456 molecules\n",
      "Training time for 100 batches: 71.51356387138367 s\n",
      "Training loss (for one batch) at step 300: 2.994511604309082\n",
      "Seen so far: 77056 molecules\n",
      "Training time for 100 batches: 79.5966010093689 s\n",
      "Training loss (for one batch) at step 400: 1.2767127752304077\n",
      "Seen so far: 102656 molecules\n",
      "Training time for 100 batches: 91.73267698287964 s\n",
      "Training time for epoch 1: 316.51854705810547 s\n",
      "Starting validation for epoch 1\n",
      "Initial loss (for one batch): 170.52392578125\n",
      "Seen so far: 256 molecules\n",
      "Validation loss (for one batch) at step 100: 165.39002990722656\n",
      "Seen so far: 25856 molecules\n",
      "Validation time for 100 batches: 65.2418122291565 s\n",
      "D_MAE: 9.314730644226074\n",
      "D_RMSE: 11.271841049194336\n",
      "Q_MAE: 7.141120910644531\n",
      "Q_RMSE: 7.321422576904297\n",
      "Time taken for epoch 1: 365.94598722457886 s\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_network(network, params=params, train_set=train_set, test_set=test_set, batch_size=batch_size, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4025a0c18342a57b4a17c482f921a5b0f0c41971fda061095662d1f6a4a25c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9 (pinn)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
