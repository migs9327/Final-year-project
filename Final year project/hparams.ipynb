{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pinn import get_network\n",
    "from pinn.utils import atomic_dress\n",
    "from ase.collections import g2\n",
    "from pinn.io import  load_numpy\n",
    "from docs.notebooks.network_fns import get_traintest_sets, preprocess_traintest_sets\n",
    "from ase import Atoms\n",
    "from ase.calculators.lj import LennardJones\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: get the position given PES dimension(s)\n",
    "def three_body_sample(atoms, a, r):\n",
    "    x = a * np.pi / 180\n",
    "    pos = [[0, 0, 0],\n",
    "           [0, 2, 0],\n",
    "           [0, r*np.cos(x), r*np.sin(x)]]\n",
    "    atoms.set_positions(pos)\n",
    "    return atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = Atoms('H3', calculator=LennardJones())\n",
    "\n",
    "na, nr = 50, 50\n",
    "arange = np.linspace(30,180,na)\n",
    "rrange = np.linspace(1,3,nr)\n",
    "\n",
    "# Truth\n",
    "agrid, rgrid = np.meshgrid(arange, rrange)\n",
    "egrid = np.zeros([na, nr])\n",
    "for i in range(na):\n",
    "    for j in range(nr):\n",
    "        atoms = three_body_sample(atoms, arange[i], rrange[j])\n",
    "        egrid[i,j] = atoms.get_potential_energy()\n",
    "        \n",
    "# Samples\n",
    "nsample = 100\n",
    "asample, rsample = [], []\n",
    "distsample = []\n",
    "data = {'e_data':[], 'f_data':[], 'elems':[], 'coord':[]}\n",
    "for i in range(nsample):\n",
    "    a, r = np.random.choice(arange), np.random.choice(rrange)\n",
    "    atoms = three_body_sample(atoms, a, r)\n",
    "    dist = atoms.get_all_distances()\n",
    "    dist = dist[np.nonzero(dist)]\n",
    "    data['e_data'].append(atoms.get_potential_energy())\n",
    "    data['f_data'].append(atoms.get_forces())\n",
    "    data['coord'].append(atoms.get_positions())\n",
    "    data['elems'].append(atoms.numbers)\n",
    "    asample.append(a)\n",
    "    rsample.append(r)\n",
    "    distsample.append(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset from numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 15:14:35.356853: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-22 15:14:35.356964: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "data = {k:np.array(v) for k,v in data.items()}\n",
    "dataset = load_numpy(data, splits={'train':8, 'test':2})\n",
    "\n",
    "train_set, test_set, batch_size = get_traintest_sets(dataset, buffer_size=100, batch_size=1) #train set contains only 80 molecules so batches are unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_train_network(hparams):\n",
    " \n",
    "    network = get_network(params['network'])\n",
    "    preprocess_traintest_sets(train_set, test_set, network=network)\n",
    "\n",
    "    #Set up tensorboard directory and writer\n",
    "    nodes = hparams[HP_NUM_NODES]\n",
    "    depth = hparams[HP_DEPTH]\n",
    "    lr = hparams[HP_LR]\n",
    "\n",
    "    run_dir = (\n",
    "    \"/Users/miguelnavaharris/Project/hparams_logs/fixedbatch_1_epoch/\"\n",
    "    + str(nodes)\n",
    "    + \"nodes_\"\n",
    "    + str(depth)\n",
    "    + \"depth_\"\n",
    "    + str(lr)\n",
    "    + \"lr\"\n",
    "    )\n",
    "\n",
    "    run_dir_writer = tf.summary.create_file_writer(run_dir)\n",
    "\n",
    "\n",
    "    import time\n",
    "\n",
    "    # Instantiate an optimizer\n",
    "    from pinn.optimizers import get\n",
    "    optimizer = get(params['optimizer'])\n",
    "    # Define a loss function\n",
    "    loss_fn = tf.keras.losses.mse\n",
    "    # Define metrics\n",
    "    train_loss_metric = tf.keras.metrics.MeanSquaredError()\n",
    "    val_loss_metric = tf.keras.metrics.MeanSquaredError()\n",
    "    train_err_metric = tf.keras.metrics.RootMeanSquaredError()\n",
    "    val_err_metric = tf.keras.metrics.RootMeanSquaredError()\n",
    "    \n",
    "\n",
    "    for epoch in range(1):\n",
    "        print(f'Start of epoch {epoch + 1}')\n",
    "        start_time_epoch = time.time()\n",
    "        hund_step_times = []\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, batch in enumerate(train_set):\n",
    "            \n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                # Run the forward pass of the layer.\n",
    "                pred = network(batch, training=True)  # Logits for this minibatch\n",
    "\n",
    "                ind = batch['ind_1']\n",
    "                nbatch = tf.reduce_max(ind)+1\n",
    "                pred = tf.math.unsorted_segment_sum(pred, ind[:, 0], nbatch)\n",
    "                e_data = batch['e_data']\n",
    "\n",
    "                if params['e_dress']:\n",
    "                    e_data -= atomic_dress(batch, params['e_dress'], dtype=pred.dtype)\n",
    "                    e_data *= params['e_scale']\n",
    "\n",
    "\n",
    "                # Compute the loss value for this minibatch.\n",
    "                loss_value = loss_fn(e_data, pred)\n",
    "\n",
    "            # Use the gradient tape to automatically retrieve\n",
    "            # the gradients of the trainable variables with respect to the loss.\n",
    "            grads = tape.gradient(loss_value, network.trainable_weights)\n",
    "\n",
    "            # Run one step of gradient descent by updating\n",
    "            # the value of the variables to minimize the loss.\n",
    "            optimizer.apply_gradients(zip(grads, network.trainable_weights))\n",
    "\n",
    "            # Update the loss and error metrics\n",
    "            train_loss_metric.update_state(e_data, pred)\n",
    "            train_err_metric.update_state(e_data, pred)\n",
    "\n",
    "\n",
    "\n",
    "            # Log every 100 batches.\n",
    "            if step == 0:\n",
    "                print(f\"Initial loss (for one batch): {float(loss_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "\n",
    "\n",
    "\n",
    "            elif step % 5 == 0:\n",
    "                print(f\"Training loss (for one batch) at step {step}: {float(loss_value)}\")\n",
    "                print(f\"Seen so far: {((step + 1) * batch_size)} molecules\")\n",
    "                hund_step_times += [(time.time() - start_time_epoch)]\n",
    "                print(f'Training time for 20 batches: {((hund_step_times[-1] - hund_step_times[-2]) if len(hund_step_times) > 1 else hund_step_times[-1])} s')\n",
    "                \n",
    "                # Record tensorboad metrics\n",
    "                with run_dir_writer.as_default():\n",
    "                    print('writing to tb')\n",
    "                    tf.summary.scalar('Running training loss', train_loss_metric.result(), step=step)\n",
    "                    tf.summary.scalar('Running training error', train_err_metric.result(), step=step)\n",
    "                    tf.summary.scalar('Training time/20 batches', ((hund_step_times[-1] - hund_step_times[-2]) if len(hund_step_times) > 1 else hund_step_times[-1]), step=step)\n",
    "\n",
    "\n",
    "\n",
    "        print(f'Training time for epoch {epoch + 1}: {(time.time() - start_time_epoch)} s')\n",
    "        \n",
    "                \n",
    "\n",
    "        # Run a validation loop at the end of each epoch\n",
    "        print(f'Starting validation for epoch {(epoch + 1)}')\n",
    "        for step, batch in enumerate(test_set):\n",
    "            val_pred = network(batch, training=False)\n",
    "            ind = batch['ind_1']\n",
    "            nbatch = tf.reduce_max(ind)+1\n",
    "            val_pred = tf.math.unsorted_segment_sum(val_pred, ind[:, 0], nbatch)\n",
    "            e_data = batch['e_data']\n",
    "\n",
    "            if params['e_dress']:\n",
    "                e_data -= atomic_dress(batch, params['e_dress'], dtype=pred.dtype)\n",
    "                e_data *= params['e_scale']\n",
    "\n",
    "\n",
    "            # Update val metrics\n",
    "            val_loss_metric.update_state(e_data, val_pred)\n",
    "            val_err_metric.update_state(e_data, val_pred)\n",
    "\n",
    "\n",
    "        print(f\"Time taken for epoch {epoch + 1}: {(time.time() - start_time_epoch)} s\")\n",
    "\n",
    "        # Display metrics at the end of each epoch\n",
    "        train_err = train_err_metric.result()\n",
    "        print(f\"Training err over epoch {(epoch + 1)}: {float(train_err)}\")\n",
    "        val_err = val_err_metric.result()\n",
    "        print(f\"Validation err for epoch {(epoch + 1)}: {float(val_err)}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Write to TensorBoard and reset metrics\n",
    "        with run_dir_writer.as_default():\n",
    "            hp.hparams(hparams)\n",
    "            train_error = train_err_metric.result()\n",
    "            val_error = val_err_metric.result()\n",
    "            train_loss = train_loss_metric.result()\n",
    "            val_loss = val_loss_metric.result()\n",
    "            tf.summary.scalar(\"training error\", train_error, step=1)\n",
    "            tf.summary.scalar(\"validation error\", val_error, step=1)\n",
    "            tf.summary.scalar(\"training loss\", train_loss, step=1)\n",
    "            tf.summary.scalar(\"validation loss\", val_loss, step=1)\n",
    "\n",
    "\n",
    "      \n",
    "        train_err_metric.reset_states()\n",
    "        val_err_metric.reset_states()\n",
    "        train_loss_metric.reset_states()\n",
    "        val_loss_metric.reset_states()\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_NODES = hp.HParam(\"num nodes\", hp.Discrete([32,64, 128]))\n",
    "HP_DEPTH = hp.HParam(\"depth\", hp.Discrete([6,8,10,128]))\n",
    "HP_LR = hp.HParam(\"initial learning rate\", hp.Discrete([0.03, 0.009, 0.003]))\n",
    "\n",
    "for num_nodes in HP_NUM_NODES.domain.values:\n",
    "    for depth in HP_DEPTH.domain.values:\n",
    "        for lr in HP_LR.domain.values:\n",
    "            hparams = {\n",
    "                HP_NUM_NODES: num_nodes,\n",
    "                HP_DEPTH: depth,\n",
    "                HP_LR: lr\n",
    "            }\n",
    "            \n",
    "            params={\n",
    "                'optimizer': {\n",
    "                    'class_name': 'Adam',\n",
    "                    'config': {\n",
    "                        'learning_rate': {\n",
    "                            'class_name': 'ExponentialDecay',\n",
    "                            'config': {\n",
    "                                'initial_learning_rate': lr,\n",
    "                                'decay_steps': 10000, \n",
    "                                'decay_rate': 0.994}}, \n",
    "                                'clipnorm': 0.01}},\n",
    "                'network': {\n",
    "                    'name': 'PiNet',\n",
    "                    'params': {\n",
    "                        'ii_nodes':[num_nodes,num_nodes],\n",
    "                        'pi_nodes':[num_nodes,num_nodes],\n",
    "                        'pp_nodes':[num_nodes,num_nodes],\n",
    "                        'out_nodes':[num_nodes,num_nodes],\n",
    "                        'depth': depth,\n",
    "                        'rc': 3.0,\n",
    "                        'atom_types':[1]}},\n",
    "\n",
    "                'e_dress': {},\n",
    "                'e_scale': 1,\n",
    "                'e_unit': 1.0,\n",
    "            } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 15:14:36.093229: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-07-22 15:14:36.093508: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Initial loss (for one batch): 1935.3538818359375\n",
      "Seen so far: 1 molecules\n",
      "Training loss (for one batch) at step 5: 358396960.0\n",
      "Seen so far: 6 molecules\n",
      "Training time for 20 batches: 22.261831998825073 s\n",
      "writing to tb\n",
      "Training loss (for one batch) at step 10: 1325567744.0\n",
      "Seen so far: 11 molecules\n",
      "Training time for 20 batches: 16.935457944869995 s\n",
      "writing to tb\n",
      "Training loss (for one batch) at step 15: 495481696.0\n",
      "Seen so far: 16 molecules\n",
      "Training time for 20 batches: 16.96381688117981 s\n",
      "writing to tb\n",
      "Training loss (for one batch) at step 20: 9251945.0\n",
      "Seen so far: 21 molecules\n",
      "Training time for 20 batches: 16.749802112579346 s\n",
      "writing to tb\n",
      "Training loss (for one batch) at step 25: 16451140608.0\n",
      "Seen so far: 26 molecules\n",
      "Training time for 20 batches: 17.07881999015808 s\n",
      "writing to tb\n",
      "Training loss (for one batch) at step 30: 15802391552.0\n",
      "Seen so far: 31 molecules\n",
      "Training time for 20 batches: 16.8231041431427 s\n",
      "writing to tb\n",
      "Training loss (for one batch) at step 35: 5214738.0\n",
      "Seen so far: 36 molecules\n",
      "Training time for 20 batches: 17.116415977478027 s\n",
      "writing to tb\n",
      "Training loss (for one batch) at step 40: 3331906816.0\n",
      "Seen so far: 41 molecules\n",
      "Training time for 20 batches: 17.381691932678223 s\n",
      "writing to tb\n",
      "Training loss (for one batch) at step 45: 386829312.0\n",
      "Seen so far: 46 molecules\n",
      "Training time for 20 batches: 17.12206506729126 s\n",
      "writing to tb\n",
      "Training loss (for one batch) at step 50: 426902272.0\n",
      "Seen so far: 51 molecules\n",
      "Training time for 20 batches: 17.095797061920166 s\n",
      "writing to tb\n",
      "Training loss (for one batch) at step 55: 25094291456.0\n",
      "Seen so far: 56 molecules\n",
      "Training time for 20 batches: 17.046499729156494 s\n",
      "writing to tb\n",
      "Training loss (for one batch) at step 60: 297655776.0\n",
      "Seen so far: 61 molecules\n",
      "Training time for 20 batches: 17.816189289093018 s\n",
      "writing to tb\n",
      "Training loss (for one batch) at step 65: 86090360.0\n",
      "Seen so far: 66 molecules\n",
      "Training time for 20 batches: 17.308883905410767 s\n",
      "writing to tb\n",
      "Training loss (for one batch) at step 70: 14133308416.0\n",
      "Seen so far: 71 molecules\n",
      "Training time for 20 batches: 17.193243980407715 s\n",
      "writing to tb\n",
      "Training loss (for one batch) at step 75: 1207593728.0\n",
      "Seen so far: 76 molecules\n",
      "Training time for 20 batches: 17.692625045776367 s\n",
      "writing to tb\n",
      "Training time for epoch 1: 276.925498008728 s\n",
      "Starting validation for epoch 1\n",
      "hello output: tf.Tensor(\n",
      "[[4548.505]\n",
      " [4548.505]], shape=(2, 1), dtype=float32)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguelnavaharris/miniforge3/envs/pinn/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3369: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "error_count = 0\n",
    "while True:\n",
    "    try:\n",
    "        get_and_train_network(hparams)\n",
    "        break\n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        print('Raised an error')\n",
    "        error_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search 1 (5 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_NODES = hp.HParam(\"num nodes\", hp.Discrete([4,8,16]))\n",
    "HP_DEPTH = hp.HParam(\"depth\", hp.Discrete([4,8,16]))\n",
    "HP_LR = hp.HParam(\"initial learning rate\", hp.Discrete([0.003, 0.0003, 0.00003]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gives 16, 8, 0.003 as lowest validation error. \\\n",
    "Training error 0.70893 \\\n",
    "Validation error 0.46347"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search 2 (5 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_NODES = hp.HParam(\"num nodes\", hp.Discrete([16,32,64]))\n",
    "HP_DEPTH = hp.HParam(\"depth\", hp.Discrete([8,32,64]))\n",
    "HP_LR = hp.HParam(\"initial learning rate\", hp.Discrete([0.03, 0.009, 0.003]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gives 32, 8, 0.003 as lowest validation error. \\\n",
    "Training error 2.9923 \\\n",
    "Validation error 0.99355"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat gives 16, 8, 0.003 as lowest validation error. \\\n",
    "Training error 1.8622\n",
    "Validation error 1.0966"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search 3 (5 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_NODES = hp.HParam(\"num nodes\", hp.Discrete([32,64, 128]))\n",
    "HP_DEPTH = hp.HParam(\"depth\", hp.Discrete([6,8,10,128]))\n",
    "HP_LR = hp.HParam(\"initial learning rate\", hp.Discrete([0.03, 0.009, 0.003]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gives 32, 6, 0.003 as lowest validation error. \\\n",
    "Training error 2.7386 \\\n",
    "Validation error 1.0408"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat gives 32, 8, 0.003 as lowest validation error. \\\n",
    "Training error 1.0956\n",
    "Validation error 1.4258"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4025a0c18342a57b4a17c482f921a5b0f0c41971fda061095662d1f6a4a25c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('pinn2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
