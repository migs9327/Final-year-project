{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pinn import get_network\n",
    "from pinn.utils import pi_named, atomic_dress, connect_dist_grad\n",
    "from pinn.models.base import export_model, get_train_op, MetricsCollector\n",
    "import os, warnings\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from ase.collections import g2\n",
    "from pinn.io import load_qm9, sparse_batch\n",
    "from pinn import get_model, get_calc\n",
    "from pinn.models.potential import make_metrics\n",
    "from pinn.optimizers import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = glob('/Users/miguelnavaharris/Project/QM9/*.xyz')\n",
    "dataset = lambda: load_qm9(filelist, splits={'train':8, 'test':2}) #lambda is being used here to assign execution of load_qm9 to the variable \"dataset\"\n",
    "train = lambda: dataset()['train'].repeat().shuffle(1000).apply(sparse_batch(256))\n",
    "test = lambda: dataset()['test'].repeat().apply(sparse_batch(256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 17:42:59.930520: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-20 17:42:59.930631: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-04-20 17:43:00.027939: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419\n"
     ]
    }
   ],
   "source": [
    "newtrain = dataset()['train'].shuffle(1000).apply(sparse_batch(256))\n",
    "trainbatchcount = 0\n",
    "for i in newtrain:\n",
    "    trainbatchcount += 1 \n",
    "\n",
    "print(trainbatchcount) #so 419 batches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n"
     ]
    }
   ],
   "source": [
    "newtest = dataset()['test'].shuffle(1000).apply(sparse_batch(100))\n",
    "testbatchcount = 0\n",
    "for i in newtest:\n",
    "    testbatchcount += 1 \n",
    "\n",
    "print(testbatchcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'optimizer': {'class_name': 'Adam', 'config': {'learning_rate': {'class_name': 'ExponentialDecay', 'config': {'initial_learning_rate': 0.0003, 'decay_steps': 10000, 'decay_rate': 0.994}}, 'clipnorm': 0.01}}, 'model_dir': '/Users/miguelnavaharris/Project/miguelmodels/newtrainloop', 'network': {'name': 'PiNet', 'params': {'depth': 4, 'rc': 4.0, 'atom_types': [1, 6, 7, 8, 9]}}, 'model': {'name': 'potential_model', 'params': {'learning_rate': 0.001}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = {\n",
    "    ### Scaling and units # The loss function will be MSE((pred - label) * scale)\n",
    "    # For vector/tensor predictions\n",
    "    # the error will be pre-component instead of per-atom\n",
    "    # e_unit is the unit of energy to report w.r.t the input labels\n",
    "    # no f_unit yet, f_unit is just e_unit/input coordinate unit\n",
    "    # e.g. if one have input in Hartree, scales it by 100 for training\n",
    "    #      and output eV when report error\n",
    "    #      then e_scale should be 100, and e_unit = hartree2evp\n",
    "    'e_dress': {},  # element-specific energy dress\n",
    "    'e_scale': 1.0,  # energy scale for prediction\n",
    "    'e_unit': 1.0,  # output unit of energy during prediction\n",
    "    # Loss function options\n",
    "    ## Energy\n",
    "    'max_energy': False,        # if set to float, omit energies larger than it\n",
    "    'use_e_per_atom': False,    # use e_per_atom to calculate e_loss\n",
    "    'log_e_per_atom': False,    # log e_per_atom and its distribution\n",
    "                                # ^- this is forcely done if use_e_per_atom\n",
    "    'use_e_weight': False,      # scales the loss according to e_weight\n",
    "    ## Force\n",
    "    'use_force': False,         # include force in loss function\n",
    "    'max_force_comp': False,    # if set to float, omit forces components larger than it\n",
    "    'no_force_comp': False,     # if set to int, use as maximum number of force components for a update\n",
    "    'use_f_weight': False,      # scales the loss according to f_weights\n",
    "    ## Stress\n",
    "    'use_stress': False,        # include stress in Loss function\n",
    "    ## L2\n",
    "    'use_l2': False,\n",
    "    ## Loss function multipliers\n",
    "    'e_loss_multiplier': 1.0,\n",
    "    'f_loss_multiplier': 1.0,\n",
    "    's_loss_multiplier': 1.0,\n",
    "    'l2_loss_multiplier': 1.0,\n",
    "    'separate_errors': False,   # workaround at this point\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pinn.networks.pinet.PiNet object at 0x16c6daaf0>\n"
     ]
    }
   ],
   "source": [
    "network = get_network(params['network'])\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = default_params.copy()\n",
    "model_params.update(params['model']['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in newtrain:\n",
    "    batch = network.preprocess(batch)\n",
    "    connect_dist_grad(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_dict = {'elems': tf.TensorSpec(shape=[], dtype=tf.float32),\n",
    "                 'coord': tf.TensorSpec(shape=[], dtype=tf.float32),\n",
    "                 'e_data': tf.TensorSpec(shape=[], dtype=tf.float32),\n",
    "                 'ind_1': tf.TensorSpec(shape=[], dtype=tf.float32)}\n",
    "#@tf.function can take this as argument like so:\n",
    "#@tf.function(input_signature=([signature_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_loop(newtrain):\n",
    "\n",
    "    optimizer = get(params['optimizer'])\n",
    "\n",
    "\n",
    "    epochs = 1\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "\n",
    "        for step, batch in enumerate(newtrain):\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                # Run the forward pass of the layer.\n",
    "                # The operations that the layer applies\n",
    "                # to its inputs are going to be recorded\n",
    "                # on the GradientTape.\n",
    "                pred = network(batch, training=True)  # Logits for this minibatch\n",
    "\n",
    "                ind = batch['ind_1']\n",
    "                nbatch = tf.reduce_max(ind)+1\n",
    "                pred = tf.math.unsorted_segment_sum(pred, ind[:, 0], nbatch)\n",
    "\n",
    "                metrics = make_metrics(batch, pred, model_params, tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "                # Compute the loss value for this minibatch.\n",
    "                loss_value = tf.reduce_sum(metrics.LOSS)\n",
    "\n",
    "            # Use the gradient tape to automatically retrieve\n",
    "            # the gradients of the trainable variables with respect to the loss.\n",
    "            grads = tape.gradient(loss_value, network.trainable_weights)\n",
    "\n",
    "            # Run one step of gradient descent by updating\n",
    "            # the value of the variables to minimize the loss.\n",
    "            optimizer.apply_gradients(zip(grads, network.trainable_weights))\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(\n",
    "                    f\"Training loss (for one batch) at step {step}: {float(loss_value)}\"\n",
    "                )\n",
    "                print('100 steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"concat_1:0\", shape=(None,), dtype=int32), values=Tensor(\"concat:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradient_tape/pi_net/gc_block_3/pi_layer_3/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"concat_3:0\", shape=(None,), dtype=int32), values=Tensor(\"concat_2:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradient_tape/pi_net/gc_block_2/pi_layer_2/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"concat_5:0\", shape=(None,), dtype=int32), values=Tensor(\"concat_4:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradient_tape/pi_net/gc_block_1/pi_layer_1/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step Tensor(\"args_1:0\", shape=(), dtype=int64): Tensor(\"Sum:0\", shape=(), dtype=float32)\n",
      "100 steps\n",
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/var/folders/2r/53clw21s5zv7ml4w3_xhd8v40000gq/T/ipykernel_99516/3938954701.py\", line 40, in train_loop  *\n        optimizer.apply_gradients(zip(grads, network.trainable_weights))\n    File \"/Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 639, in apply_gradients  **\n        self._create_all_weights(var_list)\n    File \"/Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 823, in _create_all_weights\n        _ = self.iterations\n    File \"/Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 830, in __getattribute__\n        return super(OptimizerV2, self).__getattribute__(name)\n    File \"/Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 985, in iterations\n        self._iterations = self.add_weight(\n    File \"/Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 1187, in add_weight\n        variable = self._add_variable_with_custom_getter(\n    File \"/Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/keras/engine/base_layer_utils.py\", line 117, in make_variable\n        return tf.compat.v1.Variable(\n\n    ValueError: tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/miguelnavaharris/PiNN/docs/notebooks/trying_tffunction_decorator.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/miguelnavaharris/PiNN/docs/notebooks/trying_tffunction_decorator.ipynb#ch0000012?line=0'>1</a>\u001b[0m train_loop(newtrain)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/tffunction/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "\u001b[0;32m--> <a href='file:///Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=152'>153</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=153'>154</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;32m    <a href='file:///Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=154'>155</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/envs/tffunction/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n",
      "\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;32m-> <a href='file:///Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n",
      "\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m   <a href='file:///Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n",
      "\n",
      "    File \"/var/folders/2r/53clw21s5zv7ml4w3_xhd8v40000gq/T/ipykernel_99516/3938954701.py\", line 40, in train_loop  *\n",
      "        optimizer.apply_gradients(zip(grads, network.trainable_weights))\n",
      "    File \"/Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 639, in apply_gradients  **\n",
      "        self._create_all_weights(var_list)\n",
      "    File \"/Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 823, in _create_all_weights\n",
      "        _ = self.iterations\n",
      "    File \"/Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 830, in __getattribute__\n",
      "        return super(OptimizerV2, self).__getattribute__(name)\n",
      "    File \"/Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 985, in iterations\n",
      "        self._iterations = self.add_weight(\n",
      "    File \"/Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 1187, in add_weight\n",
      "        variable = self._add_variable_with_custom_getter(\n",
      "    File \"/Users/miguelnavaharris/miniforge3/envs/tffunction/lib/python3.9/site-packages/keras/engine/base_layer_utils.py\", line 117, in make_variable\n",
      "        return tf.compat.v1.Variable(\n",
      "\n",
      "    ValueError: tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\n"
     ]
    }
   ],
   "source": [
    "train_loop(newtrain)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eff9d1635c537b0ae1e71a48bd7271b570b4b02d7cc7afd96be6d2c9e9acc782"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('tffunction')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
